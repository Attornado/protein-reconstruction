{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/prot_prj/PycharmProjects/protein-reconstruction\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "cwd = os.getcwd()\n",
    "if re.search(\"protein-reconstruction.+\", cwd):\n",
    "    os.chdir(\"..\")\n",
    "print(os.getcwd())\n",
    "import torch\n",
    "from torch_geometric.loader import DataLoader\n",
    "from preprocessing.dataset import load_dataset\n",
    "from models.pretraining.graph_infomax import DeepGraphInfomaxV2, RandomPermutationCorruption, MeanPoolReadout\n",
    "from models.pretraining.encoders import RevSAGEConvEncoder\n",
    "from tqdm.autonotebook import tqdm\n",
    "from torchinfo import summary\n",
    "from typing import final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(edge_index=[2, 1616], node_id=[858], coords=[858, 3], meiler=[858, 7], name=[1], dist_mat=[1], num_nodes=858, graph_y=5, x=[858, 10], y=5) Data(edge_index=[2, 1721], node_id=[901], coords=[901, 3], meiler=[901, 7], name=[1], dist_mat=[1], num_nodes=901, graph_y=5, x=[901, 10], y=5)\n"
     ]
    }
   ],
   "source": [
    "ds_train = load_dataset(\"data/cleaned/pscdb/train\")\n",
    "ds_val = load_dataset(\"data/cleaned/pscdb/validation\")\n",
    "dl_train = DataLoader(ds_train, batch_size=1, shuffle=True)\n",
    "dl_val = DataLoader(ds_val, batch_size=1, shuffle=True)\n",
    "print(ds_train[0], ds_val[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================================================\n",
      "Layer (type:depth-idx)                                       Param #\n",
      "=====================================================================================\n",
      "DeepGraphInfomaxV2                                           2,500\n",
      "├─RevSAGEConvEncoder: 1-1                                    --\n",
      "│    └─Linear: 2-1                                           550\n",
      "│    └─LayerNorm: 2-2                                        100\n",
      "│    └─ModuleList: 2-3                                       --\n",
      "│    │    └─GroupAddRev: 3-1                                 --\n",
      "│    │    │    └─ModuleList: 4-1                             --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-1                     65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-2                     65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-3                     65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-4                     65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-5                     65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-6                     65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-7                     65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-8                     65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-9                     65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-10                    65\n",
      "│    │    └─GroupAddRev: 3-2                                 --\n",
      "│    │    │    └─ModuleList: 4-2                             --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-11                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-12                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-13                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-14                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-15                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-16                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-17                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-18                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-19                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-20                    65\n",
      "│    │    └─GroupAddRev: 3-3                                 --\n",
      "│    │    │    └─ModuleList: 4-3                             --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-21                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-22                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-23                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-24                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-25                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-26                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-27                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-28                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-29                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-30                    65\n",
      "│    │    └─GroupAddRev: 3-4                                 --\n",
      "│    │    │    └─ModuleList: 4-4                             --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-31                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-32                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-33                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-34                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-35                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-36                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-37                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-38                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-39                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-40                    65\n",
      "│    │    └─GroupAddRev: 3-5                                 --\n",
      "│    │    │    └─ModuleList: 4-5                             --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-41                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-42                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-43                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-44                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-45                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-46                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-47                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-48                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-49                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-50                    65\n",
      "│    │    └─GroupAddRev: 3-6                                 --\n",
      "│    │    │    └─ModuleList: 4-6                             --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-51                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-52                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-53                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-54                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-55                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-56                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-57                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-58                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-59                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-60                    65\n",
      "│    │    └─GroupAddRev: 3-7                                 --\n",
      "│    │    │    └─ModuleList: 4-7                             --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-61                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-62                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-63                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-64                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-65                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-66                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-67                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-68                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-69                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-70                    65\n",
      "│    │    └─GroupAddRev: 3-8                                 --\n",
      "│    │    │    └─ModuleList: 4-8                             --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-71                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-72                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-73                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-74                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-75                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-76                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-77                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-78                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-79                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-80                    65\n",
      "│    │    └─GroupAddRev: 3-9                                 --\n",
      "│    │    │    └─ModuleList: 4-9                             --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-81                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-82                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-83                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-84                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-85                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-86                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-87                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-88                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-89                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-90                    65\n",
      "│    │    └─GroupAddRev: 3-10                                --\n",
      "│    │    │    └─ModuleList: 4-10                            --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-91                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-92                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-93                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-94                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-95                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-96                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-97                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-98                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-99                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-100                   65\n",
      "│    │    └─GroupAddRev: 3-11                                --\n",
      "│    │    │    └─ModuleList: 4-11                            --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-101                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-102                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-103                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-104                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-105                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-106                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-107                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-108                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-109                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-110                   65\n",
      "│    │    └─GroupAddRev: 3-12                                --\n",
      "│    │    │    └─ModuleList: 4-12                            --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-111                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-112                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-113                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-114                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-115                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-116                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-117                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-118                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-119                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-120                   65\n",
      "│    │    └─GroupAddRev: 3-13                                --\n",
      "│    │    │    └─ModuleList: 4-13                            --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-121                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-122                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-123                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-124                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-125                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-126                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-127                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-128                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-129                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-130                   65\n",
      "│    │    └─GroupAddRev: 3-14                                --\n",
      "│    │    │    └─ModuleList: 4-14                            --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-131                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-132                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-133                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-134                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-135                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-136                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-137                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-138                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-139                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-140                   65\n",
      "│    │    └─GroupAddRev: 3-15                                --\n",
      "│    │    │    └─ModuleList: 4-15                            --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-141                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-142                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-143                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-144                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-145                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-146                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-147                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-148                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-149                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-150                   65\n",
      "│    │    └─GroupAddRev: 3-16                                --\n",
      "│    │    │    └─ModuleList: 4-16                            --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-151                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-152                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-153                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-154                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-155                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-156                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-157                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-158                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-159                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-160                   65\n",
      "│    │    └─GroupAddRev: 3-17                                --\n",
      "│    │    │    └─ModuleList: 4-17                            --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-161                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-162                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-163                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-164                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-165                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-166                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-167                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-168                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-169                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-170                   65\n",
      "│    │    └─GroupAddRev: 3-18                                --\n",
      "│    │    │    └─ModuleList: 4-18                            --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-171                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-172                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-173                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-174                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-175                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-176                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-177                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-178                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-179                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-180                   65\n",
      "│    │    └─GroupAddRev: 3-19                                --\n",
      "│    │    │    └─ModuleList: 4-19                            --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-181                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-182                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-183                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-184                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-185                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-186                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-187                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-188                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-189                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-190                   65\n",
      "│    │    └─GroupAddRev: 3-20                                --\n",
      "│    │    │    └─ModuleList: 4-20                            --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-191                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-192                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-193                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-194                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-195                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-196                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-197                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-198                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-199                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-200                   65\n",
      "│    │    └─GroupAddRev: 3-21                                --\n",
      "│    │    │    └─ModuleList: 4-21                            --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-201                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-202                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-203                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-204                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-205                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-206                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-207                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-208                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-209                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-210                   65\n",
      "│    │    └─GroupAddRev: 3-22                                --\n",
      "│    │    │    └─ModuleList: 4-22                            --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-211                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-212                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-213                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-214                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-215                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-216                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-217                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-218                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-219                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-220                   65\n",
      "│    │    └─GroupAddRev: 3-23                                --\n",
      "│    │    │    └─ModuleList: 4-23                            --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-221                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-222                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-223                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-224                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-225                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-226                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-227                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-228                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-229                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-230                   65\n",
      "│    │    └─GroupAddRev: 3-24                                --\n",
      "│    │    │    └─ModuleList: 4-24                            --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-231                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-232                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-233                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-234                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-235                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-236                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-237                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-238                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-239                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-240                   65\n",
      "│    │    └─GroupAddRev: 3-25                                --\n",
      "│    │    │    └─ModuleList: 4-25                            --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-241                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-242                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-243                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-244                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-245                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-246                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-247                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-248                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-249                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-250                   65\n",
      "│    │    └─GroupAddRev: 3-26                                --\n",
      "│    │    │    └─ModuleList: 4-26                            --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-251                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-252                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-253                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-254                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-255                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-256                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-257                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-258                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-259                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-260                   65\n",
      "│    │    └─GroupAddRev: 3-27                                --\n",
      "│    │    │    └─ModuleList: 4-27                            --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-261                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-262                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-263                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-264                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-265                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-266                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-267                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-268                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-269                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-270                   65\n",
      "│    │    └─GroupAddRev: 3-28                                --\n",
      "│    │    │    └─ModuleList: 4-28                            --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-271                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-272                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-273                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-274                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-275                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-276                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-277                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-278                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-279                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-280                   65\n",
      "│    │    └─GroupAddRev: 3-29                                --\n",
      "│    │    │    └─ModuleList: 4-29                            --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-281                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-282                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-283                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-284                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-285                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-286                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-287                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-288                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-289                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-290                   65\n",
      "│    │    └─GroupAddRev: 3-30                                --\n",
      "│    │    │    └─ModuleList: 4-30                            --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-291                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-292                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-293                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-294                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-295                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-296                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-297                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-298                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-299                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-300                   65\n",
      "│    │    └─GroupAddRev: 3-31                                --\n",
      "│    │    │    └─ModuleList: 4-31                            --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-301                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-302                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-303                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-304                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-305                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-306                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-307                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-308                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-309                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-310                   65\n",
      "│    │    └─GroupAddRev: 3-32                                --\n",
      "│    │    │    └─ModuleList: 4-32                            --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-311                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-312                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-313                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-314                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-315                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-316                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-317                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-318                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-319                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-320                   65\n",
      "│    │    └─GroupAddRev: 3-33                                --\n",
      "│    │    │    └─ModuleList: 4-33                            --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-321                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-322                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-323                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-324                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-325                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-326                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-327                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-328                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-329                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-330                   65\n",
      "│    │    └─GroupAddRev: 3-34                                --\n",
      "│    │    │    └─ModuleList: 4-34                            --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-331                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-332                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-333                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-334                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-335                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-336                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-337                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-338                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-339                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-340                   65\n",
      "│    │    └─GroupAddRev: 3-35                                --\n",
      "│    │    │    └─ModuleList: 4-35                            --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-341                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-342                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-343                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-344                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-345                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-346                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-347                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-348                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-349                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-350                   65\n",
      "│    │    └─GroupAddRev: 3-36                                --\n",
      "│    │    │    └─ModuleList: 4-36                            --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-351                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-352                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-353                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-354                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-355                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-356                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-357                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-358                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-359                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-360                   65\n",
      "│    │    └─GroupAddRev: 3-37                                --\n",
      "│    │    │    └─ModuleList: 4-37                            --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-361                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-362                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-363                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-364                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-365                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-366                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-367                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-368                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-369                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-370                   65\n",
      "│    │    └─GroupAddRev: 3-38                                --\n",
      "│    │    │    └─ModuleList: 4-38                            --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-371                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-372                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-373                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-374                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-375                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-376                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-377                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-378                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-379                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-380                   65\n",
      "│    │    └─GroupAddRev: 3-39                                --\n",
      "│    │    │    └─ModuleList: 4-39                            --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-381                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-382                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-383                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-384                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-385                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-386                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-387                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-388                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-389                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-390                   65\n",
      "│    │    └─GroupAddRev: 3-40                                --\n",
      "│    │    │    └─ModuleList: 4-40                            --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-391                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-392                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-393                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-394                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-395                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-396                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-397                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-398                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-399                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-400                   65\n",
      "│    │    └─GroupAddRev: 3-41                                --\n",
      "│    │    │    └─ModuleList: 4-41                            --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-401                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-402                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-403                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-404                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-405                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-406                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-407                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-408                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-409                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-410                   65\n",
      "│    │    └─GroupAddRev: 3-42                                --\n",
      "│    │    │    └─ModuleList: 4-42                            --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-411                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-412                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-413                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-414                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-415                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-416                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-417                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-418                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-419                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-420                   65\n",
      "│    │    └─GroupAddRev: 3-43                                --\n",
      "│    │    │    └─ModuleList: 4-43                            --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-421                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-422                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-423                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-424                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-425                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-426                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-427                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-428                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-429                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-430                   65\n",
      "│    │    └─GroupAddRev: 3-44                                --\n",
      "│    │    │    └─ModuleList: 4-44                            --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-431                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-432                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-433                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-434                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-435                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-436                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-437                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-438                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-439                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-440                   65\n",
      "│    │    └─GroupAddRev: 3-45                                --\n",
      "│    │    │    └─ModuleList: 4-45                            --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-441                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-442                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-443                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-444                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-445                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-446                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-447                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-448                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-449                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-450                   65\n",
      "│    │    └─GroupAddRev: 3-46                                --\n",
      "│    │    │    └─ModuleList: 4-46                            --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-451                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-452                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-453                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-454                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-455                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-456                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-457                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-458                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-459                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-460                   65\n",
      "│    │    └─GroupAddRev: 3-47                                --\n",
      "│    │    │    └─ModuleList: 4-47                            --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-461                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-462                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-463                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-464                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-465                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-466                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-467                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-468                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-469                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-470                   65\n",
      "│    │    └─GroupAddRev: 3-48                                --\n",
      "│    │    │    └─ModuleList: 4-48                            --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-471                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-472                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-473                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-474                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-475                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-476                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-477                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-478                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-479                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-480                   65\n",
      "│    │    └─GroupAddRev: 3-49                                --\n",
      "│    │    │    └─ModuleList: 4-49                            --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-481                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-482                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-483                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-484                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-485                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-486                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-487                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-488                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-489                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-490                   65\n",
      "│    │    └─GroupAddRev: 3-50                                --\n",
      "│    │    │    └─ModuleList: 4-50                            --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-491                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-492                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-493                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-494                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-495                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-496                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-497                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-498                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-499                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-500                   65\n",
      "│    │    └─GroupAddRev: 3-51                                --\n",
      "│    │    │    └─ModuleList: 4-51                            --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-501                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-502                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-503                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-504                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-505                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-506                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-507                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-508                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-509                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-510                   65\n",
      "│    │    └─GroupAddRev: 3-52                                --\n",
      "│    │    │    └─ModuleList: 4-52                            --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-511                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-512                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-513                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-514                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-515                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-516                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-517                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-518                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-519                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-520                   65\n",
      "│    │    └─GroupAddRev: 3-53                                --\n",
      "│    │    │    └─ModuleList: 4-53                            --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-521                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-522                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-523                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-524                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-525                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-526                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-527                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-528                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-529                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-530                   65\n",
      "│    │    └─GroupAddRev: 3-54                                --\n",
      "│    │    │    └─ModuleList: 4-54                            --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-531                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-532                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-533                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-534                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-535                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-536                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-537                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-538                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-539                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-540                   65\n",
      "│    │    └─GroupAddRev: 3-55                                --\n",
      "│    │    │    └─ModuleList: 4-55                            --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-541                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-542                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-543                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-544                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-545                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-546                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-547                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-548                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-549                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-550                   65\n",
      "│    │    └─GroupAddRev: 3-56                                --\n",
      "│    │    │    └─ModuleList: 4-56                            --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-551                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-552                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-553                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-554                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-555                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-556                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-557                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-558                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-559                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-560                   65\n",
      "│    │    └─GroupAddRev: 3-57                                --\n",
      "│    │    │    └─ModuleList: 4-57                            --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-561                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-562                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-563                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-564                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-565                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-566                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-567                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-568                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-569                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-570                   65\n",
      "│    │    └─GroupAddRev: 3-58                                --\n",
      "│    │    │    └─ModuleList: 4-58                            --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-571                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-572                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-573                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-574                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-575                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-576                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-577                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-578                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-579                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-580                   65\n",
      "│    │    └─GroupAddRev: 3-59                                --\n",
      "│    │    │    └─ModuleList: 4-59                            --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-581                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-582                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-583                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-584                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-585                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-586                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-587                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-588                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-589                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-590                   65\n",
      "│    │    └─GroupAddRev: 3-60                                --\n",
      "│    │    │    └─ModuleList: 4-60                            --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-591                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-592                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-593                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-594                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-595                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-596                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-597                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-598                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-599                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-600                   65\n",
      "├─LayerNorm: 1-2                                             100\n",
      "=====================================================================================\n",
      "Total params: 42,250\n",
      "Trainable params: 42,250\n",
      "Non-trainable params: 0\n",
      "=====================================================================================\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH_STATE_DICT: final = \"data/fitted/pretraining/dgi/dgi_rev_sage_test5/checkpoint.pt\"\n",
    "MODEL_PATH_PARAMS: final = \"data/fitted/pretraining/dgi/dgi_rev_sage_test5/constructor_params.pt\"\n",
    "\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "state_dict = torch.load(MODEL_PATH_STATE_DICT)\n",
    "constructor_params = torch.load(MODEL_PATH_PARAMS)\n",
    "dgi = DeepGraphInfomaxV2.from_constructor_params(constructor_params, encoder_constructor=RevSAGEConvEncoder, readout=MeanPoolReadout(device, sigmoid=False), corruption=RandomPermutationCorruption(device))\n",
    "print(summary(dgi, device=device, depth=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7e59f9b82e5485cada7b1a2ae9cea91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/550 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done train.\n"
     ]
    }
   ],
   "source": [
    "x_list = []\n",
    "y_list = []\n",
    "dgi.eval()\n",
    "dgi = dgi.to(device)\n",
    "with torch.no_grad():\n",
    "    for el in tqdm(iter(dl_train)):\n",
    "        el = el.to(device)\n",
    "        pos_z, neg_z, summary = dgi(el.x, el.edge_index)\n",
    "        x_list.append(summary)\n",
    "        y_list.append(el.y)\n",
    "    print(\"Done train conversion.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bb368a91ee84aa988c30f6e3e55706b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/119 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done validation.\n"
     ]
    }
   ],
   "source": [
    "x_list_val = []\n",
    "y_list_val = []\n",
    "with torch.no_grad():\n",
    "    for el in tqdm(iter(dl_val)):\n",
    "        el = el.to(device)\n",
    "        pos_z, neg_z, summary = dgi(el.x, el.edge_index)\n",
    "        x_list_val.append(summary)\n",
    "        y_list_val.append(el.y)\n",
    "    print(\"Done validation conversion.\")\n",
    "dgi_2 = dgi.to('cpu')  # free GPU memory\n",
    "del dgi\n",
    "dgi = dgi_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20c5e4e48f4547e5b7fd7155c836e37f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/550 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6687abd01e2b4ad9806a967b65827e16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/119 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50])\n",
      "torch.Size([50])\n",
      "torch.Size([550, 50])\n",
      "torch.Size([550])\n",
      "------------------------------------\n",
      "torch.Size([119, 50])\n",
      "torch.Size([119])\n"
     ]
    }
   ],
   "source": [
    "max_len = 0\n",
    "for i in range(0, len(x_list)):\n",
    "    if x_list[i].shape[0] > max_len:\n",
    "        max_len = x_list[i].shape[0]\n",
    "for i in range(0, len(x_list_val)):\n",
    "    if x_list_val[i].shape[0] > max_len:\n",
    "        max_len = x_list_val[i].shape[0]\n",
    "\n",
    "# Mean embeddings if required\n",
    "for i in tqdm(range(0, len(x_list))):\n",
    "    if len(x_list[i].shape) > 1:\n",
    "        x_list[i] = torch.mean(x_list[i], dim=0, keepdim=False)\n",
    "    #x_list[i] = torch.nn.functional.normalize(x_list[i], p=2.0, dim=0, eps=1e-12)\n",
    "\n",
    "for i in tqdm(range(0, len(x_list_val))):\n",
    "    if len(x_list_val[i].shape) > 1:\n",
    "        x_list_val[i] = torch.mean(x_list_val[i], dim=0, keepdim=False)\n",
    "    #x_list[i] = torch.nn.functional.normalize(x_list[i], p=2.0, dim=0, eps=1e-12)\n",
    "\n",
    "print(x_list[0].shape)\n",
    "print(x_list_val[0].shape)\n",
    "x_train = torch.stack(x_list)\n",
    "y_train = torch.tensor(y_list)\n",
    "x_val = torch.stack(x_list_val)\n",
    "y_val = torch.tensor(y_list_val)\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(\"------------------------------------\")\n",
    "print(x_val.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Newton-Cg Score: 0.3865546218487395\n",
      "liblinear Score: 0.3865546218487395\n",
      "sag Score: 0.3865546218487395\n",
      "saga Score: 0.3865546218487395\n",
      "lbfgs Score: 0.3865546218487395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/scipy/optimize/linesearch.py:327: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/utils/optimize.py:203: UserWarning: Line Search failed\n",
      "  warnings.warn(\"Line Search failed\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "scores = dgi.test(train_z=x_train, train_y=y_train, test_z=x_val, test_y=y_val, solver='newton-cg', max_iter=1000)\n",
    "print(f\"Newton-Cg Score: {scores}\")\n",
    "scores = dgi.test(train_z=x_train, train_y=y_train, test_z=x_val, test_y=y_val, solver='liblinear', max_iter=1000)\n",
    "print(f\"liblinear Score: {scores}\")\n",
    "#scores = dgi.test(train_z=x_train, train_y=y_train, test_z=x_val, test_y=y_val, solver='newton-cholesky')\n",
    "#print(f\"newton-cholesky Score: {scores}\")\n",
    "scores = dgi.test(train_z=x_train, train_y=y_train, test_z=x_val, test_y=y_val, solver='sag', max_iter=1000)\n",
    "print(f\"sag Score: {scores}\")\n",
    "scores = dgi.test(train_z=x_train, train_y=y_train, test_z=x_val, test_y=y_val, solver='saga', max_iter=1000)\n",
    "print(f\"saga Score: {scores}\")\n",
    "scores = dgi.test(train_z=x_train, train_y=y_train, test_z=x_val, test_y=y_val, solver='lbfgs', max_iter=1000)\n",
    "print(f\"lbfgs Score: {scores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.89673801\n",
      "Validation score: 0.321429\n",
      "Iteration 2, loss = 1.88269287\n",
      "Validation score: 0.321429\n",
      "Iteration 3, loss = 1.86173678\n",
      "Validation score: 0.321429\n",
      "Iteration 4, loss = 1.83960396\n",
      "Validation score: 0.321429\n",
      "Iteration 5, loss = 1.81642375\n",
      "Validation score: 0.321429\n",
      "Iteration 6, loss = 1.79556764\n",
      "Validation score: 0.321429\n",
      "Iteration 7, loss = 1.77708550\n",
      "Validation score: 0.392857\n",
      "Iteration 8, loss = 1.76085625\n",
      "Validation score: 0.464286\n",
      "Iteration 9, loss = 1.74667987\n",
      "Validation score: 0.500000\n",
      "Iteration 10, loss = 1.73482416\n",
      "Validation score: 0.500000\n",
      "Iteration 11, loss = 1.72416425\n",
      "Validation score: 0.464286\n",
      "Iteration 12, loss = 1.71471113\n",
      "Validation score: 0.464286\n",
      "Iteration 13, loss = 1.70692746\n",
      "Validation score: 0.464286\n",
      "Iteration 14, loss = 1.69982682\n",
      "Validation score: 0.464286\n",
      "Iteration 15, loss = 1.69373022\n",
      "Validation score: 0.464286\n",
      "Iteration 16, loss = 1.68827917\n",
      "Validation score: 0.464286\n",
      "Iteration 17, loss = 1.68319450\n",
      "Validation score: 0.464286\n",
      "Iteration 18, loss = 1.67885377\n",
      "Validation score: 0.464286\n",
      "Iteration 19, loss = 1.67487637\n",
      "Validation score: 0.464286\n",
      "Iteration 20, loss = 1.67119991\n",
      "Validation score: 0.464286\n",
      "Iteration 21, loss = 1.66762429\n",
      "Validation score: 0.464286\n",
      "Iteration 22, loss = 1.66452584\n",
      "Validation score: 0.464286\n",
      "Iteration 23, loss = 1.66146716\n",
      "Validation score: 0.464286\n",
      "Iteration 24, loss = 1.65848674\n",
      "Validation score: 0.464286\n",
      "Iteration 25, loss = 1.65597953\n",
      "Validation score: 0.464286\n",
      "Iteration 26, loss = 1.65319499\n",
      "Validation score: 0.464286\n",
      "Iteration 27, loss = 1.65115347\n",
      "Validation score: 0.464286\n",
      "Iteration 28, loss = 1.64880822\n",
      "Validation score: 0.464286\n",
      "Iteration 29, loss = 1.64643378\n",
      "Validation score: 0.464286\n",
      "Iteration 30, loss = 1.64452149\n",
      "Validation score: 0.464286\n",
      "Iteration 31, loss = 1.64263243\n",
      "Validation score: 0.464286\n",
      "Iteration 32, loss = 1.64079205\n",
      "Validation score: 0.464286\n",
      "Iteration 33, loss = 1.63896843\n",
      "Validation score: 0.464286\n",
      "Iteration 34, loss = 1.63720830\n",
      "Validation score: 0.464286\n",
      "Iteration 35, loss = 1.63559857\n",
      "Validation score: 0.464286\n",
      "Iteration 36, loss = 1.63408832\n",
      "Validation score: 0.464286\n",
      "Iteration 37, loss = 1.63249658\n",
      "Validation score: 0.464286\n",
      "Iteration 38, loss = 1.63097395\n",
      "Validation score: 0.464286\n",
      "Iteration 39, loss = 1.62957806\n",
      "Validation score: 0.464286\n",
      "Iteration 40, loss = 1.62808256\n",
      "Validation score: 0.464286\n",
      "Iteration 41, loss = 1.62668640\n",
      "Validation score: 0.464286\n",
      "Iteration 42, loss = 1.62529739\n",
      "Validation score: 0.464286\n",
      "Iteration 43, loss = 1.62407130\n",
      "Validation score: 0.464286\n",
      "Iteration 44, loss = 1.62281901\n",
      "Validation score: 0.464286\n",
      "Iteration 45, loss = 1.62153746\n",
      "Validation score: 0.464286\n",
      "Iteration 46, loss = 1.62030543\n",
      "Validation score: 0.464286\n",
      "Iteration 47, loss = 1.61917109\n",
      "Validation score: 0.464286\n",
      "Iteration 48, loss = 1.61792935\n",
      "Validation score: 0.464286\n",
      "Iteration 49, loss = 1.61680883\n",
      "Validation score: 0.464286\n",
      "Iteration 50, loss = 1.61575812\n",
      "Validation score: 0.464286\n",
      "Iteration 51, loss = 1.61484272\n",
      "Validation score: 0.464286\n",
      "Iteration 52, loss = 1.61371618\n",
      "Validation score: 0.464286\n",
      "Iteration 53, loss = 1.61277007\n",
      "Validation score: 0.464286\n",
      "Iteration 54, loss = 1.61181454\n",
      "Validation score: 0.464286\n",
      "Iteration 55, loss = 1.61082732\n",
      "Validation score: 0.464286\n",
      "Iteration 56, loss = 1.60982593\n",
      "Validation score: 0.464286\n",
      "Iteration 57, loss = 1.60900058\n",
      "Validation score: 0.464286\n",
      "Iteration 58, loss = 1.60820081\n",
      "Validation score: 0.464286\n",
      "Iteration 59, loss = 1.60726549\n",
      "Validation score: 0.464286\n",
      "Iteration 60, loss = 1.60642864\n",
      "Validation score: 0.464286\n",
      "Iteration 61, loss = 1.60560544\n",
      "Validation score: 0.464286\n",
      "Iteration 62, loss = 1.60478740\n",
      "Validation score: 0.464286\n",
      "Iteration 63, loss = 1.60404514\n",
      "Validation score: 0.464286\n",
      "Iteration 64, loss = 1.60321868\n",
      "Validation score: 0.464286\n",
      "Iteration 65, loss = 1.60247007\n",
      "Validation score: 0.464286\n",
      "Iteration 66, loss = 1.60171596\n",
      "Validation score: 0.464286\n",
      "Iteration 67, loss = 1.60102021\n",
      "Validation score: 0.464286\n",
      "Iteration 68, loss = 1.60024455\n",
      "Validation score: 0.464286\n",
      "Iteration 69, loss = 1.59957744\n",
      "Validation score: 0.464286\n",
      "Iteration 70, loss = 1.59876133\n",
      "Validation score: 0.464286\n",
      "Iteration 71, loss = 1.59819779\n",
      "Validation score: 0.464286\n",
      "Iteration 72, loss = 1.59751902\n",
      "Validation score: 0.464286\n",
      "Iteration 73, loss = 1.59679778\n",
      "Validation score: 0.464286\n",
      "Iteration 74, loss = 1.59607277\n",
      "Validation score: 0.464286\n",
      "Iteration 75, loss = 1.59549531\n",
      "Validation score: 0.464286\n",
      "Iteration 76, loss = 1.59491843\n",
      "Validation score: 0.464286\n",
      "Iteration 77, loss = 1.59414940\n",
      "Validation score: 0.464286\n",
      "Iteration 78, loss = 1.59347526\n",
      "Validation score: 0.464286\n",
      "Iteration 79, loss = 1.59283380\n",
      "Validation score: 0.464286\n",
      "Iteration 80, loss = 1.59219902\n",
      "Validation score: 0.464286\n",
      "Iteration 81, loss = 1.59159912\n",
      "Validation score: 0.464286\n",
      "Iteration 82, loss = 1.59093257\n",
      "Validation score: 0.464286\n",
      "Iteration 83, loss = 1.59035469\n",
      "Validation score: 0.464286\n",
      "Iteration 84, loss = 1.58977943\n",
      "Validation score: 0.464286\n",
      "Iteration 85, loss = 1.58915033\n",
      "Validation score: 0.464286\n",
      "Iteration 86, loss = 1.58864034\n",
      "Validation score: 0.464286\n",
      "Iteration 87, loss = 1.58810663\n",
      "Validation score: 0.464286\n",
      "Iteration 88, loss = 1.58754347\n",
      "Validation score: 0.464286\n",
      "Iteration 89, loss = 1.58695294\n",
      "Validation score: 0.464286\n",
      "Iteration 90, loss = 1.58645372\n",
      "Validation score: 0.464286\n",
      "Iteration 91, loss = 1.58603658\n",
      "Validation score: 0.464286\n",
      "Iteration 92, loss = 1.58551563\n",
      "Validation score: 0.464286\n",
      "Iteration 93, loss = 1.58494543\n",
      "Validation score: 0.464286\n",
      "Iteration 94, loss = 1.58444157\n",
      "Validation score: 0.464286\n",
      "Iteration 95, loss = 1.58396352\n",
      "Validation score: 0.464286\n",
      "Iteration 96, loss = 1.58350594\n",
      "Validation score: 0.464286\n",
      "Iteration 97, loss = 1.58305057\n",
      "Validation score: 0.464286\n",
      "Iteration 98, loss = 1.58257454\n",
      "Validation score: 0.464286\n",
      "Iteration 99, loss = 1.58218688\n",
      "Validation score: 0.464286\n",
      "Iteration 100, loss = 1.58167582\n",
      "Validation score: 0.464286\n",
      "Iteration 101, loss = 1.58138606\n",
      "Validation score: 0.464286\n",
      "Iteration 102, loss = 1.58084551\n",
      "Validation score: 0.464286\n",
      "Iteration 103, loss = 1.58038567\n",
      "Validation score: 0.464286\n",
      "Iteration 104, loss = 1.57998217\n",
      "Validation score: 0.464286\n",
      "Iteration 105, loss = 1.57953965\n",
      "Validation score: 0.464286\n",
      "Iteration 106, loss = 1.57921113\n",
      "Validation score: 0.464286\n",
      "Iteration 107, loss = 1.57876147\n",
      "Validation score: 0.464286\n",
      "Iteration 108, loss = 1.57837247\n",
      "Validation score: 0.464286\n",
      "Iteration 109, loss = 1.57800563\n",
      "Validation score: 0.464286\n",
      "Iteration 110, loss = 1.57767847\n",
      "Validation score: 0.464286\n",
      "Iteration 111, loss = 1.57730648\n",
      "Validation score: 0.464286\n",
      "Iteration 112, loss = 1.57689126\n",
      "Validation score: 0.464286\n",
      "Iteration 113, loss = 1.57662325\n",
      "Validation score: 0.464286\n",
      "Iteration 114, loss = 1.57624425\n",
      "Validation score: 0.464286\n",
      "Iteration 115, loss = 1.57589128\n",
      "Validation score: 0.464286\n",
      "Iteration 116, loss = 1.57559038\n",
      "Validation score: 0.464286\n",
      "Iteration 117, loss = 1.57524441\n",
      "Validation score: 0.464286\n",
      "Iteration 118, loss = 1.57490389\n",
      "Validation score: 0.464286\n",
      "Iteration 119, loss = 1.57457773\n",
      "Validation score: 0.464286\n",
      "Iteration 120, loss = 1.57416279\n",
      "Validation score: 0.464286\n",
      "Iteration 121, loss = 1.57391659\n",
      "Validation score: 0.464286\n",
      "Iteration 122, loss = 1.57358857\n",
      "Validation score: 0.464286\n",
      "Iteration 123, loss = 1.57324887\n",
      "Validation score: 0.464286\n",
      "Iteration 124, loss = 1.57293111\n",
      "Validation score: 0.464286\n",
      "Iteration 125, loss = 1.57264258\n",
      "Validation score: 0.464286\n",
      "Iteration 126, loss = 1.57232839\n",
      "Validation score: 0.464286\n",
      "Iteration 127, loss = 1.57207327\n",
      "Validation score: 0.464286\n",
      "Iteration 128, loss = 1.57176258\n",
      "Validation score: 0.464286\n",
      "Iteration 129, loss = 1.57138507\n",
      "Validation score: 0.464286\n",
      "Iteration 130, loss = 1.57108004\n",
      "Validation score: 0.464286\n",
      "Iteration 131, loss = 1.57076187\n",
      "Validation score: 0.464286\n",
      "Iteration 132, loss = 1.57051512\n",
      "Validation score: 0.464286\n",
      "Iteration 133, loss = 1.57012057\n",
      "Validation score: 0.464286\n",
      "Iteration 134, loss = 1.56982739\n",
      "Validation score: 0.464286\n",
      "Iteration 135, loss = 1.56957498\n",
      "Validation score: 0.464286\n",
      "Iteration 136, loss = 1.56924644\n",
      "Validation score: 0.464286\n",
      "Iteration 137, loss = 1.56891453\n",
      "Validation score: 0.464286\n",
      "Iteration 138, loss = 1.56861180\n",
      "Validation score: 0.464286\n",
      "Iteration 139, loss = 1.56830404\n",
      "Validation score: 0.464286\n",
      "Iteration 140, loss = 1.56801953\n",
      "Validation score: 0.464286\n",
      "Iteration 141, loss = 1.56770651\n",
      "Validation score: 0.464286\n",
      "Iteration 142, loss = 1.56745337\n",
      "Validation score: 0.464286\n",
      "Iteration 143, loss = 1.56714000\n",
      "Validation score: 0.464286\n",
      "Iteration 144, loss = 1.56690229\n",
      "Validation score: 0.428571\n",
      "Iteration 145, loss = 1.56656004\n",
      "Validation score: 0.428571\n",
      "Iteration 146, loss = 1.56630010\n",
      "Validation score: 0.428571\n",
      "Iteration 147, loss = 1.56602226\n",
      "Validation score: 0.428571\n",
      "Iteration 148, loss = 1.56584604\n",
      "Validation score: 0.428571\n",
      "Iteration 149, loss = 1.56546029\n",
      "Validation score: 0.428571\n",
      "Iteration 150, loss = 1.56523881\n",
      "Validation score: 0.428571\n",
      "Iteration 151, loss = 1.56503167\n",
      "Validation score: 0.428571\n",
      "Iteration 152, loss = 1.56471938\n",
      "Validation score: 0.428571\n",
      "Iteration 153, loss = 1.56451836\n",
      "Validation score: 0.428571\n",
      "Iteration 154, loss = 1.56427788\n",
      "Validation score: 0.428571\n",
      "Iteration 155, loss = 1.56395658\n",
      "Validation score: 0.428571\n",
      "Iteration 156, loss = 1.56368646\n",
      "Validation score: 0.428571\n",
      "Iteration 157, loss = 1.56345975\n",
      "Validation score: 0.428571\n",
      "Iteration 158, loss = 1.56323557\n",
      "Validation score: 0.428571\n",
      "Iteration 159, loss = 1.56302602\n",
      "Validation score: 0.428571\n",
      "Iteration 160, loss = 1.56267044\n",
      "Validation score: 0.428571\n",
      "Iteration 161, loss = 1.56242224\n",
      "Validation score: 0.428571\n",
      "Iteration 162, loss = 1.56222317\n",
      "Validation score: 0.428571\n",
      "Iteration 163, loss = 1.56194617\n",
      "Validation score: 0.428571\n",
      "Iteration 164, loss = 1.56170500\n",
      "Validation score: 0.428571\n",
      "Iteration 165, loss = 1.56152106\n",
      "Validation score: 0.428571\n",
      "Iteration 166, loss = 1.56118865\n",
      "Validation score: 0.428571\n",
      "Iteration 167, loss = 1.56093128\n",
      "Validation score: 0.428571\n",
      "Iteration 168, loss = 1.56074933\n",
      "Validation score: 0.428571\n",
      "Iteration 169, loss = 1.56054343\n",
      "Validation score: 0.428571\n",
      "Iteration 170, loss = 1.56022739\n",
      "Validation score: 0.428571\n",
      "Iteration 171, loss = 1.56003026\n",
      "Validation score: 0.428571\n",
      "Iteration 172, loss = 1.55979323\n",
      "Validation score: 0.428571\n",
      "Iteration 173, loss = 1.55963029\n",
      "Validation score: 0.428571\n",
      "Iteration 174, loss = 1.55930537\n",
      "Validation score: 0.428571\n",
      "Iteration 175, loss = 1.55907513\n",
      "Validation score: 0.428571\n",
      "Iteration 176, loss = 1.55881115\n",
      "Validation score: 0.428571\n",
      "Iteration 177, loss = 1.55860284\n",
      "Validation score: 0.428571\n",
      "Iteration 178, loss = 1.55842166\n",
      "Validation score: 0.428571\n",
      "Iteration 179, loss = 1.55812732\n",
      "Validation score: 0.428571\n",
      "Iteration 180, loss = 1.55789070\n",
      "Validation score: 0.428571\n",
      "Iteration 181, loss = 1.55767435\n",
      "Validation score: 0.428571\n",
      "Validation score did not improve more than tol=0.000000 for 170 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 182, loss = 1.55733480\n",
      "Validation score: 0.428571\n",
      "Iteration 183, loss = 1.55717991\n",
      "Validation score: 0.428571\n",
      "Iteration 184, loss = 1.55703054\n",
      "Validation score: 0.428571\n",
      "Iteration 185, loss = 1.55692583\n",
      "Validation score: 0.428571\n",
      "Iteration 186, loss = 1.55683064\n",
      "Validation score: 0.428571\n",
      "Iteration 187, loss = 1.55676017\n",
      "Validation score: 0.428571\n",
      "Iteration 188, loss = 1.55667658\n",
      "Validation score: 0.428571\n",
      "Iteration 189, loss = 1.55660825\n",
      "Validation score: 0.428571\n",
      "Iteration 190, loss = 1.55656388\n",
      "Validation score: 0.428571\n",
      "Iteration 191, loss = 1.55648593\n",
      "Validation score: 0.428571\n",
      "Iteration 192, loss = 1.55645117\n",
      "Validation score: 0.428571\n",
      "Iteration 193, loss = 1.55639598\n",
      "Validation score: 0.428571\n",
      "Iteration 194, loss = 1.55634663\n",
      "Validation score: 0.428571\n",
      "Iteration 195, loss = 1.55628662\n",
      "Validation score: 0.428571\n",
      "Iteration 196, loss = 1.55623713\n",
      "Validation score: 0.428571\n",
      "Iteration 197, loss = 1.55618977\n",
      "Validation score: 0.428571\n",
      "Iteration 198, loss = 1.55617353\n",
      "Validation score: 0.428571\n",
      "Iteration 199, loss = 1.55610659\n",
      "Validation score: 0.428571\n",
      "Iteration 200, loss = 1.55604458\n",
      "Validation score: 0.428571\n",
      "Iteration 201, loss = 1.55603583\n",
      "Validation score: 0.428571\n",
      "Iteration 202, loss = 1.55598370\n",
      "Validation score: 0.428571\n",
      "Iteration 203, loss = 1.55591424\n",
      "Validation score: 0.428571\n",
      "Iteration 204, loss = 1.55586962\n",
      "Validation score: 0.428571\n",
      "Iteration 205, loss = 1.55582548\n",
      "Validation score: 0.428571\n",
      "Iteration 206, loss = 1.55577610\n",
      "Validation score: 0.428571\n",
      "Iteration 207, loss = 1.55573988\n",
      "Validation score: 0.428571\n",
      "Iteration 208, loss = 1.55568287\n",
      "Validation score: 0.428571\n",
      "Iteration 209, loss = 1.55564193\n",
      "Validation score: 0.428571\n",
      "Iteration 210, loss = 1.55559416\n",
      "Validation score: 0.428571\n",
      "Iteration 211, loss = 1.55554896\n",
      "Validation score: 0.428571\n",
      "Iteration 212, loss = 1.55550558\n",
      "Validation score: 0.428571\n",
      "Iteration 213, loss = 1.55545311\n",
      "Validation score: 0.428571\n",
      "Iteration 214, loss = 1.55542811\n",
      "Validation score: 0.428571\n",
      "Iteration 215, loss = 1.55537368\n",
      "Validation score: 0.428571\n",
      "Iteration 216, loss = 1.55532716\n",
      "Validation score: 0.428571\n",
      "Iteration 217, loss = 1.55527965\n",
      "Validation score: 0.428571\n",
      "Iteration 218, loss = 1.55522439\n",
      "Validation score: 0.428571\n",
      "Iteration 219, loss = 1.55517914\n",
      "Validation score: 0.428571\n",
      "Iteration 220, loss = 1.55514014\n",
      "Validation score: 0.428571\n",
      "Iteration 221, loss = 1.55510247\n",
      "Validation score: 0.428571\n",
      "Iteration 222, loss = 1.55506386\n",
      "Validation score: 0.428571\n",
      "Iteration 223, loss = 1.55500454\n",
      "Validation score: 0.428571\n",
      "Iteration 224, loss = 1.55496390\n",
      "Validation score: 0.428571\n",
      "Iteration 225, loss = 1.55491928\n",
      "Validation score: 0.428571\n",
      "Iteration 226, loss = 1.55488279\n",
      "Validation score: 0.428571\n",
      "Iteration 227, loss = 1.55483218\n",
      "Validation score: 0.428571\n",
      "Iteration 228, loss = 1.55478455\n",
      "Validation score: 0.428571\n",
      "Iteration 229, loss = 1.55474397\n",
      "Validation score: 0.428571\n",
      "Iteration 230, loss = 1.55470544\n",
      "Validation score: 0.428571\n",
      "Iteration 231, loss = 1.55467076\n",
      "Validation score: 0.428571\n",
      "Iteration 232, loss = 1.55462077\n",
      "Validation score: 0.428571\n",
      "Iteration 233, loss = 1.55457339\n",
      "Validation score: 0.428571\n",
      "Iteration 234, loss = 1.55451834\n",
      "Validation score: 0.428571\n",
      "Iteration 235, loss = 1.55449244\n",
      "Validation score: 0.428571\n",
      "Iteration 236, loss = 1.55444204\n",
      "Validation score: 0.428571\n",
      "Iteration 237, loss = 1.55438805\n",
      "Validation score: 0.428571\n",
      "Iteration 238, loss = 1.55434007\n",
      "Validation score: 0.428571\n",
      "Iteration 239, loss = 1.55429943\n",
      "Validation score: 0.428571\n",
      "Iteration 240, loss = 1.55425576\n",
      "Validation score: 0.428571\n",
      "Iteration 241, loss = 1.55423838\n",
      "Validation score: 0.428571\n",
      "Iteration 242, loss = 1.55416656\n",
      "Validation score: 0.428571\n",
      "Iteration 243, loss = 1.55412750\n",
      "Validation score: 0.428571\n",
      "Iteration 244, loss = 1.55408917\n",
      "Validation score: 0.428571\n",
      "Iteration 245, loss = 1.55403842\n",
      "Validation score: 0.428571\n",
      "Iteration 246, loss = 1.55400464\n",
      "Validation score: 0.428571\n",
      "Iteration 247, loss = 1.55395375\n",
      "Validation score: 0.428571\n",
      "Iteration 248, loss = 1.55391334\n",
      "Validation score: 0.428571\n",
      "Iteration 249, loss = 1.55387504\n",
      "Validation score: 0.428571\n",
      "Iteration 250, loss = 1.55382821\n",
      "Validation score: 0.428571\n",
      "Iteration 251, loss = 1.55378625\n",
      "Validation score: 0.428571\n",
      "Iteration 252, loss = 1.55373816\n",
      "Validation score: 0.428571\n",
      "Iteration 253, loss = 1.55369700\n",
      "Validation score: 0.428571\n",
      "Iteration 254, loss = 1.55365934\n",
      "Validation score: 0.428571\n",
      "Iteration 255, loss = 1.55361303\n",
      "Validation score: 0.428571\n",
      "Iteration 256, loss = 1.55356854\n",
      "Validation score: 0.428571\n",
      "Iteration 257, loss = 1.55353181\n",
      "Validation score: 0.428571\n",
      "Iteration 258, loss = 1.55348059\n",
      "Validation score: 0.428571\n",
      "Iteration 259, loss = 1.55345650\n",
      "Validation score: 0.428571\n",
      "Iteration 260, loss = 1.55339116\n",
      "Validation score: 0.428571\n",
      "Iteration 261, loss = 1.55338030\n",
      "Validation score: 0.428571\n",
      "Iteration 262, loss = 1.55330693\n",
      "Validation score: 0.428571\n",
      "Iteration 263, loss = 1.55327248\n",
      "Validation score: 0.428571\n",
      "Iteration 264, loss = 1.55323430\n",
      "Validation score: 0.428571\n",
      "Iteration 265, loss = 1.55319109\n",
      "Validation score: 0.428571\n",
      "Iteration 266, loss = 1.55314887\n",
      "Validation score: 0.428571\n",
      "Iteration 267, loss = 1.55309163\n",
      "Validation score: 0.428571\n",
      "Iteration 268, loss = 1.55307336\n",
      "Validation score: 0.428571\n",
      "Iteration 269, loss = 1.55300974\n",
      "Validation score: 0.428571\n",
      "Iteration 270, loss = 1.55297179\n",
      "Validation score: 0.428571\n",
      "Iteration 271, loss = 1.55294402\n",
      "Validation score: 0.428571\n",
      "Iteration 272, loss = 1.55288236\n",
      "Validation score: 0.428571\n",
      "Iteration 273, loss = 1.55285400\n",
      "Validation score: 0.428571\n",
      "Iteration 274, loss = 1.55280325\n",
      "Validation score: 0.428571\n",
      "Iteration 275, loss = 1.55275683\n",
      "Validation score: 0.428571\n",
      "Iteration 276, loss = 1.55272098\n",
      "Validation score: 0.428571\n",
      "Iteration 277, loss = 1.55268525\n",
      "Validation score: 0.428571\n",
      "Iteration 278, loss = 1.55264496\n",
      "Validation score: 0.428571\n",
      "Iteration 279, loss = 1.55258620\n",
      "Validation score: 0.428571\n",
      "Iteration 280, loss = 1.55254904\n",
      "Validation score: 0.428571\n",
      "Iteration 281, loss = 1.55251427\n",
      "Validation score: 0.428571\n",
      "Iteration 282, loss = 1.55246036\n",
      "Validation score: 0.428571\n",
      "Iteration 283, loss = 1.55241644\n",
      "Validation score: 0.428571\n",
      "Iteration 284, loss = 1.55238200\n",
      "Validation score: 0.428571\n",
      "Iteration 285, loss = 1.55235586\n",
      "Validation score: 0.428571\n",
      "Iteration 286, loss = 1.55228581\n",
      "Validation score: 0.428571\n",
      "Iteration 287, loss = 1.55223529\n",
      "Validation score: 0.428571\n",
      "Iteration 288, loss = 1.55220903\n",
      "Validation score: 0.428571\n",
      "Iteration 289, loss = 1.55215647\n",
      "Validation score: 0.428571\n",
      "Iteration 290, loss = 1.55214249\n",
      "Validation score: 0.428571\n",
      "Iteration 291, loss = 1.55208364\n",
      "Validation score: 0.428571\n",
      "Iteration 292, loss = 1.55204938\n",
      "Validation score: 0.428571\n",
      "Iteration 293, loss = 1.55199280\n",
      "Validation score: 0.428571\n",
      "Iteration 294, loss = 1.55196157\n",
      "Validation score: 0.428571\n",
      "Iteration 295, loss = 1.55191281\n",
      "Validation score: 0.428571\n",
      "Iteration 296, loss = 1.55187845\n",
      "Validation score: 0.428571\n",
      "Iteration 297, loss = 1.55182781\n",
      "Validation score: 0.428571\n",
      "Iteration 298, loss = 1.55180842\n",
      "Validation score: 0.428571\n",
      "Iteration 299, loss = 1.55176702\n",
      "Validation score: 0.428571\n",
      "Iteration 300, loss = 1.55171147\n",
      "Validation score: 0.428571\n",
      "Iteration 301, loss = 1.55168042\n",
      "Validation score: 0.428571\n",
      "Iteration 302, loss = 1.55163431\n",
      "Validation score: 0.428571\n",
      "Iteration 303, loss = 1.55159054\n",
      "Validation score: 0.428571\n",
      "Iteration 304, loss = 1.55155555\n",
      "Validation score: 0.428571\n",
      "Iteration 305, loss = 1.55151324\n",
      "Validation score: 0.428571\n",
      "Iteration 306, loss = 1.55148395\n",
      "Validation score: 0.428571\n",
      "Iteration 307, loss = 1.55143635\n",
      "Validation score: 0.428571\n",
      "Iteration 308, loss = 1.55139414\n",
      "Validation score: 0.428571\n",
      "Iteration 309, loss = 1.55136455\n",
      "Validation score: 0.428571\n",
      "Iteration 310, loss = 1.55129971\n",
      "Validation score: 0.428571\n",
      "Iteration 311, loss = 1.55126275\n",
      "Validation score: 0.428571\n",
      "Iteration 312, loss = 1.55122390\n",
      "Validation score: 0.428571\n",
      "Iteration 313, loss = 1.55117733\n",
      "Validation score: 0.428571\n",
      "Iteration 314, loss = 1.55113076\n",
      "Validation score: 0.428571\n",
      "Iteration 315, loss = 1.55109963\n",
      "Validation score: 0.428571\n",
      "Iteration 316, loss = 1.55107367\n",
      "Validation score: 0.428571\n",
      "Iteration 317, loss = 1.55102123\n",
      "Validation score: 0.428571\n",
      "Iteration 318, loss = 1.55097869\n",
      "Validation score: 0.428571\n",
      "Iteration 319, loss = 1.55093779\n",
      "Validation score: 0.428571\n",
      "Iteration 320, loss = 1.55088412\n",
      "Validation score: 0.428571\n",
      "Iteration 321, loss = 1.55083688\n",
      "Validation score: 0.428571\n",
      "Iteration 322, loss = 1.55081562\n",
      "Validation score: 0.428571\n",
      "Iteration 323, loss = 1.55074967\n",
      "Validation score: 0.428571\n",
      "Iteration 324, loss = 1.55072497\n",
      "Validation score: 0.428571\n",
      "Iteration 325, loss = 1.55067667\n",
      "Validation score: 0.428571\n",
      "Iteration 326, loss = 1.55063273\n",
      "Validation score: 0.428571\n",
      "Iteration 327, loss = 1.55058569\n",
      "Validation score: 0.428571\n",
      "Iteration 328, loss = 1.55054254\n",
      "Validation score: 0.428571\n",
      "Iteration 329, loss = 1.55052769\n",
      "Validation score: 0.428571\n",
      "Iteration 330, loss = 1.55046822\n",
      "Validation score: 0.428571\n",
      "Iteration 331, loss = 1.55043004\n",
      "Validation score: 0.428571\n",
      "Iteration 332, loss = 1.55038125\n",
      "Validation score: 0.428571\n",
      "Iteration 333, loss = 1.55035445\n",
      "Validation score: 0.428571\n",
      "Iteration 334, loss = 1.55030831\n",
      "Validation score: 0.428571\n",
      "Iteration 335, loss = 1.55025136\n",
      "Validation score: 0.428571\n",
      "Iteration 336, loss = 1.55021377\n",
      "Validation score: 0.428571\n",
      "Iteration 337, loss = 1.55018483\n",
      "Validation score: 0.428571\n",
      "Iteration 338, loss = 1.55013495\n",
      "Validation score: 0.428571\n",
      "Iteration 339, loss = 1.55009853\n",
      "Validation score: 0.428571\n",
      "Iteration 340, loss = 1.55006595\n",
      "Validation score: 0.428571\n",
      "Iteration 341, loss = 1.55001819\n",
      "Validation score: 0.428571\n",
      "Iteration 342, loss = 1.54998319\n",
      "Validation score: 0.428571\n",
      "Iteration 343, loss = 1.54993969\n",
      "Validation score: 0.428571\n",
      "Iteration 344, loss = 1.54992445\n",
      "Validation score: 0.428571\n",
      "Iteration 345, loss = 1.54985984\n",
      "Validation score: 0.428571\n",
      "Iteration 346, loss = 1.54981666\n",
      "Validation score: 0.428571\n",
      "Iteration 347, loss = 1.54977669\n",
      "Validation score: 0.428571\n",
      "Iteration 348, loss = 1.54973965\n",
      "Validation score: 0.428571\n",
      "Iteration 349, loss = 1.54969445\n",
      "Validation score: 0.428571\n",
      "Iteration 350, loss = 1.54968453\n",
      "Validation score: 0.428571\n",
      "Iteration 351, loss = 1.54962466\n",
      "Validation score: 0.428571\n",
      "Iteration 352, loss = 1.54958408\n",
      "Validation score: 0.428571\n",
      "Validation score did not improve more than tol=0.000000 for 170 consecutive epochs. Setting learning rate to 0.000040\n",
      "Iteration 353, loss = 1.54952508\n",
      "Validation score: 0.428571\n",
      "Iteration 354, loss = 1.54949127\n",
      "Validation score: 0.428571\n",
      "Iteration 355, loss = 1.54946494\n",
      "Validation score: 0.428571\n",
      "Iteration 356, loss = 1.54944345\n",
      "Validation score: 0.428571\n",
      "Iteration 357, loss = 1.54942734\n",
      "Validation score: 0.428571\n",
      "Iteration 358, loss = 1.54941274\n",
      "Validation score: 0.428571\n",
      "Iteration 359, loss = 1.54939820\n",
      "Validation score: 0.428571\n",
      "Iteration 360, loss = 1.54938734\n",
      "Validation score: 0.428571\n",
      "Iteration 361, loss = 1.54938227\n",
      "Validation score: 0.428571\n",
      "Iteration 362, loss = 1.54936734\n",
      "Validation score: 0.428571\n",
      "Iteration 363, loss = 1.54935870\n",
      "Validation score: 0.428571\n",
      "Iteration 364, loss = 1.54935184\n",
      "Validation score: 0.428571\n",
      "Iteration 365, loss = 1.54934525\n",
      "Validation score: 0.428571\n",
      "Iteration 366, loss = 1.54933374\n",
      "Validation score: 0.428571\n",
      "Iteration 367, loss = 1.54932481\n",
      "Validation score: 0.428571\n",
      "Iteration 368, loss = 1.54931751\n",
      "Validation score: 0.428571\n",
      "Iteration 369, loss = 1.54930957\n",
      "Validation score: 0.428571\n",
      "Iteration 370, loss = 1.54930178\n",
      "Validation score: 0.428571\n",
      "Iteration 371, loss = 1.54929164\n",
      "Validation score: 0.428571\n",
      "Iteration 372, loss = 1.54928569\n",
      "Validation score: 0.428571\n",
      "Iteration 373, loss = 1.54927594\n",
      "Validation score: 0.428571\n",
      "Iteration 374, loss = 1.54926899\n",
      "Validation score: 0.428571\n",
      "Iteration 375, loss = 1.54926523\n",
      "Validation score: 0.428571\n",
      "Iteration 376, loss = 1.54925478\n",
      "Validation score: 0.428571\n",
      "Iteration 377, loss = 1.54925157\n",
      "Validation score: 0.428571\n",
      "Iteration 378, loss = 1.54923852\n",
      "Validation score: 0.428571\n",
      "Iteration 379, loss = 1.54923145\n",
      "Validation score: 0.428571\n",
      "Iteration 380, loss = 1.54922071\n",
      "Validation score: 0.428571\n",
      "Iteration 381, loss = 1.54921279\n",
      "Validation score: 0.428571\n",
      "Iteration 382, loss = 1.54920368\n",
      "Validation score: 0.428571\n",
      "Iteration 383, loss = 1.54919607\n",
      "Validation score: 0.428571\n",
      "Iteration 384, loss = 1.54918874\n",
      "Validation score: 0.428571\n",
      "Iteration 385, loss = 1.54917861\n",
      "Validation score: 0.428571\n",
      "Iteration 386, loss = 1.54917111\n",
      "Validation score: 0.428571\n",
      "Iteration 387, loss = 1.54916308\n",
      "Validation score: 0.428571\n",
      "Iteration 388, loss = 1.54915788\n",
      "Validation score: 0.428571\n",
      "Iteration 389, loss = 1.54914658\n",
      "Validation score: 0.428571\n",
      "Iteration 390, loss = 1.54913987\n",
      "Validation score: 0.428571\n",
      "Iteration 391, loss = 1.54913289\n",
      "Validation score: 0.428571\n",
      "Iteration 392, loss = 1.54912566\n",
      "Validation score: 0.428571\n",
      "Iteration 393, loss = 1.54911780\n",
      "Validation score: 0.428571\n",
      "Iteration 394, loss = 1.54910977\n",
      "Validation score: 0.428571\n",
      "Iteration 395, loss = 1.54910043\n",
      "Validation score: 0.428571\n",
      "Iteration 396, loss = 1.54909366\n",
      "Validation score: 0.428571\n",
      "Iteration 397, loss = 1.54908575\n",
      "Validation score: 0.428571\n",
      "Iteration 398, loss = 1.54907573\n",
      "Validation score: 0.428571\n",
      "Iteration 399, loss = 1.54906861\n",
      "Validation score: 0.428571\n",
      "Iteration 400, loss = 1.54905827\n",
      "Validation score: 0.428571\n",
      "Iteration 401, loss = 1.54905270\n",
      "Validation score: 0.428571\n",
      "Iteration 402, loss = 1.54904382\n",
      "Validation score: 0.428571\n",
      "Iteration 403, loss = 1.54903506\n",
      "Validation score: 0.428571\n",
      "Iteration 404, loss = 1.54902759\n",
      "Validation score: 0.428571\n",
      "Iteration 405, loss = 1.54901810\n",
      "Validation score: 0.428571\n",
      "Iteration 406, loss = 1.54901247\n",
      "Validation score: 0.428571\n",
      "Iteration 407, loss = 1.54900161\n",
      "Validation score: 0.428571\n",
      "Iteration 408, loss = 1.54899519\n",
      "Validation score: 0.428571\n",
      "Iteration 409, loss = 1.54898649\n",
      "Validation score: 0.428571\n",
      "Iteration 410, loss = 1.54898074\n",
      "Validation score: 0.428571\n",
      "Iteration 411, loss = 1.54896985\n",
      "Validation score: 0.428571\n",
      "Iteration 412, loss = 1.54896392\n",
      "Validation score: 0.428571\n",
      "Iteration 413, loss = 1.54895929\n",
      "Validation score: 0.428571\n",
      "Iteration 414, loss = 1.54894641\n",
      "Validation score: 0.428571\n",
      "Iteration 415, loss = 1.54893823\n",
      "Validation score: 0.428571\n",
      "Iteration 416, loss = 1.54892953\n",
      "Validation score: 0.428571\n",
      "Iteration 417, loss = 1.54892612\n",
      "Validation score: 0.428571\n",
      "Iteration 418, loss = 1.54891444\n",
      "Validation score: 0.428571\n",
      "Iteration 419, loss = 1.54890448\n",
      "Validation score: 0.428571\n",
      "Iteration 420, loss = 1.54890014\n",
      "Validation score: 0.428571\n",
      "Iteration 421, loss = 1.54888899\n",
      "Validation score: 0.428571\n",
      "Iteration 422, loss = 1.54888350\n",
      "Validation score: 0.428571\n",
      "Iteration 423, loss = 1.54887167\n",
      "Validation score: 0.428571\n",
      "Iteration 424, loss = 1.54886841\n",
      "Validation score: 0.428571\n",
      "Iteration 425, loss = 1.54885843\n",
      "Validation score: 0.428571\n",
      "Iteration 426, loss = 1.54884742\n",
      "Validation score: 0.428571\n",
      "Iteration 427, loss = 1.54884003\n",
      "Validation score: 0.428571\n",
      "Iteration 428, loss = 1.54883010\n",
      "Validation score: 0.428571\n",
      "Iteration 429, loss = 1.54882199\n",
      "Validation score: 0.428571\n",
      "Iteration 430, loss = 1.54882071\n",
      "Validation score: 0.428571\n",
      "Iteration 431, loss = 1.54880800\n",
      "Validation score: 0.428571\n",
      "Iteration 432, loss = 1.54879901\n",
      "Validation score: 0.428571\n",
      "Iteration 433, loss = 1.54878914\n",
      "Validation score: 0.428571\n",
      "Iteration 434, loss = 1.54878170\n",
      "Validation score: 0.428571\n",
      "Iteration 435, loss = 1.54877475\n",
      "Validation score: 0.428571\n",
      "Iteration 436, loss = 1.54876816\n",
      "Validation score: 0.428571\n",
      "Iteration 437, loss = 1.54875893\n",
      "Validation score: 0.428571\n",
      "Iteration 438, loss = 1.54874772\n",
      "Validation score: 0.428571\n",
      "Iteration 439, loss = 1.54873986\n",
      "Validation score: 0.428571\n",
      "Iteration 440, loss = 1.54873698\n",
      "Validation score: 0.428571\n",
      "Iteration 441, loss = 1.54872460\n",
      "Validation score: 0.428571\n",
      "Iteration 442, loss = 1.54871955\n",
      "Validation score: 0.428571\n",
      "Iteration 443, loss = 1.54871059\n",
      "Validation score: 0.428571\n",
      "Iteration 444, loss = 1.54870422\n",
      "Validation score: 0.428571\n",
      "Iteration 445, loss = 1.54869228\n",
      "Validation score: 0.428571\n",
      "Iteration 446, loss = 1.54868484\n",
      "Validation score: 0.428571\n",
      "Iteration 447, loss = 1.54867520\n",
      "Validation score: 0.428571\n",
      "Iteration 448, loss = 1.54866887\n",
      "Validation score: 0.428571\n",
      "Iteration 449, loss = 1.54866069\n",
      "Validation score: 0.428571\n",
      "Iteration 450, loss = 1.54865407\n",
      "Validation score: 0.428571\n",
      "Iteration 451, loss = 1.54864356\n",
      "Validation score: 0.428571\n",
      "Iteration 452, loss = 1.54863869\n",
      "Validation score: 0.428571\n",
      "Iteration 453, loss = 1.54863151\n",
      "Validation score: 0.428571\n",
      "Iteration 454, loss = 1.54861740\n",
      "Validation score: 0.428571\n",
      "Iteration 455, loss = 1.54861153\n",
      "Validation score: 0.428571\n",
      "Iteration 456, loss = 1.54860356\n",
      "Validation score: 0.428571\n",
      "Iteration 457, loss = 1.54859498\n",
      "Validation score: 0.428571\n",
      "Iteration 458, loss = 1.54858654\n",
      "Validation score: 0.428571\n",
      "Iteration 459, loss = 1.54858027\n",
      "Validation score: 0.428571\n",
      "Iteration 460, loss = 1.54857180\n",
      "Validation score: 0.428571\n",
      "Iteration 461, loss = 1.54856450\n",
      "Validation score: 0.428571\n",
      "Iteration 462, loss = 1.54856031\n",
      "Validation score: 0.428571\n",
      "Iteration 463, loss = 1.54854471\n",
      "Validation score: 0.428571\n",
      "Iteration 464, loss = 1.54853840\n",
      "Validation score: 0.428571\n",
      "Iteration 465, loss = 1.54852801\n",
      "Validation score: 0.428571\n",
      "Iteration 466, loss = 1.54852428\n",
      "Validation score: 0.428571\n",
      "Iteration 467, loss = 1.54851569\n",
      "Validation score: 0.428571\n",
      "Iteration 468, loss = 1.54850574\n",
      "Validation score: 0.428571\n",
      "Iteration 469, loss = 1.54850139\n",
      "Validation score: 0.428571\n",
      "Iteration 470, loss = 1.54848939\n",
      "Validation score: 0.428571\n",
      "Iteration 471, loss = 1.54848101\n",
      "Validation score: 0.428571\n",
      "Iteration 472, loss = 1.54847278\n",
      "Validation score: 0.428571\n",
      "Iteration 473, loss = 1.54846665\n",
      "Validation score: 0.428571\n",
      "Iteration 474, loss = 1.54845585\n",
      "Validation score: 0.428571\n",
      "Iteration 475, loss = 1.54844767\n",
      "Validation score: 0.428571\n",
      "Iteration 476, loss = 1.54843856\n",
      "Validation score: 0.428571\n",
      "Iteration 477, loss = 1.54843328\n",
      "Validation score: 0.428571\n",
      "Iteration 478, loss = 1.54842321\n",
      "Validation score: 0.428571\n",
      "Iteration 479, loss = 1.54841641\n",
      "Validation score: 0.428571\n",
      "Iteration 480, loss = 1.54840818\n",
      "Validation score: 0.428571\n",
      "Iteration 481, loss = 1.54839910\n",
      "Validation score: 0.428571\n",
      "Iteration 482, loss = 1.54839189\n",
      "Validation score: 0.428571\n",
      "Iteration 483, loss = 1.54838334\n",
      "Validation score: 0.428571\n",
      "Iteration 484, loss = 1.54837399\n",
      "Validation score: 0.428571\n",
      "Iteration 485, loss = 1.54836974\n",
      "Validation score: 0.428571\n",
      "Iteration 486, loss = 1.54835867\n",
      "Validation score: 0.428571\n",
      "Iteration 487, loss = 1.54835102\n",
      "Validation score: 0.428571\n",
      "Iteration 488, loss = 1.54834583\n",
      "Validation score: 0.428571\n",
      "Iteration 489, loss = 1.54833219\n",
      "Validation score: 0.428571\n",
      "Iteration 490, loss = 1.54832504\n",
      "Validation score: 0.428571\n",
      "Iteration 491, loss = 1.54831754\n",
      "Validation score: 0.428571\n",
      "Iteration 492, loss = 1.54830930\n",
      "Validation score: 0.428571\n",
      "Iteration 493, loss = 1.54830075\n",
      "Validation score: 0.428571\n",
      "Iteration 494, loss = 1.54829319\n",
      "Validation score: 0.428571\n",
      "Iteration 495, loss = 1.54828539\n",
      "Validation score: 0.428571\n",
      "Iteration 496, loss = 1.54827579\n",
      "Validation score: 0.428571\n",
      "Iteration 497, loss = 1.54827007\n",
      "Validation score: 0.428571\n",
      "Iteration 498, loss = 1.54825900\n",
      "Validation score: 0.428571\n",
      "Iteration 499, loss = 1.54825559\n",
      "Validation score: 0.428571\n",
      "Iteration 500, loss = 1.54824452\n",
      "Validation score: 0.428571\n",
      "Iteration 501, loss = 1.54823620\n",
      "Validation score: 0.428571\n",
      "Iteration 502, loss = 1.54822935\n",
      "Validation score: 0.428571\n",
      "Iteration 503, loss = 1.54822103\n",
      "Validation score: 0.428571\n",
      "Iteration 504, loss = 1.54820987\n",
      "Validation score: 0.428571\n",
      "Iteration 505, loss = 1.54820433\n",
      "Validation score: 0.428571\n",
      "Iteration 506, loss = 1.54819437\n",
      "Validation score: 0.428571\n",
      "Iteration 507, loss = 1.54818523\n",
      "Validation score: 0.428571\n",
      "Iteration 508, loss = 1.54818138\n",
      "Validation score: 0.428571\n",
      "Iteration 509, loss = 1.54817151\n",
      "Validation score: 0.428571\n",
      "Iteration 510, loss = 1.54816354\n",
      "Validation score: 0.428571\n",
      "Iteration 511, loss = 1.54815727\n",
      "Validation score: 0.428571\n",
      "Iteration 512, loss = 1.54814693\n",
      "Validation score: 0.428571\n",
      "Iteration 513, loss = 1.54813823\n",
      "Validation score: 0.428571\n",
      "Iteration 514, loss = 1.54813178\n",
      "Validation score: 0.428571\n",
      "Iteration 515, loss = 1.54812294\n",
      "Validation score: 0.428571\n",
      "Iteration 516, loss = 1.54811237\n",
      "Validation score: 0.428571\n",
      "Iteration 517, loss = 1.54810633\n",
      "Validation score: 0.428571\n",
      "Iteration 518, loss = 1.54809754\n",
      "Validation score: 0.428571\n",
      "Iteration 519, loss = 1.54809091\n",
      "Validation score: 0.428571\n",
      "Iteration 520, loss = 1.54808321\n",
      "Validation score: 0.428571\n",
      "Iteration 521, loss = 1.54807378\n",
      "Validation score: 0.428571\n",
      "Iteration 522, loss = 1.54806692\n",
      "Validation score: 0.428571\n",
      "Iteration 523, loss = 1.54806000\n",
      "Validation score: 0.428571\n",
      "Validation score did not improve more than tol=0.000000 for 170 consecutive epochs. Setting learning rate to 0.000008\n",
      "Iteration 524, loss = 1.54804656\n",
      "Validation score: 0.428571\n",
      "Iteration 525, loss = 1.54804008\n",
      "Validation score: 0.428571\n",
      "Iteration 526, loss = 1.54803559\n",
      "Validation score: 0.428571\n",
      "Iteration 527, loss = 1.54803296\n",
      "Validation score: 0.428571\n",
      "Iteration 528, loss = 1.54802805\n",
      "Validation score: 0.428571\n",
      "Iteration 529, loss = 1.54802531\n",
      "Validation score: 0.428571\n",
      "Iteration 530, loss = 1.54802283\n",
      "Validation score: 0.428571\n",
      "Iteration 531, loss = 1.54802020\n",
      "Validation score: 0.428571\n",
      "Iteration 532, loss = 1.54801795\n",
      "Validation score: 0.428571\n",
      "Iteration 533, loss = 1.54801719\n",
      "Validation score: 0.428571\n",
      "Iteration 534, loss = 1.54801451\n",
      "Validation score: 0.428571\n",
      "Iteration 535, loss = 1.54801387\n",
      "Validation score: 0.428571\n",
      "Iteration 536, loss = 1.54801121\n",
      "Validation score: 0.428571\n",
      "Iteration 537, loss = 1.54800940\n",
      "Validation score: 0.428571\n",
      "Iteration 538, loss = 1.54800715\n",
      "Validation score: 0.428571\n",
      "Iteration 539, loss = 1.54800575\n",
      "Validation score: 0.428571\n",
      "Iteration 540, loss = 1.54800376\n",
      "Validation score: 0.428571\n",
      "Iteration 541, loss = 1.54800248\n",
      "Validation score: 0.428571\n",
      "Iteration 542, loss = 1.54800105\n",
      "Validation score: 0.428571\n",
      "Iteration 543, loss = 1.54799895\n",
      "Validation score: 0.428571\n",
      "Iteration 544, loss = 1.54799781\n",
      "Validation score: 0.428571\n",
      "Iteration 545, loss = 1.54799568\n",
      "Validation score: 0.428571\n",
      "Iteration 546, loss = 1.54799442\n",
      "Validation score: 0.428571\n",
      "Iteration 547, loss = 1.54799232\n",
      "Validation score: 0.428571\n",
      "Iteration 548, loss = 1.54799098\n",
      "Validation score: 0.428571\n",
      "Iteration 549, loss = 1.54798940\n",
      "Validation score: 0.428571\n",
      "Iteration 550, loss = 1.54798797\n",
      "Validation score: 0.428571\n",
      "Iteration 551, loss = 1.54798616\n",
      "Validation score: 0.428571\n",
      "Iteration 552, loss = 1.54798491\n",
      "Validation score: 0.428571\n",
      "Iteration 553, loss = 1.54798269\n",
      "Validation score: 0.428571\n",
      "Iteration 554, loss = 1.54798193\n",
      "Validation score: 0.428571\n",
      "Iteration 555, loss = 1.54797953\n",
      "Validation score: 0.428571\n",
      "Iteration 556, loss = 1.54797793\n",
      "Validation score: 0.428571\n",
      "Iteration 557, loss = 1.54797597\n",
      "Validation score: 0.428571\n",
      "Iteration 558, loss = 1.54797501\n",
      "Validation score: 0.428571\n",
      "Iteration 559, loss = 1.54797282\n",
      "Validation score: 0.428571\n",
      "Iteration 560, loss = 1.54797165\n",
      "Validation score: 0.428571\n",
      "Iteration 561, loss = 1.54796972\n",
      "Validation score: 0.428571\n",
      "Iteration 562, loss = 1.54796762\n",
      "Validation score: 0.428571\n",
      "Iteration 563, loss = 1.54796657\n",
      "Validation score: 0.428571\n",
      "Iteration 564, loss = 1.54796450\n",
      "Validation score: 0.428571\n",
      "Iteration 565, loss = 1.54796336\n",
      "Validation score: 0.428571\n",
      "Iteration 566, loss = 1.54796178\n",
      "Validation score: 0.428571\n",
      "Iteration 567, loss = 1.54795997\n",
      "Validation score: 0.428571\n",
      "Iteration 568, loss = 1.54795875\n",
      "Validation score: 0.428571\n",
      "Iteration 569, loss = 1.54795650\n",
      "Validation score: 0.428571\n",
      "Iteration 570, loss = 1.54795484\n",
      "Validation score: 0.428571\n",
      "Iteration 571, loss = 1.54795320\n",
      "Validation score: 0.428571\n",
      "Iteration 572, loss = 1.54795163\n",
      "Validation score: 0.428571\n",
      "Iteration 573, loss = 1.54795107\n",
      "Validation score: 0.428571\n",
      "Iteration 574, loss = 1.54794944\n",
      "Validation score: 0.428571\n",
      "Iteration 575, loss = 1.54794760\n",
      "Validation score: 0.428571\n",
      "Iteration 576, loss = 1.54794538\n",
      "Validation score: 0.428571\n",
      "Iteration 577, loss = 1.54794380\n",
      "Validation score: 0.428571\n",
      "Iteration 578, loss = 1.54794261\n",
      "Validation score: 0.428571\n",
      "Iteration 579, loss = 1.54794091\n",
      "Validation score: 0.428571\n",
      "Iteration 580, loss = 1.54793919\n",
      "Validation score: 0.428571\n",
      "Iteration 581, loss = 1.54793709\n",
      "Validation score: 0.428571\n",
      "Iteration 582, loss = 1.54793545\n",
      "Validation score: 0.428571\n",
      "Iteration 583, loss = 1.54793519\n",
      "Validation score: 0.428571\n",
      "Iteration 584, loss = 1.54793274\n",
      "Validation score: 0.428571\n",
      "Iteration 585, loss = 1.54793125\n",
      "Validation score: 0.428571\n",
      "Iteration 586, loss = 1.54792927\n",
      "Validation score: 0.428571\n",
      "Iteration 587, loss = 1.54792757\n",
      "Validation score: 0.428571\n",
      "Iteration 588, loss = 1.54792611\n",
      "Validation score: 0.428571\n",
      "Iteration 589, loss = 1.54792524\n",
      "Validation score: 0.428571\n",
      "Iteration 590, loss = 1.54792264\n",
      "Validation score: 0.428571\n",
      "Iteration 591, loss = 1.54792100\n",
      "Validation score: 0.428571\n",
      "Iteration 592, loss = 1.54792004\n",
      "Validation score: 0.428571\n",
      "Iteration 593, loss = 1.54791788\n",
      "Validation score: 0.428571\n",
      "Iteration 594, loss = 1.54791619\n",
      "Validation score: 0.428571\n",
      "Iteration 595, loss = 1.54791455\n",
      "Validation score: 0.428571\n",
      "Iteration 596, loss = 1.54791371\n",
      "Validation score: 0.428571\n",
      "Iteration 597, loss = 1.54791163\n",
      "Validation score: 0.428571\n",
      "Iteration 598, loss = 1.54791041\n",
      "Validation score: 0.428571\n",
      "Iteration 599, loss = 1.54790830\n",
      "Validation score: 0.428571\n",
      "Iteration 600, loss = 1.54790673\n",
      "Validation score: 0.428571\n",
      "Iteration 601, loss = 1.54790504\n",
      "Validation score: 0.428571\n",
      "Iteration 602, loss = 1.54790358\n",
      "Validation score: 0.428571\n",
      "Iteration 603, loss = 1.54790185\n",
      "Validation score: 0.428571\n",
      "Iteration 604, loss = 1.54790016\n",
      "Validation score: 0.428571\n",
      "Iteration 605, loss = 1.54789879\n",
      "Validation score: 0.428571\n",
      "Iteration 606, loss = 1.54789721\n",
      "Validation score: 0.428571\n",
      "Iteration 607, loss = 1.54789555\n",
      "Validation score: 0.428571\n",
      "Iteration 608, loss = 1.54789353\n",
      "Validation score: 0.428571\n",
      "Iteration 609, loss = 1.54789251\n",
      "Validation score: 0.428571\n",
      "Iteration 610, loss = 1.54789050\n",
      "Validation score: 0.428571\n",
      "Iteration 611, loss = 1.54788933\n",
      "Validation score: 0.428571\n",
      "Iteration 612, loss = 1.54788691\n",
      "Validation score: 0.428571\n",
      "Iteration 613, loss = 1.54788548\n",
      "Validation score: 0.428571\n",
      "Iteration 614, loss = 1.54788402\n",
      "Validation score: 0.428571\n",
      "Iteration 615, loss = 1.54788235\n",
      "Validation score: 0.428571\n",
      "Iteration 616, loss = 1.54788069\n",
      "Validation score: 0.428571\n",
      "Iteration 617, loss = 1.54787973\n",
      "Validation score: 0.428571\n",
      "Iteration 618, loss = 1.54787719\n",
      "Validation score: 0.428571\n",
      "Iteration 619, loss = 1.54787649\n",
      "Validation score: 0.428571\n",
      "Iteration 620, loss = 1.54787488\n",
      "Validation score: 0.428571\n",
      "Iteration 621, loss = 1.54787257\n",
      "Validation score: 0.428571\n",
      "Iteration 622, loss = 1.54787106\n",
      "Validation score: 0.428571\n",
      "Iteration 623, loss = 1.54786913\n",
      "Validation score: 0.428571\n",
      "Iteration 624, loss = 1.54786770\n",
      "Validation score: 0.428571\n",
      "Iteration 625, loss = 1.54786674\n",
      "Validation score: 0.428571\n",
      "Iteration 626, loss = 1.54786455\n",
      "Validation score: 0.428571\n",
      "Iteration 627, loss = 1.54786347\n",
      "Validation score: 0.428571\n",
      "Iteration 628, loss = 1.54786212\n",
      "Validation score: 0.428571\n",
      "Iteration 629, loss = 1.54786002\n",
      "Validation score: 0.428571\n",
      "Iteration 630, loss = 1.54785830\n",
      "Validation score: 0.428571\n",
      "Iteration 631, loss = 1.54785719\n",
      "Validation score: 0.428571\n",
      "Iteration 632, loss = 1.54785503\n",
      "Validation score: 0.428571\n",
      "Iteration 633, loss = 1.54785375\n",
      "Validation score: 0.428571\n",
      "Iteration 634, loss = 1.54785240\n",
      "Validation score: 0.428571\n",
      "Iteration 635, loss = 1.54785024\n",
      "Validation score: 0.428571\n",
      "Iteration 636, loss = 1.54784858\n",
      "Validation score: 0.428571\n",
      "Iteration 637, loss = 1.54784677\n",
      "Validation score: 0.428571\n",
      "Iteration 638, loss = 1.54784613\n",
      "Validation score: 0.428571\n",
      "Iteration 639, loss = 1.54784408\n",
      "Validation score: 0.428571\n",
      "Iteration 640, loss = 1.54784216\n",
      "Validation score: 0.428571\n",
      "Iteration 641, loss = 1.54784073\n",
      "Validation score: 0.428571\n",
      "Iteration 642, loss = 1.54783892\n",
      "Validation score: 0.428571\n",
      "Iteration 643, loss = 1.54783737\n",
      "Validation score: 0.428571\n",
      "Iteration 644, loss = 1.54783617\n",
      "Validation score: 0.428571\n",
      "Iteration 645, loss = 1.54783375\n",
      "Validation score: 0.428571\n",
      "Iteration 646, loss = 1.54783279\n",
      "Validation score: 0.428571\n",
      "Iteration 647, loss = 1.54783118\n",
      "Validation score: 0.428571\n",
      "Iteration 648, loss = 1.54782919\n",
      "Validation score: 0.428571\n",
      "Iteration 649, loss = 1.54782756\n",
      "Validation score: 0.428571\n",
      "Iteration 650, loss = 1.54782607\n",
      "Validation score: 0.428571\n",
      "Iteration 651, loss = 1.54782458\n",
      "Validation score: 0.428571\n",
      "Iteration 652, loss = 1.54782283\n",
      "Validation score: 0.428571\n",
      "Iteration 653, loss = 1.54782114\n",
      "Validation score: 0.428571\n",
      "Iteration 654, loss = 1.54781939\n",
      "Validation score: 0.428571\n",
      "Iteration 655, loss = 1.54781804\n",
      "Validation score: 0.428571\n",
      "Iteration 656, loss = 1.54781618\n",
      "Validation score: 0.428571\n",
      "Iteration 657, loss = 1.54781477\n",
      "Validation score: 0.428571\n",
      "Iteration 658, loss = 1.54781361\n",
      "Validation score: 0.428571\n",
      "Iteration 659, loss = 1.54781124\n",
      "Validation score: 0.428571\n",
      "Iteration 660, loss = 1.54781010\n",
      "Validation score: 0.428571\n",
      "Iteration 661, loss = 1.54780829\n",
      "Validation score: 0.428571\n",
      "Iteration 662, loss = 1.54780678\n",
      "Validation score: 0.428571\n",
      "Iteration 663, loss = 1.54780526\n",
      "Validation score: 0.428571\n",
      "Iteration 664, loss = 1.54780365\n",
      "Validation score: 0.428571\n",
      "Iteration 665, loss = 1.54780208\n",
      "Validation score: 0.428571\n",
      "Iteration 666, loss = 1.54780041\n",
      "Validation score: 0.428571\n",
      "Iteration 667, loss = 1.54779857\n",
      "Validation score: 0.428571\n",
      "Iteration 668, loss = 1.54779723\n",
      "Validation score: 0.428571\n",
      "Iteration 669, loss = 1.54779545\n",
      "Validation score: 0.428571\n",
      "Iteration 670, loss = 1.54779382\n",
      "Validation score: 0.428571\n",
      "Iteration 671, loss = 1.54779247\n",
      "Validation score: 0.428571\n",
      "Iteration 672, loss = 1.54779139\n",
      "Validation score: 0.428571\n",
      "Iteration 673, loss = 1.54778912\n",
      "Validation score: 0.428571\n",
      "Iteration 674, loss = 1.54778777\n",
      "Validation score: 0.428571\n",
      "Iteration 675, loss = 1.54778588\n",
      "Validation score: 0.428571\n",
      "Iteration 676, loss = 1.54778488\n",
      "Validation score: 0.428571\n",
      "Iteration 677, loss = 1.54778284\n",
      "Validation score: 0.428571\n",
      "Iteration 678, loss = 1.54778082\n",
      "Validation score: 0.428571\n",
      "Iteration 679, loss = 1.54777934\n",
      "Validation score: 0.428571\n",
      "Iteration 680, loss = 1.54777794\n",
      "Validation score: 0.428571\n",
      "Iteration 681, loss = 1.54777642\n",
      "Validation score: 0.428571\n",
      "Iteration 682, loss = 1.54777461\n",
      "Validation score: 0.428571\n",
      "Iteration 683, loss = 1.54777315\n",
      "Validation score: 0.428571\n",
      "Iteration 684, loss = 1.54777172\n",
      "Validation score: 0.428571\n",
      "Iteration 685, loss = 1.54777026\n",
      "Validation score: 0.428571\n",
      "Iteration 686, loss = 1.54776845\n",
      "Validation score: 0.428571\n",
      "Iteration 687, loss = 1.54776667\n",
      "Validation score: 0.428571\n",
      "Iteration 688, loss = 1.54776532\n",
      "Validation score: 0.428571\n",
      "Iteration 689, loss = 1.54776331\n",
      "Validation score: 0.428571\n",
      "Iteration 690, loss = 1.54776258\n",
      "Validation score: 0.428571\n",
      "Iteration 691, loss = 1.54776097\n",
      "Validation score: 0.428571\n",
      "Iteration 692, loss = 1.54775911\n",
      "Validation score: 0.428571\n",
      "Iteration 693, loss = 1.54775794\n",
      "Validation score: 0.428571\n",
      "Iteration 694, loss = 1.54775575\n",
      "Validation score: 0.428571\n",
      "Validation score did not improve more than tol=0.000000 for 170 consecutive epochs. Setting learning rate to 0.000002\n",
      "Iteration 695, loss = 1.54775306\n",
      "Validation score: 0.428571\n",
      "Iteration 696, loss = 1.54775178\n",
      "Validation score: 0.428571\n",
      "Iteration 697, loss = 1.54775090\n",
      "Validation score: 0.428571\n",
      "Iteration 698, loss = 1.54775043\n",
      "Validation score: 0.428571\n",
      "Iteration 699, loss = 1.54774956\n",
      "Validation score: 0.428571\n",
      "Iteration 700, loss = 1.54774906\n",
      "Validation score: 0.428571\n",
      "Iteration 701, loss = 1.54774845\n",
      "Validation score: 0.428571\n",
      "Iteration 702, loss = 1.54774810\n",
      "Validation score: 0.428571\n",
      "Iteration 703, loss = 1.54774757\n",
      "Validation score: 0.428571\n",
      "Iteration 704, loss = 1.54774722\n",
      "Validation score: 0.428571\n",
      "Iteration 705, loss = 1.54774687\n",
      "Validation score: 0.428571\n",
      "Iteration 706, loss = 1.54774649\n",
      "Validation score: 0.428571\n",
      "Iteration 707, loss = 1.54774623\n",
      "Validation score: 0.428571\n",
      "Iteration 708, loss = 1.54774571\n",
      "Validation score: 0.428571\n",
      "Iteration 709, loss = 1.54774547\n",
      "Validation score: 0.428571\n",
      "Iteration 710, loss = 1.54774518\n",
      "Validation score: 0.428571\n",
      "Iteration 711, loss = 1.54774500\n",
      "Validation score: 0.428571\n",
      "Iteration 712, loss = 1.54774465\n",
      "Validation score: 0.428571\n",
      "Iteration 713, loss = 1.54774439\n",
      "Validation score: 0.428571\n",
      "Iteration 714, loss = 1.54774387\n",
      "Validation score: 0.428571\n",
      "Iteration 715, loss = 1.54774369\n",
      "Validation score: 0.428571\n",
      "Iteration 716, loss = 1.54774343\n",
      "Validation score: 0.428571\n",
      "Iteration 717, loss = 1.54774290\n",
      "Validation score: 0.428571\n",
      "Iteration 718, loss = 1.54774267\n",
      "Validation score: 0.428571\n",
      "Iteration 719, loss = 1.54774232\n",
      "Validation score: 0.428571\n",
      "Iteration 720, loss = 1.54774211\n",
      "Validation score: 0.428571\n",
      "Iteration 721, loss = 1.54774159\n",
      "Validation score: 0.428571\n",
      "Iteration 722, loss = 1.54774127\n",
      "Validation score: 0.428571\n",
      "Iteration 723, loss = 1.54774095\n",
      "Validation score: 0.428571\n",
      "Iteration 724, loss = 1.54774086\n",
      "Validation score: 0.428571\n",
      "Iteration 725, loss = 1.54774030\n",
      "Validation score: 0.428571\n",
      "Iteration 726, loss = 1.54774010\n",
      "Validation score: 0.428571\n",
      "Iteration 727, loss = 1.54773975\n",
      "Validation score: 0.428571\n",
      "Iteration 728, loss = 1.54773934\n",
      "Validation score: 0.428571\n",
      "Iteration 729, loss = 1.54773917\n",
      "Validation score: 0.428571\n",
      "Iteration 730, loss = 1.54773882\n",
      "Validation score: 0.428571\n",
      "Iteration 731, loss = 1.54773844\n",
      "Validation score: 0.428571\n",
      "Iteration 732, loss = 1.54773812\n",
      "Validation score: 0.428571\n",
      "Iteration 733, loss = 1.54773782\n",
      "Validation score: 0.428571\n",
      "Iteration 734, loss = 1.54773750\n",
      "Validation score: 0.428571\n",
      "Iteration 735, loss = 1.54773724\n",
      "Validation score: 0.428571\n",
      "Iteration 736, loss = 1.54773683\n",
      "Validation score: 0.428571\n",
      "Iteration 737, loss = 1.54773666\n",
      "Validation score: 0.428571\n",
      "Iteration 738, loss = 1.54773636\n",
      "Validation score: 0.428571\n",
      "Iteration 739, loss = 1.54773598\n",
      "Validation score: 0.428571\n",
      "Iteration 740, loss = 1.54773555\n",
      "Validation score: 0.428571\n",
      "Iteration 741, loss = 1.54773517\n",
      "Validation score: 0.428571\n",
      "Iteration 742, loss = 1.54773490\n",
      "Validation score: 0.428571\n",
      "Iteration 743, loss = 1.54773470\n",
      "Validation score: 0.428571\n",
      "Iteration 744, loss = 1.54773435\n",
      "Validation score: 0.428571\n",
      "Iteration 745, loss = 1.54773400\n",
      "Validation score: 0.428571\n",
      "Iteration 746, loss = 1.54773374\n",
      "Validation score: 0.428571\n",
      "Iteration 747, loss = 1.54773353\n",
      "Validation score: 0.428571\n",
      "Iteration 748, loss = 1.54773304\n",
      "Validation score: 0.428571\n",
      "Iteration 749, loss = 1.54773280\n",
      "Validation score: 0.428571\n",
      "Iteration 750, loss = 1.54773245\n",
      "Validation score: 0.428571\n",
      "Iteration 751, loss = 1.54773198\n",
      "Validation score: 0.428571\n",
      "Iteration 752, loss = 1.54773184\n",
      "Validation score: 0.428571\n",
      "Iteration 753, loss = 1.54773134\n",
      "Validation score: 0.428571\n",
      "Iteration 754, loss = 1.54773131\n",
      "Validation score: 0.428571\n",
      "Iteration 755, loss = 1.54773082\n",
      "Validation score: 0.428571\n",
      "Iteration 756, loss = 1.54773047\n",
      "Validation score: 0.428571\n",
      "Iteration 757, loss = 1.54773023\n",
      "Validation score: 0.428571\n",
      "Iteration 758, loss = 1.54772982\n",
      "Validation score: 0.428571\n",
      "Iteration 759, loss = 1.54772953\n",
      "Validation score: 0.428571\n",
      "Iteration 760, loss = 1.54772915\n",
      "Validation score: 0.428571\n",
      "Iteration 761, loss = 1.54772892\n",
      "Validation score: 0.428571\n",
      "Iteration 762, loss = 1.54772854\n",
      "Validation score: 0.428571\n",
      "Iteration 763, loss = 1.54772819\n",
      "Validation score: 0.428571\n",
      "Iteration 764, loss = 1.54772790\n",
      "Validation score: 0.428571\n",
      "Iteration 765, loss = 1.54772761\n",
      "Validation score: 0.428571\n",
      "Iteration 766, loss = 1.54772731\n",
      "Validation score: 0.428571\n",
      "Iteration 767, loss = 1.54772702\n",
      "Validation score: 0.428571\n",
      "Iteration 768, loss = 1.54772676\n",
      "Validation score: 0.428571\n",
      "Iteration 769, loss = 1.54772632\n",
      "Validation score: 0.428571\n",
      "Iteration 770, loss = 1.54772600\n",
      "Validation score: 0.428571\n",
      "Iteration 771, loss = 1.54772568\n",
      "Validation score: 0.428571\n",
      "Iteration 772, loss = 1.54772553\n",
      "Validation score: 0.428571\n",
      "Iteration 773, loss = 1.54772527\n",
      "Validation score: 0.428571\n",
      "Iteration 774, loss = 1.54772480\n",
      "Validation score: 0.428571\n",
      "Iteration 775, loss = 1.54772440\n",
      "Validation score: 0.428571\n",
      "Iteration 776, loss = 1.54772399\n",
      "Validation score: 0.428571\n",
      "Iteration 777, loss = 1.54772364\n",
      "Validation score: 0.428571\n",
      "Iteration 778, loss = 1.54772329\n",
      "Validation score: 0.428571\n",
      "Iteration 779, loss = 1.54772317\n",
      "Validation score: 0.428571\n",
      "Iteration 780, loss = 1.54772270\n",
      "Validation score: 0.428571\n",
      "Iteration 781, loss = 1.54772235\n",
      "Validation score: 0.428571\n",
      "Iteration 782, loss = 1.54772212\n",
      "Validation score: 0.428571\n",
      "Iteration 783, loss = 1.54772177\n",
      "Validation score: 0.428571\n",
      "Iteration 784, loss = 1.54772142\n",
      "Validation score: 0.428571\n",
      "Iteration 785, loss = 1.54772107\n",
      "Validation score: 0.428571\n",
      "Iteration 786, loss = 1.54772080\n",
      "Validation score: 0.428571\n",
      "Iteration 787, loss = 1.54772060\n",
      "Validation score: 0.428571\n",
      "Iteration 788, loss = 1.54772016\n",
      "Validation score: 0.428571\n",
      "Iteration 789, loss = 1.54771984\n",
      "Validation score: 0.428571\n",
      "Iteration 790, loss = 1.54771946\n",
      "Validation score: 0.428571\n",
      "Iteration 791, loss = 1.54771932\n",
      "Validation score: 0.428571\n",
      "Iteration 792, loss = 1.54771891\n",
      "Validation score: 0.428571\n",
      "Iteration 793, loss = 1.54771850\n",
      "Validation score: 0.428571\n",
      "Iteration 794, loss = 1.54771818\n",
      "Validation score: 0.428571\n",
      "Iteration 795, loss = 1.54771797\n",
      "Validation score: 0.428571\n",
      "Iteration 796, loss = 1.54771771\n",
      "Validation score: 0.428571\n",
      "Iteration 797, loss = 1.54771733\n",
      "Validation score: 0.428571\n",
      "Iteration 798, loss = 1.54771689\n",
      "Validation score: 0.428571\n",
      "Iteration 799, loss = 1.54771657\n",
      "Validation score: 0.428571\n",
      "Iteration 800, loss = 1.54771640\n",
      "Validation score: 0.428571\n",
      "Iteration 801, loss = 1.54771587\n",
      "Validation score: 0.428571\n",
      "Iteration 802, loss = 1.54771558\n",
      "Validation score: 0.428571\n",
      "Iteration 803, loss = 1.54771532\n",
      "Validation score: 0.428571\n",
      "Iteration 804, loss = 1.54771500\n",
      "Validation score: 0.428571\n",
      "Iteration 805, loss = 1.54771464\n",
      "Validation score: 0.428571\n",
      "Iteration 806, loss = 1.54771424\n",
      "Validation score: 0.428571\n",
      "Iteration 807, loss = 1.54771406\n",
      "Validation score: 0.428571\n",
      "Iteration 808, loss = 1.54771365\n",
      "Validation score: 0.428571\n",
      "Iteration 809, loss = 1.54771339\n",
      "Validation score: 0.428571\n",
      "Iteration 810, loss = 1.54771307\n",
      "Validation score: 0.428571\n",
      "Iteration 811, loss = 1.54771272\n",
      "Validation score: 0.428571\n",
      "Iteration 812, loss = 1.54771249\n",
      "Validation score: 0.428571\n",
      "Iteration 813, loss = 1.54771199\n",
      "Validation score: 0.428571\n",
      "Iteration 814, loss = 1.54771178\n",
      "Validation score: 0.428571\n",
      "Iteration 815, loss = 1.54771143\n",
      "Validation score: 0.428571\n",
      "Iteration 816, loss = 1.54771120\n",
      "Validation score: 0.428571\n",
      "Iteration 817, loss = 1.54771082\n",
      "Validation score: 0.428571\n",
      "Iteration 818, loss = 1.54771056\n",
      "Validation score: 0.428571\n",
      "Iteration 819, loss = 1.54771032\n",
      "Validation score: 0.428571\n",
      "Iteration 820, loss = 1.54770983\n",
      "Validation score: 0.428571\n",
      "Iteration 821, loss = 1.54770968\n",
      "Validation score: 0.428571\n",
      "Iteration 822, loss = 1.54770916\n",
      "Validation score: 0.428571\n",
      "Iteration 823, loss = 1.54770881\n",
      "Validation score: 0.428571\n",
      "Iteration 824, loss = 1.54770857\n",
      "Validation score: 0.428571\n",
      "Iteration 825, loss = 1.54770822\n",
      "Validation score: 0.428571\n",
      "Iteration 826, loss = 1.54770784\n",
      "Validation score: 0.428571\n",
      "Iteration 827, loss = 1.54770764\n",
      "Validation score: 0.428571\n",
      "Iteration 828, loss = 1.54770714\n",
      "Validation score: 0.428571\n",
      "Iteration 829, loss = 1.54770703\n",
      "Validation score: 0.428571\n",
      "Iteration 830, loss = 1.54770671\n",
      "Validation score: 0.428571\n",
      "Iteration 831, loss = 1.54770635\n",
      "Validation score: 0.428571\n",
      "Iteration 832, loss = 1.54770595\n",
      "Validation score: 0.428571\n",
      "Iteration 833, loss = 1.54770568\n",
      "Validation score: 0.428571\n",
      "Iteration 834, loss = 1.54770522\n",
      "Validation score: 0.428571\n",
      "Iteration 835, loss = 1.54770504\n",
      "Validation score: 0.428571\n",
      "Iteration 836, loss = 1.54770469\n",
      "Validation score: 0.428571\n",
      "Iteration 837, loss = 1.54770437\n",
      "Validation score: 0.428571\n",
      "Iteration 838, loss = 1.54770399\n",
      "Validation score: 0.428571\n",
      "Iteration 839, loss = 1.54770367\n",
      "Validation score: 0.428571\n",
      "Iteration 840, loss = 1.54770338\n",
      "Validation score: 0.428571\n",
      "Iteration 841, loss = 1.54770309\n",
      "Validation score: 0.428571\n",
      "Iteration 842, loss = 1.54770268\n",
      "Validation score: 0.428571\n",
      "Iteration 843, loss = 1.54770241\n",
      "Validation score: 0.428571\n",
      "Iteration 844, loss = 1.54770215\n",
      "Validation score: 0.428571\n",
      "Iteration 845, loss = 1.54770189\n",
      "Validation score: 0.428571\n",
      "Iteration 846, loss = 1.54770148\n",
      "Validation score: 0.428571\n",
      "Iteration 847, loss = 1.54770119\n",
      "Validation score: 0.428571\n",
      "Iteration 848, loss = 1.54770093\n",
      "Validation score: 0.428571\n",
      "Iteration 849, loss = 1.54770046\n",
      "Validation score: 0.428571\n",
      "Iteration 850, loss = 1.54770020\n",
      "Validation score: 0.428571\n",
      "Iteration 851, loss = 1.54769979\n",
      "Validation score: 0.428571\n",
      "Iteration 852, loss = 1.54769947\n",
      "Validation score: 0.428571\n",
      "Iteration 853, loss = 1.54769923\n",
      "Validation score: 0.428571\n",
      "Iteration 854, loss = 1.54769903\n",
      "Validation score: 0.428571\n",
      "Iteration 855, loss = 1.54769859\n",
      "Validation score: 0.428571\n",
      "Iteration 856, loss = 1.54769818\n",
      "Validation score: 0.428571\n",
      "Iteration 857, loss = 1.54769789\n",
      "Validation score: 0.428571\n",
      "Iteration 858, loss = 1.54769760\n",
      "Validation score: 0.428571\n",
      "Iteration 859, loss = 1.54769742\n",
      "Validation score: 0.428571\n",
      "Iteration 860, loss = 1.54769707\n",
      "Validation score: 0.428571\n",
      "Iteration 861, loss = 1.54769672\n",
      "Validation score: 0.428571\n",
      "Iteration 862, loss = 1.54769637\n",
      "Validation score: 0.428571\n",
      "Iteration 863, loss = 1.54769602\n",
      "Validation score: 0.428571\n",
      "Iteration 864, loss = 1.54769579\n",
      "Validation score: 0.428571\n",
      "Iteration 865, loss = 1.54769544\n",
      "Validation score: 0.428571\n",
      "Validation score did not improve more than tol=0.000000 for 170 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 866, loss = 1.54769482\n",
      "Validation score: 0.428571\n",
      "Iteration 867, loss = 1.54769468\n",
      "Validation score: 0.428571\n",
      "Iteration 868, loss = 1.54769444\n",
      "Validation score: 0.428571\n",
      "Iteration 869, loss = 1.54769433\n",
      "Validation score: 0.428571\n",
      "Iteration 870, loss = 1.54769415\n",
      "Validation score: 0.428571\n",
      "Iteration 871, loss = 1.54769404\n",
      "Validation score: 0.428571\n",
      "Iteration 872, loss = 1.54769398\n",
      "Validation score: 0.428571\n",
      "Iteration 873, loss = 1.54769392\n",
      "Validation score: 0.428571\n",
      "Iteration 874, loss = 1.54769386\n",
      "Validation score: 0.428571\n",
      "Iteration 875, loss = 1.54769377\n",
      "Validation score: 0.428571\n",
      "Iteration 876, loss = 1.54769371\n",
      "Validation score: 0.428571\n",
      "Iteration 877, loss = 1.54769363\n",
      "Validation score: 0.428571\n",
      "Iteration 878, loss = 1.54769363\n",
      "Validation score: 0.428571\n",
      "Iteration 879, loss = 1.54769351\n",
      "Validation score: 0.428571\n",
      "Iteration 880, loss = 1.54769348\n",
      "Validation score: 0.428571\n",
      "Iteration 881, loss = 1.54769351\n",
      "Validation score: 0.428571\n",
      "Iteration 882, loss = 1.54769336\n",
      "Validation score: 0.428571\n",
      "Iteration 883, loss = 1.54769328\n",
      "Validation score: 0.428571\n",
      "Iteration 884, loss = 1.54769328\n",
      "Validation score: 0.428571\n",
      "Iteration 885, loss = 1.54769319\n",
      "Validation score: 0.428571\n",
      "Iteration 886, loss = 1.54769310\n",
      "Validation score: 0.428571\n",
      "Iteration 887, loss = 1.54769307\n",
      "Validation score: 0.428571\n",
      "Iteration 888, loss = 1.54769298\n",
      "Validation score: 0.428571\n",
      "Iteration 889, loss = 1.54769298\n",
      "Validation score: 0.428571\n",
      "Iteration 890, loss = 1.54769287\n",
      "Validation score: 0.428571\n",
      "Iteration 891, loss = 1.54769293\n",
      "Validation score: 0.428571\n",
      "Iteration 892, loss = 1.54769284\n",
      "Validation score: 0.428571\n",
      "Iteration 893, loss = 1.54769281\n",
      "Validation score: 0.428571\n",
      "Iteration 894, loss = 1.54769272\n",
      "Validation score: 0.428571\n",
      "Iteration 895, loss = 1.54769263\n",
      "Validation score: 0.428571\n",
      "Iteration 896, loss = 1.54769258\n",
      "Validation score: 0.428571\n",
      "Iteration 897, loss = 1.54769249\n",
      "Validation score: 0.428571\n",
      "Iteration 898, loss = 1.54769258\n",
      "Validation score: 0.428571\n",
      "Iteration 899, loss = 1.54769243\n",
      "Validation score: 0.428571\n",
      "Iteration 900, loss = 1.54769243\n",
      "Validation score: 0.428571\n",
      "Iteration 901, loss = 1.54769228\n",
      "Validation score: 0.428571\n",
      "Iteration 902, loss = 1.54769228\n",
      "Validation score: 0.428571\n",
      "Iteration 903, loss = 1.54769223\n",
      "Validation score: 0.428571\n",
      "Iteration 904, loss = 1.54769217\n",
      "Validation score: 0.428571\n",
      "Iteration 905, loss = 1.54769211\n",
      "Validation score: 0.428571\n",
      "Iteration 906, loss = 1.54769211\n",
      "Validation score: 0.428571\n",
      "Iteration 907, loss = 1.54769199\n",
      "Validation score: 0.428571\n",
      "Iteration 908, loss = 1.54769193\n",
      "Validation score: 0.428571\n",
      "Iteration 909, loss = 1.54769188\n",
      "Validation score: 0.428571\n",
      "Iteration 910, loss = 1.54769176\n",
      "Validation score: 0.428571\n",
      "Iteration 911, loss = 1.54769173\n",
      "Validation score: 0.428571\n",
      "Iteration 912, loss = 1.54769170\n",
      "Validation score: 0.428571\n",
      "Iteration 913, loss = 1.54769170\n",
      "Validation score: 0.428571\n",
      "Iteration 914, loss = 1.54769161\n",
      "Validation score: 0.428571\n",
      "Iteration 915, loss = 1.54769158\n",
      "Validation score: 0.428571\n",
      "Iteration 916, loss = 1.54769147\n",
      "Validation score: 0.428571\n",
      "Iteration 917, loss = 1.54769147\n",
      "Validation score: 0.428571\n",
      "Iteration 918, loss = 1.54769147\n",
      "Validation score: 0.428571\n",
      "Iteration 919, loss = 1.54769129\n",
      "Validation score: 0.428571\n",
      "Iteration 920, loss = 1.54769126\n",
      "Validation score: 0.428571\n",
      "Iteration 921, loss = 1.54769123\n",
      "Validation score: 0.428571\n",
      "Iteration 922, loss = 1.54769117\n",
      "Validation score: 0.428571\n",
      "Iteration 923, loss = 1.54769115\n",
      "Validation score: 0.428571\n",
      "Iteration 924, loss = 1.54769106\n",
      "Validation score: 0.428571\n",
      "Iteration 925, loss = 1.54769106\n",
      "Validation score: 0.428571\n",
      "Iteration 926, loss = 1.54769100\n",
      "Validation score: 0.428571\n",
      "Iteration 927, loss = 1.54769088\n",
      "Validation score: 0.428571\n",
      "Iteration 928, loss = 1.54769088\n",
      "Validation score: 0.428571\n",
      "Iteration 929, loss = 1.54769077\n",
      "Validation score: 0.428571\n",
      "Iteration 930, loss = 1.54769082\n",
      "Validation score: 0.428571\n",
      "Iteration 931, loss = 1.54769071\n",
      "Validation score: 0.428571\n",
      "Iteration 932, loss = 1.54769065\n",
      "Validation score: 0.428571\n",
      "Iteration 933, loss = 1.54769062\n",
      "Validation score: 0.428571\n",
      "Iteration 934, loss = 1.54769065\n",
      "Validation score: 0.428571\n",
      "Iteration 935, loss = 1.54769047\n",
      "Validation score: 0.428571\n",
      "Iteration 936, loss = 1.54769044\n",
      "Validation score: 0.428571\n",
      "Iteration 937, loss = 1.54769042\n",
      "Validation score: 0.428571\n",
      "Iteration 938, loss = 1.54769036\n",
      "Validation score: 0.428571\n",
      "Iteration 939, loss = 1.54769027\n",
      "Validation score: 0.428571\n",
      "Iteration 940, loss = 1.54769030\n",
      "Validation score: 0.428571\n",
      "Iteration 941, loss = 1.54769015\n",
      "Validation score: 0.428571\n",
      "Iteration 942, loss = 1.54769018\n",
      "Validation score: 0.428571\n",
      "Iteration 943, loss = 1.54769006\n",
      "Validation score: 0.428571\n",
      "Iteration 944, loss = 1.54769001\n",
      "Validation score: 0.428571\n",
      "Iteration 945, loss = 1.54769001\n",
      "Validation score: 0.428571\n",
      "Iteration 946, loss = 1.54768989\n",
      "Validation score: 0.428571\n",
      "Iteration 947, loss = 1.54768989\n",
      "Validation score: 0.428571\n",
      "Iteration 948, loss = 1.54768980\n",
      "Validation score: 0.428571\n",
      "Iteration 949, loss = 1.54768977\n",
      "Validation score: 0.428571\n",
      "Iteration 950, loss = 1.54768980\n",
      "Validation score: 0.428571\n",
      "Iteration 951, loss = 1.54768966\n",
      "Validation score: 0.428571\n",
      "Iteration 952, loss = 1.54768957\n",
      "Validation score: 0.428571\n",
      "Iteration 953, loss = 1.54768954\n",
      "Validation score: 0.428571\n",
      "Iteration 954, loss = 1.54768942\n",
      "Validation score: 0.428571\n",
      "Iteration 955, loss = 1.54768942\n",
      "Validation score: 0.428571\n",
      "Iteration 956, loss = 1.54768931\n",
      "Validation score: 0.428571\n",
      "Iteration 957, loss = 1.54768925\n",
      "Validation score: 0.428571\n",
      "Iteration 958, loss = 1.54768925\n",
      "Validation score: 0.428571\n",
      "Iteration 959, loss = 1.54768919\n",
      "Validation score: 0.428571\n",
      "Iteration 960, loss = 1.54768913\n",
      "Validation score: 0.428571\n",
      "Iteration 961, loss = 1.54768910\n",
      "Validation score: 0.428571\n",
      "Iteration 962, loss = 1.54768913\n",
      "Validation score: 0.428571\n",
      "Iteration 963, loss = 1.54768901\n",
      "Validation score: 0.428571\n",
      "Iteration 964, loss = 1.54768896\n",
      "Validation score: 0.428571\n",
      "Iteration 965, loss = 1.54768890\n",
      "Validation score: 0.428571\n",
      "Iteration 966, loss = 1.54768884\n",
      "Validation score: 0.428571\n",
      "Iteration 967, loss = 1.54768872\n",
      "Validation score: 0.428571\n",
      "Iteration 968, loss = 1.54768872\n",
      "Validation score: 0.428571\n",
      "Iteration 969, loss = 1.54768863\n",
      "Validation score: 0.428571\n",
      "Iteration 970, loss = 1.54768860\n",
      "Validation score: 0.428571\n",
      "Iteration 971, loss = 1.54768860\n",
      "Validation score: 0.428571\n",
      "Iteration 972, loss = 1.54768849\n",
      "Validation score: 0.428571\n",
      "Iteration 973, loss = 1.54768849\n",
      "Validation score: 0.428571\n",
      "Iteration 974, loss = 1.54768840\n",
      "Validation score: 0.428571\n",
      "Iteration 975, loss = 1.54768837\n",
      "Validation score: 0.428571\n",
      "Iteration 976, loss = 1.54768831\n",
      "Validation score: 0.428571\n",
      "Iteration 977, loss = 1.54768831\n",
      "Validation score: 0.428571\n",
      "Iteration 978, loss = 1.54768817\n",
      "Validation score: 0.428571\n",
      "Iteration 979, loss = 1.54768814\n",
      "Validation score: 0.428571\n",
      "Iteration 980, loss = 1.54768814\n",
      "Validation score: 0.428571\n",
      "Iteration 981, loss = 1.54768802\n",
      "Validation score: 0.428571\n",
      "Iteration 982, loss = 1.54768802\n",
      "Validation score: 0.428571\n",
      "Iteration 983, loss = 1.54768790\n",
      "Validation score: 0.428571\n",
      "Iteration 984, loss = 1.54768785\n",
      "Validation score: 0.428571\n",
      "Iteration 985, loss = 1.54768779\n",
      "Validation score: 0.428571\n",
      "Iteration 986, loss = 1.54768779\n",
      "Validation score: 0.428571\n",
      "Iteration 987, loss = 1.54768773\n",
      "Validation score: 0.428571\n",
      "Iteration 988, loss = 1.54768758\n",
      "Validation score: 0.428571\n",
      "Iteration 989, loss = 1.54768758\n",
      "Validation score: 0.428571\n",
      "Iteration 990, loss = 1.54768752\n",
      "Validation score: 0.428571\n",
      "Iteration 991, loss = 1.54768744\n",
      "Validation score: 0.428571\n",
      "Iteration 992, loss = 1.54768747\n",
      "Validation score: 0.428571\n",
      "Iteration 993, loss = 1.54768732\n",
      "Validation score: 0.428571\n",
      "Iteration 994, loss = 1.54768732\n",
      "Validation score: 0.428571\n",
      "Iteration 995, loss = 1.54768732\n",
      "Validation score: 0.428571\n",
      "Iteration 996, loss = 1.54768723\n",
      "Validation score: 0.428571\n",
      "Iteration 997, loss = 1.54768714\n",
      "Validation score: 0.428571\n",
      "Iteration 998, loss = 1.54768714\n",
      "Validation score: 0.428571\n",
      "Iteration 999, loss = 1.54768709\n",
      "Validation score: 0.428571\n",
      "Iteration 1000, loss = 1.54768700\n",
      "Validation score: 0.428571\n",
      "0.40336134453781514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(50, 50, 30), #, 40, 30, 20, 10),\n",
    "    verbose=True,\n",
    "    warm_start=False,\n",
    "    early_stopping=True,\n",
    "    momentum=0.9,\n",
    "    #learning_rate=\"constant\",\n",
    "    learning_rate='adaptive',\n",
    "    nesterovs_momentum=True,\n",
    "    learning_rate_init=0.001,\n",
    "    max_iter=1000,\n",
    "    validation_fraction=0.05,\n",
    "    n_iter_no_change=170,\n",
    "    solver='sgd',\n",
    "    tol=1e-30,\n",
    "    random_state=47,\n",
    "    alpha=0.01\n",
    ")\n",
    "mlp.fit(x_train.detach().cpu().numpy(), y_train.detach().cpu().numpy())\n",
    "print(mlp.score(x_val.detach().cpu().numpy(), y_val.detach().cpu().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/4 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4568f08375d2408ba075850af019b81f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM: best_params: {'kernel': 'rbf', 'C': 0.0001, 'decision_shape': 'ovr', 'degree': None}, accuracy: 0.4369747899159664\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "kernels = [\"rbf\", \"linear\", \"sigmoid\", \"poly\"]\n",
    "decision_shapes = [\"ovr\", \"ovo\"]\n",
    "penalties = [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0, 1.5]\n",
    "degrees = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 20, 25, 30, 40, 50]\n",
    "max_acc: float = 0.0\n",
    "best_params = {\"kernel\": None, \"C\": None, \"decision_shape\": None, \"degree\": None}\n",
    "\n",
    "\n",
    "for kernel in tqdm(kernels):\n",
    "    for decision_shape in decision_shapes:\n",
    "        for c in penalties:\n",
    "            if kernel == \"poly\":\n",
    "                for degree in penalties:\n",
    "                    svm = SVC(kernel=kernel, decision_function_shape=decision_shape, C=c, degree=degree, random_state=47)\n",
    "                    svm.fit(x_train.detach().cpu().numpy(), y_train.detach().cpu().numpy())\n",
    "                    acc = svm.score(x_val.detach().cpu().numpy(), y_val.detach().cpu().numpy())\n",
    "\n",
    "                    if acc > max_acc:\n",
    "                        best_params[\"kernel\"] = kernel\n",
    "                        best_params[\"C\"] = c\n",
    "                        best_params[\"decision_shape\"] = decision_shape\n",
    "                        best_params[\"degree\"] = degree\n",
    "            else:\n",
    "                svm = SVC(kernel=kernel, decision_function_shape=decision_shape, C=c, random_state=47)\n",
    "                svm.fit(x_train.detach().cpu().numpy(), y_train.detach().cpu().numpy())\n",
    "                acc = svm.score(x_val.detach().cpu().numpy(), y_val.detach().cpu().numpy())\n",
    "\n",
    "                if acc > max_acc:\n",
    "                    max_acc = acc\n",
    "                    best_params[\"kernel\"] = kernel\n",
    "                    best_params[\"C\"] = c\n",
    "                    best_params[\"decision_shape\"] = decision_shape\n",
    "                    best_params[\"degree\"] = None\n",
    "\n",
    "print(f\"SVM: best_params: {best_params}, accuracy: {max_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/148 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6b86fb66d0634f7fb3b40c3af394089d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k: 2, score: 0.14285714285714285\n",
      "k: 3, score: 0.19327731092436976\n",
      "k: 4, score: 0.20168067226890757\n",
      "k: 5, score: 0.19327731092436976\n",
      "k: 6, score: 0.21008403361344538\n",
      "k: 7, score: 0.23529411764705882\n",
      "k: 8, score: 0.2773109243697479\n",
      "k: 9, score: 0.2773109243697479\n",
      "k: 10, score: 0.31932773109243695\n",
      "k: 11, score: 0.2773109243697479\n",
      "k: 12, score: 0.31932773109243695\n",
      "k: 13, score: 0.31932773109243695\n",
      "k: 14, score: 0.3277310924369748\n",
      "k: 15, score: 0.33613445378151263\n",
      "k: 16, score: 0.3445378151260504\n",
      "k: 17, score: 0.35294117647058826\n",
      "k: 18, score: 0.3445378151260504\n",
      "k: 19, score: 0.35294117647058826\n",
      "k: 20, score: 0.35294117647058826\n",
      "k: 21, score: 0.3949579831932773\n",
      "k: 22, score: 0.3697478991596639\n",
      "k: 23, score: 0.3697478991596639\n",
      "k: 24, score: 0.3697478991596639\n",
      "k: 25, score: 0.37815126050420167\n",
      "k: 26, score: 0.36134453781512604\n",
      "k: 27, score: 0.36134453781512604\n",
      "k: 28, score: 0.37815126050420167\n",
      "k: 29, score: 0.35294117647058826\n",
      "k: 30, score: 0.36134453781512604\n",
      "k: 31, score: 0.3697478991596639\n",
      "k: 32, score: 0.3697478991596639\n",
      "k: 33, score: 0.3949579831932773\n",
      "k: 34, score: 0.37815126050420167\n",
      "k: 35, score: 0.3865546218487395\n",
      "k: 36, score: 0.3865546218487395\n",
      "k: 37, score: 0.3865546218487395\n",
      "k: 38, score: 0.3949579831932773\n",
      "k: 39, score: 0.40336134453781514\n",
      "k: 40, score: 0.3949579831932773\n",
      "k: 41, score: 0.3865546218487395\n",
      "k: 42, score: 0.3949579831932773\n",
      "k: 43, score: 0.3949579831932773\n",
      "k: 44, score: 0.3949579831932773\n",
      "k: 45, score: 0.42016806722689076\n",
      "k: 46, score: 0.4117647058823529\n",
      "k: 47, score: 0.40336134453781514\n",
      "k: 48, score: 0.4117647058823529\n",
      "k: 49, score: 0.42016806722689076\n",
      "k: 50, score: 0.42857142857142855\n",
      "k: 51, score: 0.42016806722689076\n",
      "k: 52, score: 0.42016806722689076\n",
      "k: 53, score: 0.42016806722689076\n",
      "k: 54, score: 0.42016806722689076\n",
      "k: 55, score: 0.4117647058823529\n",
      "k: 56, score: 0.4117647058823529\n",
      "k: 57, score: 0.4117647058823529\n",
      "k: 58, score: 0.42016806722689076\n",
      "k: 59, score: 0.42016806722689076\n",
      "k: 60, score: 0.42016806722689076\n",
      "k: 61, score: 0.42016806722689076\n",
      "k: 62, score: 0.42016806722689076\n",
      "k: 63, score: 0.42016806722689076\n",
      "k: 64, score: 0.42016806722689076\n",
      "k: 65, score: 0.42857142857142855\n",
      "k: 66, score: 0.4369747899159664\n",
      "k: 67, score: 0.44537815126050423\n",
      "k: 68, score: 0.44537815126050423\n",
      "k: 69, score: 0.44537815126050423\n",
      "k: 70, score: 0.44537815126050423\n",
      "k: 71, score: 0.44537815126050423\n",
      "k: 72, score: 0.4369747899159664\n",
      "k: 73, score: 0.44537815126050423\n",
      "k: 74, score: 0.44537815126050423\n",
      "k: 75, score: 0.44537815126050423\n",
      "k: 76, score: 0.44537815126050423\n",
      "k: 77, score: 0.44537815126050423\n",
      "k: 78, score: 0.44537815126050423\n",
      "k: 79, score: 0.44537815126050423\n",
      "k: 80, score: 0.44537815126050423\n",
      "k: 81, score: 0.44537815126050423\n",
      "k: 82, score: 0.4369747899159664\n",
      "k: 83, score: 0.4369747899159664\n",
      "k: 84, score: 0.44537815126050423\n",
      "k: 85, score: 0.4369747899159664\n",
      "k: 86, score: 0.4369747899159664\n",
      "k: 87, score: 0.4369747899159664\n",
      "k: 88, score: 0.4369747899159664\n",
      "k: 89, score: 0.4369747899159664\n",
      "k: 90, score: 0.4369747899159664\n",
      "k: 91, score: 0.4369747899159664\n",
      "k: 92, score: 0.4369747899159664\n",
      "k: 93, score: 0.4369747899159664\n",
      "k: 94, score: 0.4369747899159664\n",
      "k: 95, score: 0.4369747899159664\n",
      "k: 96, score: 0.4369747899159664\n",
      "k: 97, score: 0.4369747899159664\n",
      "k: 98, score: 0.4369747899159664\n",
      "k: 99, score: 0.4369747899159664\n",
      "k: 100, score: 0.4369747899159664\n",
      "k: 101, score: 0.4369747899159664\n",
      "k: 102, score: 0.4369747899159664\n",
      "k: 103, score: 0.4369747899159664\n",
      "k: 104, score: 0.4369747899159664\n",
      "k: 105, score: 0.4369747899159664\n",
      "k: 106, score: 0.4369747899159664\n",
      "k: 107, score: 0.4369747899159664\n",
      "k: 108, score: 0.4369747899159664\n",
      "k: 109, score: 0.4369747899159664\n",
      "k: 110, score: 0.4369747899159664\n",
      "k: 111, score: 0.4369747899159664\n",
      "k: 112, score: 0.4369747899159664\n",
      "k: 113, score: 0.4369747899159664\n",
      "k: 114, score: 0.4369747899159664\n",
      "k: 115, score: 0.4369747899159664\n",
      "k: 116, score: 0.4369747899159664\n",
      "k: 117, score: 0.4369747899159664\n",
      "k: 118, score: 0.4369747899159664\n",
      "k: 119, score: 0.4369747899159664\n",
      "k: 120, score: 0.4369747899159664\n",
      "k: 121, score: 0.4369747899159664\n",
      "k: 122, score: 0.4369747899159664\n",
      "k: 123, score: 0.4369747899159664\n",
      "k: 124, score: 0.4369747899159664\n",
      "k: 125, score: 0.4369747899159664\n",
      "k: 126, score: 0.4369747899159664\n",
      "k: 127, score: 0.4369747899159664\n",
      "k: 128, score: 0.4369747899159664\n",
      "k: 129, score: 0.4369747899159664\n",
      "k: 130, score: 0.4369747899159664\n",
      "k: 131, score: 0.4369747899159664\n",
      "k: 132, score: 0.4369747899159664\n",
      "k: 133, score: 0.4369747899159664\n",
      "k: 134, score: 0.4369747899159664\n",
      "k: 135, score: 0.4369747899159664\n",
      "k: 136, score: 0.4369747899159664\n",
      "k: 137, score: 0.4369747899159664\n",
      "k: 138, score: 0.4369747899159664\n",
      "k: 139, score: 0.4369747899159664\n",
      "k: 140, score: 0.4369747899159664\n",
      "k: 141, score: 0.4369747899159664\n",
      "k: 142, score: 0.4369747899159664\n",
      "k: 143, score: 0.4369747899159664\n",
      "k: 144, score: 0.4369747899159664\n",
      "k: 145, score: 0.4369747899159664\n",
      "k: 146, score: 0.4369747899159664\n",
      "k: 147, score: 0.4369747899159664\n",
      "k: 148, score: 0.4369747899159664\n",
      "k: 149, score: 0.4369747899159664\n",
      "Best k: 67, best score: 0.44537815126050423\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "max_acc_k = 2\n",
    "max_acc: float = 0.0\n",
    "max_neighbors = 150\n",
    "for k in tqdm(range(2, max_neighbors)):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k, n_jobs=-1)\n",
    "    knn.fit(x_train.detach().cpu().numpy(), y_train.detach().cpu().numpy())\n",
    "    acc = knn.score(x_val.detach().cpu().numpy(), y_val.detach().cpu().numpy())\n",
    "\n",
    "    if acc > max_acc:\n",
    "        max_acc = acc\n",
    "        max_acc_k = k\n",
    "    print(f\"k: {k}, score: {acc}\")\n",
    "\n",
    "print(f\"Best k: {max_acc_k}, best score: {max_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4369747899159664, warm_starts: True, n_restarts: 0, kernel: RBF(length_scale=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4369747899159664, warm_starts: False, n_restarts: 0, kernel: RBF(length_scale=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4369747899159664, warm_starts: True, n_restarts: 5, kernel: RBF(length_scale=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4369747899159664, warm_starts: False, n_restarts: 5, kernel: RBF(length_scale=1)\n",
      "Accuracy: 0.4369747899159664, warm_starts: True, n_restarts: 10, kernel: RBF(length_scale=1)\n",
      "Accuracy: 0.4369747899159664, warm_starts: False, n_restarts: 10, kernel: RBF(length_scale=1)\n",
      "Accuracy: 0.4369747899159664, warm_starts: True, n_restarts: 15, kernel: RBF(length_scale=1)\n",
      "Accuracy: 0.4369747899159664, warm_starts: False, n_restarts: 15, kernel: RBF(length_scale=1)\n",
      "Accuracy: 0.4369747899159664, warm_starts: True, n_restarts: 20, kernel: RBF(length_scale=1)\n",
      "Accuracy: 0.4369747899159664, warm_starts: False, n_restarts: 20, kernel: RBF(length_scale=1)\n",
      "Accuracy: 0.4369747899159664, warm_starts: True, n_restarts: 25, kernel: RBF(length_scale=1)\n",
      "Accuracy: 0.4369747899159664, warm_starts: False, n_restarts: 25, kernel: RBF(length_scale=1)\n",
      "Accuracy: 0.4369747899159664, warm_starts: True, n_restarts: 0, kernel: WhiteKernel(noise_level=1)\n",
      "Accuracy: 0.4369747899159664, warm_starts: False, n_restarts: 0, kernel: WhiteKernel(noise_level=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter noise_level is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter noise_level is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter noise_level is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter noise_level is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter noise_level is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter noise_level is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4369747899159664, warm_starts: True, n_restarts: 5, kernel: WhiteKernel(noise_level=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter noise_level is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter noise_level is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter noise_level is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter noise_level is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter noise_level is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter noise_level is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4369747899159664, warm_starts: False, n_restarts: 5, kernel: WhiteKernel(noise_level=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter noise_level is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter noise_level is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter noise_level is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter noise_level is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter noise_level is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter noise_level is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4369747899159664, warm_starts: True, n_restarts: 10, kernel: WhiteKernel(noise_level=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter noise_level is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter noise_level is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4369747899159664, warm_starts: False, n_restarts: 10, kernel: WhiteKernel(noise_level=1)\n",
      "Accuracy: 0.4369747899159664, warm_starts: True, n_restarts: 15, kernel: WhiteKernel(noise_level=1)\n",
      "Accuracy: 0.4369747899159664, warm_starts: False, n_restarts: 15, kernel: WhiteKernel(noise_level=1)\n",
      "Accuracy: 0.4369747899159664, warm_starts: True, n_restarts: 20, kernel: WhiteKernel(noise_level=1)\n",
      "Accuracy: 0.4369747899159664, warm_starts: False, n_restarts: 20, kernel: WhiteKernel(noise_level=1)\n",
      "Accuracy: 0.4369747899159664, warm_starts: True, n_restarts: 25, kernel: WhiteKernel(noise_level=1)\n",
      "Accuracy: 0.4369747899159664, warm_starts: False, n_restarts: 25, kernel: WhiteKernel(noise_level=1)\n",
      "Accuracy: 0.4369747899159664, warm_starts: True, n_restarts: 0, kernel: 1**2\n",
      "Accuracy: 0.4369747899159664, warm_starts: False, n_restarts: 0, kernel: 1**2\n",
      "Accuracy: 0.4369747899159664, warm_starts: True, n_restarts: 5, kernel: 1**2\n",
      "Accuracy: 0.4369747899159664, warm_starts: False, n_restarts: 5, kernel: 1**2\n",
      "Accuracy: 0.4369747899159664, warm_starts: True, n_restarts: 10, kernel: 1**2\n",
      "Accuracy: 0.4369747899159664, warm_starts: False, n_restarts: 10, kernel: 1**2\n",
      "Accuracy: 0.4369747899159664, warm_starts: True, n_restarts: 15, kernel: 1**2\n",
      "Accuracy: 0.4369747899159664, warm_starts: False, n_restarts: 15, kernel: 1**2\n",
      "Accuracy: 0.4369747899159664, warm_starts: True, n_restarts: 20, kernel: 1**2\n",
      "Accuracy: 0.4369747899159664, warm_starts: False, n_restarts: 20, kernel: 1**2\n",
      "Accuracy: 0.4369747899159664, warm_starts: True, n_restarts: 25, kernel: 1**2\n",
      "Accuracy: 0.4369747899159664, warm_starts: False, n_restarts: 25, kernel: 1**2\n",
      "Accuracy: 0.3865546218487395, warm_starts: True, n_restarts: 0, kernel: PairwiseKernel(gamma=1.0, metric=linear)\n",
      "Accuracy: 0.3865546218487395, warm_starts: False, n_restarts: 0, kernel: PairwiseKernel(gamma=1.0, metric=linear)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter gamma is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter gamma is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter gamma is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter gamma is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3865546218487395, warm_starts: True, n_restarts: 5, kernel: PairwiseKernel(gamma=1.0, metric=linear)\n",
      "Accuracy: 0.3865546218487395, warm_starts: False, n_restarts: 5, kernel: PairwiseKernel(gamma=1.0, metric=linear)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter gamma is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter gamma is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter gamma is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter gamma is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter gamma is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3865546218487395, warm_starts: True, n_restarts: 10, kernel: PairwiseKernel(gamma=1.0, metric=linear)\n",
      "Accuracy: 0.3865546218487395, warm_starts: False, n_restarts: 10, kernel: PairwiseKernel(gamma=1.0, metric=linear)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter gamma is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter gamma is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter gamma is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter gamma is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3865546218487395, warm_starts: True, n_restarts: 15, kernel: PairwiseKernel(gamma=1.0, metric=linear)\n",
      "Accuracy: 0.3865546218487395, warm_starts: False, n_restarts: 15, kernel: PairwiseKernel(gamma=1.0, metric=linear)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter gamma is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter gamma is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter gamma is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3865546218487395, warm_starts: True, n_restarts: 20, kernel: PairwiseKernel(gamma=1.0, metric=linear)\n",
      "Accuracy: 0.3865546218487395, warm_starts: False, n_restarts: 20, kernel: PairwiseKernel(gamma=1.0, metric=linear)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter gamma is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter gamma is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter gamma is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter gamma is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3865546218487395, warm_starts: True, n_restarts: 25, kernel: PairwiseKernel(gamma=1.0, metric=linear)\n",
      "Accuracy: 0.3865546218487395, warm_starts: False, n_restarts: 25, kernel: PairwiseKernel(gamma=1.0, metric=linear)\n",
      "Best params: {'n_restarts': 0, 'warm_start': True, 'kernel': RBF(length_scale=1)}, best accuracy: 0.4369747899159664\n"
     ]
    }
   ],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF, WhiteKernel, ConstantKernel, PairwiseKernel\n",
    "\n",
    "\n",
    "n_restarts = [0, 5, 10, 15, 20, 25]\n",
    "kernels = [RBF(), WhiteKernel(), ConstantKernel(), PairwiseKernel()]\n",
    "warm_starts = [True, False]\n",
    "max_acc = 0.0\n",
    "best_params = {\"n_restarts\": None, \"warm_start\": None, \"kernel\": None}\n",
    "\n",
    "for kernel in kernels:\n",
    "    for n in n_restarts:\n",
    "        for ws in warm_starts:\n",
    "            gpc = GaussianProcessClassifier(kernel=kernel, n_restarts_optimizer=n, max_iter_predict=100, warm_start=ws, random_state=47, n_jobs=-1)\n",
    "            gpc.fit(x_train.detach().cpu().numpy(), y_train.detach().cpu().numpy())\n",
    "            acc = gpc.score(x_val.detach().cpu().numpy(), y_val.detach().cpu().numpy())\n",
    "            if acc > max_acc:\n",
    "                max_acc = acc\n",
    "                best_params[\"n_restarts\"] = n\n",
    "                best_params[\"warm_start\"] = ws\n",
    "                best_params[\"kernel\"] = kernel\n",
    "            print(f\"Accuracy: {acc}, warm_starts: {ws}, n_restarts: {n}, kernel: {kernel}\")\n",
    "print(f\"Best params: {best_params}, best accuracy: {max_acc}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14285714285714285\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "nbc = GaussianNB()\n",
    "nbc.fit(x_train.detach().cpu().numpy(), y_train.detach().cpu().numpy())\n",
    "acc = nbc.score(x_val.detach().cpu().numpy(), y_val.detach().cpu().numpy())\n",
    "print(acc)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear discriminant analysis: best params: {'solver': 'lsqr'}, best accuracy: 0.33613445378151263\n",
      "Quadratic discriminant analysis: best params: {'reg_param': 0.5}, best accuracy: 0.3445378151260504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/discriminant_analysis.py:878: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/discriminant_analysis.py:878: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/discriminant_analysis.py:878: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/discriminant_analysis.py:878: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/discriminant_analysis.py:878: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/discriminant_analysis.py:878: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/discriminant_analysis.py:878: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/discriminant_analysis.py:878: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/discriminant_analysis.py:878: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/discriminant_analysis.py:878: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/discriminant_analysis.py:878: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/discriminant_analysis.py:878: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "\n",
    "solvers = [\"svd\", \"lsqr\", \"eigen\"]\n",
    "reg_params = [0.0, 0.01, 0.05, 0.1, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 1.0, 1.1]\n",
    "max_acc = 0.0\n",
    "best_params = {\"solver\": None}\n",
    "\n",
    "for solver in solvers:\n",
    "    lda = LinearDiscriminantAnalysis(solver=solver)\n",
    "    lda.fit(x_train.detach().cpu().numpy(), y_train.detach().cpu().numpy())\n",
    "    acc = lda.score(x_val.detach().cpu().numpy(), y_val.detach().cpu().numpy())\n",
    "\n",
    "    if acc > max_acc:\n",
    "        best_params[\"solver\"] = solver\n",
    "        max_acc = acc\n",
    "print(f\"Linear discriminant analysis: best params: {best_params}, best accuracy: {max_acc}\")\n",
    "\n",
    "best_params = {\"reg_param\": None}\n",
    "max_acc = 0.0\n",
    "for reg in reg_params:\n",
    "    qda = QuadraticDiscriminantAnalysis(reg_param=reg)\n",
    "    qda.fit(x_train.detach().cpu().numpy(), y_train.detach().cpu().numpy())\n",
    "    acc = qda.score(x_val.detach().cpu().numpy(), y_val.detach().cpu().numpy())\n",
    "    if acc > max_acc:\n",
    "        best_params[\"reg_param\"] = reg\n",
    "        max_acc = acc\n",
    "print(f\"Quadratic discriminant analysis: best params: {best_params}, best accuracy: {max_acc}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/18 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "15fb1a98a2c445e6ae8578692a3e4bf5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision tree: best params: {'criterion': 'entropy', 'max_depth': 5}, best accuracy: 0.37815126050420167\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "criterions = [\"gini\", \"entropy\"]\n",
    "max_depths = [None, 5, 6, 7, 8, 9, 10, 20, 25, 30, 40, 50, 60, 70, 80, 90, 100, 150]\n",
    "best_params = {\"criterion\": None, \"max_depth\": None}\n",
    "max_acc = 0.0\n",
    "\n",
    "for max_depth in tqdm(max_depths):\n",
    "    for criterion in criterions:\n",
    "        dtc = DecisionTreeClassifier(criterion=criterion, max_depth=max_depth)\n",
    "        dtc.fit(x_train.detach().cpu().numpy(), y_train.detach().cpu().numpy())\n",
    "        acc = dtc.score(x_val.detach().cpu().numpy(), y_val.detach().cpu().numpy())\n",
    "        if acc > max_acc:\n",
    "            best_params[\"criterion\"] = criterion\n",
    "            best_params[\"max_depth\"] = max_depth\n",
    "            max_acc = acc\n",
    "print(f\"Decision tree: best params: {best_params}, best accuracy: {max_acc}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/18 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9feb4f1fd5474fd5a275e36ab7a09d8e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest: best params: {'n_estimators': 4, 'criterion': 'gini', 'max_depth': 5}, best accuracy: 0.453781512605042\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "estimators = [2, 3, 4, 5, 10, 20, 30, 40, 50, 100, 150, 200, 250, 300, 350, 400, 450, 500]\n",
    "criterions = [\"gini\", \"entropy\"]\n",
    "max_depths = [None, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 20, 25, 30, 50]\n",
    "best_params = {\"n_estimators\": None, \"criterion\": None, \"max_depth\": None}\n",
    "max_acc = 0.0\n",
    "\n",
    "for n in tqdm(estimators):\n",
    "    for max_depth in max_depths:\n",
    "        for criterion in criterions:\n",
    "            rfc = RandomForestClassifier(n_estimators=n, criterion=criterion, max_depth=max_depth, random_state=47)\n",
    "            rfc.fit(x_train.detach().cpu().numpy(), y_train.detach().cpu().numpy())\n",
    "            acc = rfc.score(x_val.detach().cpu().numpy(), y_val.detach().cpu().numpy())\n",
    "            if acc > max_acc:\n",
    "                best_params[\"criterion\"] = criterion\n",
    "                best_params[\"max_depth\"] = max_depth\n",
    "                best_params[\"n_estimators\"] = n\n",
    "                max_acc = acc\n",
    "print(f\"Random Forest: best params: {best_params}, best accuracy: {max_acc}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/13 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2ddf0cf941ce44d1a70a0c2aa90c9251"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost: best params: {'n_estimators': 3, 'criterion': 'gini', 'max_depth': 1}, best accuracy: 0.44537815126050423\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "\n",
    "estimators = [2, 3, 4, 5, 10, 20, 30, 40, 50, 100, 150, 200, 250]\n",
    "criterions = [\"gini\", \"entropy\"]\n",
    "max_depths = [None, 1, 2, 3, 5, 6, 7, 8, 9, 10]\n",
    "best_params = {\"n_estimators\": None, \"criterion\": None, \"max_depth\": None}\n",
    "max_acc = 0.0\n",
    "\n",
    "for n in tqdm(estimators):\n",
    "    for max_depth in max_depths:\n",
    "        for criterion in criterions:\n",
    "            dtc = DecisionTreeClassifier(criterion=criterion, max_depth=max_depth)\n",
    "            adaboost = AdaBoostClassifier(base_estimator=dtc, n_estimators=n, random_state=47)\n",
    "            adaboost.fit(x_train.detach().cpu().numpy(), y_train.detach().cpu().numpy())\n",
    "            acc = adaboost.score(x_val.detach().cpu().numpy(), y_val.detach().cpu().numpy())\n",
    "            if acc > max_acc:\n",
    "                best_params[\"criterion\"] = criterion\n",
    "                best_params[\"max_depth\"] = max_depth\n",
    "                best_params[\"n_estimators\"] = n\n",
    "                max_acc = acc\n",
    "print(f\"AdaBoost: best params: {best_params}, best accuracy: {max_acc}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/13 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "643b7e5531224dfe9d627d41c516f198"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoost: best params: {'n_estimators': 2, 'criterion': 'friedman_mse', 'max_depth': 1, 'loss': None}, best accuracy: 0.4369747899159664\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "\n",
    "estimators = [2, 3, 4, 5, 10, 20, 30, 40, 50, 100, 150, 200, 250]\n",
    "criterions = [\"friedman_mse\", \"squared_error\"]\n",
    "losses = [\"deviance\"]\n",
    "max_depths = [None, 1, 2, 3, 5, 6, 7, 8, 9, 10]\n",
    "best_params = {\"n_estimators\": None, \"criterion\": None, \"max_depth\": None}\n",
    "max_acc = 0.0\n",
    "\n",
    "for n in tqdm(estimators):\n",
    "    for max_depth in max_depths:\n",
    "        for criterion in criterions:\n",
    "            adaboost = GradientBoostingClassifier(n_estimators=n, criterion=criterion, max_depth=max_depth, random_state=47)\n",
    "            adaboost.fit(x_train.detach().cpu().numpy(), y_train.detach().cpu().numpy())\n",
    "            acc = adaboost.score(x_val.detach().cpu().numpy(), y_val.detach().cpu().numpy())\n",
    "            if acc > max_acc:\n",
    "                best_params[\"criterion\"] = criterion\n",
    "                best_params[\"max_depth\"] = max_depth\n",
    "                best_params[\"n_estimators\"] = n\n",
    "                max_acc = acc\n",
    "print(f\"GradientBoost: best params: {best_params}, best accuracy: {max_acc}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
