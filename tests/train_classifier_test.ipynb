{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/prot_prj/PycharmProjects/protein-reconstruction\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "cwd = os.getcwd()\n",
    "if re.search(\"protein-reconstruction.+\", cwd):\n",
    "    os.chdir(\"..\")\n",
    "print(os.getcwd())\n",
    "import torch\n",
    "from torch_geometric.loader import DataLoader\n",
    "from preprocessing.dataset import load_dataset\n",
    "from models.pretraining.graph_infomax import DeepGraphInfomaxV2, RandomPermutationCorruption, MeanPoolReadout\n",
    "from models.pretraining.encoders import RevSAGEConvEncoder\n",
    "from tqdm.autonotebook import tqdm\n",
    "from torchinfo import summary\n",
    "from typing import final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(edge_index=[2, 1616], node_id=[858], coords=[858, 3], meiler=[858, 7], name=[1], dist_mat=[1], num_nodes=858, graph_y=5, x=[858, 10], y=5) Data(edge_index=[2, 1721], node_id=[901], coords=[901, 3], meiler=[901, 7], name=[1], dist_mat=[1], num_nodes=901, graph_y=5, x=[901, 10], y=5)\n"
     ]
    }
   ],
   "source": [
    "ds_train = load_dataset(\"data/cleaned/pscdb/train\")\n",
    "ds_val = load_dataset(\"data/cleaned/pscdb/validation\")\n",
    "dl_train = DataLoader(ds_train, batch_size=1, shuffle=True)\n",
    "dl_val = DataLoader(ds_val, batch_size=1, shuffle=True)\n",
    "print(ds_train[0], ds_val[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================================================\n",
      "Layer (type:depth-idx)                                       Param #\n",
      "=====================================================================================\n",
      "DeepGraphInfomaxV2                                           2,500\n",
      "├─RevSAGEConvEncoder: 1-1                                    --\n",
      "│    └─Linear: 2-1                                           550\n",
      "│    └─LayerNorm: 2-2                                        100\n",
      "│    └─ModuleList: 2-3                                       --\n",
      "│    │    └─GroupAddRev: 3-1                                 --\n",
      "│    │    │    └─ModuleList: 4-1                             --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-1                     65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-2                     65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-3                     65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-4                     65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-5                     65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-6                     65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-7                     65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-8                     65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-9                     65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-10                    65\n",
      "│    │    └─GroupAddRev: 3-2                                 --\n",
      "│    │    │    └─ModuleList: 4-2                             --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-11                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-12                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-13                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-14                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-15                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-16                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-17                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-18                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-19                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-20                    65\n",
      "│    │    └─GroupAddRev: 3-3                                 --\n",
      "│    │    │    └─ModuleList: 4-3                             --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-21                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-22                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-23                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-24                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-25                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-26                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-27                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-28                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-29                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-30                    65\n",
      "│    │    └─GroupAddRev: 3-4                                 --\n",
      "│    │    │    └─ModuleList: 4-4                             --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-31                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-32                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-33                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-34                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-35                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-36                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-37                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-38                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-39                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-40                    65\n",
      "│    │    └─GroupAddRev: 3-5                                 --\n",
      "│    │    │    └─ModuleList: 4-5                             --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-41                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-42                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-43                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-44                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-45                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-46                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-47                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-48                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-49                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-50                    65\n",
      "│    │    └─GroupAddRev: 3-6                                 --\n",
      "│    │    │    └─ModuleList: 4-6                             --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-51                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-52                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-53                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-54                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-55                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-56                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-57                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-58                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-59                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-60                    65\n",
      "│    │    └─GroupAddRev: 3-7                                 --\n",
      "│    │    │    └─ModuleList: 4-7                             --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-61                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-62                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-63                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-64                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-65                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-66                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-67                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-68                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-69                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-70                    65\n",
      "│    │    └─GroupAddRev: 3-8                                 --\n",
      "│    │    │    └─ModuleList: 4-8                             --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-71                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-72                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-73                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-74                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-75                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-76                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-77                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-78                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-79                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-80                    65\n",
      "│    │    └─GroupAddRev: 3-9                                 --\n",
      "│    │    │    └─ModuleList: 4-9                             --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-81                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-82                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-83                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-84                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-85                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-86                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-87                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-88                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-89                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-90                    65\n",
      "│    │    └─GroupAddRev: 3-10                                --\n",
      "│    │    │    └─ModuleList: 4-10                            --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-91                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-92                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-93                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-94                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-95                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-96                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-97                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-98                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-99                    65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-100                   65\n",
      "│    │    └─GroupAddRev: 3-11                                --\n",
      "│    │    │    └─ModuleList: 4-11                            --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-101                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-102                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-103                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-104                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-105                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-106                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-107                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-108                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-109                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-110                   65\n",
      "│    │    └─GroupAddRev: 3-12                                --\n",
      "│    │    │    └─ModuleList: 4-12                            --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-111                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-112                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-113                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-114                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-115                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-116                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-117                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-118                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-119                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-120                   65\n",
      "│    │    └─GroupAddRev: 3-13                                --\n",
      "│    │    │    └─ModuleList: 4-13                            --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-121                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-122                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-123                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-124                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-125                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-126                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-127                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-128                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-129                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-130                   65\n",
      "│    │    └─GroupAddRev: 3-14                                --\n",
      "│    │    │    └─ModuleList: 4-14                            --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-131                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-132                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-133                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-134                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-135                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-136                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-137                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-138                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-139                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-140                   65\n",
      "│    │    └─GroupAddRev: 3-15                                --\n",
      "│    │    │    └─ModuleList: 4-15                            --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-141                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-142                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-143                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-144                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-145                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-146                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-147                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-148                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-149                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-150                   65\n",
      "│    │    └─GroupAddRev: 3-16                                --\n",
      "│    │    │    └─ModuleList: 4-16                            --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-151                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-152                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-153                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-154                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-155                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-156                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-157                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-158                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-159                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-160                   65\n",
      "│    │    └─GroupAddRev: 3-17                                --\n",
      "│    │    │    └─ModuleList: 4-17                            --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-161                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-162                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-163                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-164                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-165                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-166                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-167                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-168                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-169                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-170                   65\n",
      "│    │    └─GroupAddRev: 3-18                                --\n",
      "│    │    │    └─ModuleList: 4-18                            --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-171                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-172                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-173                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-174                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-175                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-176                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-177                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-178                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-179                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-180                   65\n",
      "│    │    └─GroupAddRev: 3-19                                --\n",
      "│    │    │    └─ModuleList: 4-19                            --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-181                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-182                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-183                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-184                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-185                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-186                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-187                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-188                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-189                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-190                   65\n",
      "│    │    └─GroupAddRev: 3-20                                --\n",
      "│    │    │    └─ModuleList: 4-20                            --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-191                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-192                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-193                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-194                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-195                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-196                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-197                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-198                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-199                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-200                   65\n",
      "│    │    └─GroupAddRev: 3-21                                --\n",
      "│    │    │    └─ModuleList: 4-21                            --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-201                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-202                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-203                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-204                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-205                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-206                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-207                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-208                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-209                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-210                   65\n",
      "│    │    └─GroupAddRev: 3-22                                --\n",
      "│    │    │    └─ModuleList: 4-22                            --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-211                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-212                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-213                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-214                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-215                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-216                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-217                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-218                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-219                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-220                   65\n",
      "│    │    └─GroupAddRev: 3-23                                --\n",
      "│    │    │    └─ModuleList: 4-23                            --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-221                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-222                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-223                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-224                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-225                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-226                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-227                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-228                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-229                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-230                   65\n",
      "│    │    └─GroupAddRev: 3-24                                --\n",
      "│    │    │    └─ModuleList: 4-24                            --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-231                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-232                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-233                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-234                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-235                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-236                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-237                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-238                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-239                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-240                   65\n",
      "│    │    └─GroupAddRev: 3-25                                --\n",
      "│    │    │    └─ModuleList: 4-25                            --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-241                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-242                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-243                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-244                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-245                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-246                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-247                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-248                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-249                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-250                   65\n",
      "│    │    └─GroupAddRev: 3-26                                --\n",
      "│    │    │    └─ModuleList: 4-26                            --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-251                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-252                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-253                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-254                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-255                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-256                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-257                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-258                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-259                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-260                   65\n",
      "│    │    └─GroupAddRev: 3-27                                --\n",
      "│    │    │    └─ModuleList: 4-27                            --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-261                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-262                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-263                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-264                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-265                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-266                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-267                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-268                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-269                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-270                   65\n",
      "│    │    └─GroupAddRev: 3-28                                --\n",
      "│    │    │    └─ModuleList: 4-28                            --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-271                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-272                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-273                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-274                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-275                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-276                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-277                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-278                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-279                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-280                   65\n",
      "│    │    └─GroupAddRev: 3-29                                --\n",
      "│    │    │    └─ModuleList: 4-29                            --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-281                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-282                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-283                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-284                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-285                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-286                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-287                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-288                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-289                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-290                   65\n",
      "│    │    └─GroupAddRev: 3-30                                --\n",
      "│    │    │    └─ModuleList: 4-30                            --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-291                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-292                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-293                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-294                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-295                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-296                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-297                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-298                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-299                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-300                   65\n",
      "│    │    └─GroupAddRev: 3-31                                --\n",
      "│    │    │    └─ModuleList: 4-31                            --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-301                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-302                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-303                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-304                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-305                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-306                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-307                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-308                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-309                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-310                   65\n",
      "│    │    └─GroupAddRev: 3-32                                --\n",
      "│    │    │    └─ModuleList: 4-32                            --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-311                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-312                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-313                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-314                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-315                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-316                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-317                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-318                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-319                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-320                   65\n",
      "│    │    └─GroupAddRev: 3-33                                --\n",
      "│    │    │    └─ModuleList: 4-33                            --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-321                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-322                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-323                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-324                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-325                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-326                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-327                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-328                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-329                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-330                   65\n",
      "│    │    └─GroupAddRev: 3-34                                --\n",
      "│    │    │    └─ModuleList: 4-34                            --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-331                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-332                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-333                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-334                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-335                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-336                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-337                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-338                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-339                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-340                   65\n",
      "│    │    └─GroupAddRev: 3-35                                --\n",
      "│    │    │    └─ModuleList: 4-35                            --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-341                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-342                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-343                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-344                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-345                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-346                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-347                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-348                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-349                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-350                   65\n",
      "│    │    └─GroupAddRev: 3-36                                --\n",
      "│    │    │    └─ModuleList: 4-36                            --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-351                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-352                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-353                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-354                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-355                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-356                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-357                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-358                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-359                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-360                   65\n",
      "│    │    └─GroupAddRev: 3-37                                --\n",
      "│    │    │    └─ModuleList: 4-37                            --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-361                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-362                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-363                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-364                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-365                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-366                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-367                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-368                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-369                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-370                   65\n",
      "│    │    └─GroupAddRev: 3-38                                --\n",
      "│    │    │    └─ModuleList: 4-38                            --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-371                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-372                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-373                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-374                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-375                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-376                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-377                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-378                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-379                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-380                   65\n",
      "│    │    └─GroupAddRev: 3-39                                --\n",
      "│    │    │    └─ModuleList: 4-39                            --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-381                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-382                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-383                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-384                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-385                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-386                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-387                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-388                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-389                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-390                   65\n",
      "│    │    └─GroupAddRev: 3-40                                --\n",
      "│    │    │    └─ModuleList: 4-40                            --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-391                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-392                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-393                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-394                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-395                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-396                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-397                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-398                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-399                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-400                   65\n",
      "│    │    └─GroupAddRev: 3-41                                --\n",
      "│    │    │    └─ModuleList: 4-41                            --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-401                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-402                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-403                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-404                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-405                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-406                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-407                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-408                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-409                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-410                   65\n",
      "│    │    └─GroupAddRev: 3-42                                --\n",
      "│    │    │    └─ModuleList: 4-42                            --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-411                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-412                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-413                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-414                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-415                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-416                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-417                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-418                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-419                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-420                   65\n",
      "│    │    └─GroupAddRev: 3-43                                --\n",
      "│    │    │    └─ModuleList: 4-43                            --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-421                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-422                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-423                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-424                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-425                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-426                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-427                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-428                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-429                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-430                   65\n",
      "│    │    └─GroupAddRev: 3-44                                --\n",
      "│    │    │    └─ModuleList: 4-44                            --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-431                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-432                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-433                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-434                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-435                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-436                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-437                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-438                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-439                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-440                   65\n",
      "│    │    └─GroupAddRev: 3-45                                --\n",
      "│    │    │    └─ModuleList: 4-45                            --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-441                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-442                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-443                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-444                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-445                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-446                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-447                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-448                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-449                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-450                   65\n",
      "│    │    └─GroupAddRev: 3-46                                --\n",
      "│    │    │    └─ModuleList: 4-46                            --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-451                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-452                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-453                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-454                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-455                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-456                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-457                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-458                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-459                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-460                   65\n",
      "│    │    └─GroupAddRev: 3-47                                --\n",
      "│    │    │    └─ModuleList: 4-47                            --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-461                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-462                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-463                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-464                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-465                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-466                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-467                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-468                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-469                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-470                   65\n",
      "│    │    └─GroupAddRev: 3-48                                --\n",
      "│    │    │    └─ModuleList: 4-48                            --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-471                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-472                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-473                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-474                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-475                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-476                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-477                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-478                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-479                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-480                   65\n",
      "│    │    └─GroupAddRev: 3-49                                --\n",
      "│    │    │    └─ModuleList: 4-49                            --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-481                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-482                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-483                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-484                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-485                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-486                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-487                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-488                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-489                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-490                   65\n",
      "│    │    └─GroupAddRev: 3-50                                --\n",
      "│    │    │    └─ModuleList: 4-50                            --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-491                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-492                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-493                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-494                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-495                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-496                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-497                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-498                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-499                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-500                   65\n",
      "│    │    └─GroupAddRev: 3-51                                --\n",
      "│    │    │    └─ModuleList: 4-51                            --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-501                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-502                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-503                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-504                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-505                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-506                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-507                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-508                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-509                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-510                   65\n",
      "│    │    └─GroupAddRev: 3-52                                --\n",
      "│    │    │    └─ModuleList: 4-52                            --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-511                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-512                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-513                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-514                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-515                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-516                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-517                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-518                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-519                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-520                   65\n",
      "│    │    └─GroupAddRev: 3-53                                --\n",
      "│    │    │    └─ModuleList: 4-53                            --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-521                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-522                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-523                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-524                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-525                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-526                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-527                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-528                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-529                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-530                   65\n",
      "│    │    └─GroupAddRev: 3-54                                --\n",
      "│    │    │    └─ModuleList: 4-54                            --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-531                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-532                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-533                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-534                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-535                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-536                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-537                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-538                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-539                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-540                   65\n",
      "│    │    └─GroupAddRev: 3-55                                --\n",
      "│    │    │    └─ModuleList: 4-55                            --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-541                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-542                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-543                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-544                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-545                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-546                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-547                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-548                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-549                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-550                   65\n",
      "│    │    └─GroupAddRev: 3-56                                --\n",
      "│    │    │    └─ModuleList: 4-56                            --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-551                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-552                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-553                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-554                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-555                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-556                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-557                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-558                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-559                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-560                   65\n",
      "│    │    └─GroupAddRev: 3-57                                --\n",
      "│    │    │    └─ModuleList: 4-57                            --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-561                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-562                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-563                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-564                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-565                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-566                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-567                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-568                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-569                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-570                   65\n",
      "│    │    └─GroupAddRev: 3-58                                --\n",
      "│    │    │    └─ModuleList: 4-58                            --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-571                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-572                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-573                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-574                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-575                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-576                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-577                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-578                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-579                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-580                   65\n",
      "│    │    └─GroupAddRev: 3-59                                --\n",
      "│    │    │    └─ModuleList: 4-59                            --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-581                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-582                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-583                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-584                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-585                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-586                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-587                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-588                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-589                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-590                   65\n",
      "│    │    └─GroupAddRev: 3-60                                --\n",
      "│    │    │    └─ModuleList: 4-60                            --\n",
      "│    │    │    │    └─SAGEConvBlock: 5-591                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-592                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-593                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-594                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-595                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-596                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-597                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-598                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-599                   65\n",
      "│    │    │    │    └─SAGEConvBlock: 5-600                   65\n",
      "├─LayerNorm: 1-2                                             100\n",
      "=====================================================================================\n",
      "Total params: 42,250\n",
      "Trainable params: 42,250\n",
      "Non-trainable params: 0\n",
      "=====================================================================================\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH_STATE_DICT: final = \"data/fitted/pretraining/dgi/dgi_rev_sage_test5/checkpoint.pt\"\n",
    "MODEL_PATH_PARAMS: final = \"data/fitted/pretraining/dgi/dgi_rev_sage_test5/constructor_params.pt\"\n",
    "\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "state_dict = torch.load(MODEL_PATH_STATE_DICT)\n",
    "constructor_params = torch.load(MODEL_PATH_PARAMS)\n",
    "dgi = DeepGraphInfomaxV2.from_constructor_params(constructor_params, encoder_constructor=RevSAGEConvEncoder, readout=MeanPoolReadout(device, sigmoid=False), corruption=RandomPermutationCorruption(device))\n",
    "print(summary(dgi, device=device, depth=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7e59f9b82e5485cada7b1a2ae9cea91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/550 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done train.\n"
     ]
    }
   ],
   "source": [
    "x_list = []\n",
    "y_list = []\n",
    "dgi.eval()\n",
    "dgi = dgi.to(device)\n",
    "with torch.no_grad():\n",
    "    for el in tqdm(iter(dl_train)):\n",
    "        el = el.to(device)\n",
    "        pos_z, neg_z, summary = dgi(el.x, el.edge_index)\n",
    "        x_list.append(summary)\n",
    "        y_list.append(el.y)\n",
    "    print(\"Done train conversion.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bb368a91ee84aa988c30f6e3e55706b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/119 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done validation.\n"
     ]
    }
   ],
   "source": [
    "x_list_val = []\n",
    "y_list_val = []\n",
    "with torch.no_grad():\n",
    "    for el in tqdm(iter(dl_val)):\n",
    "        el = el.to(device)\n",
    "        pos_z, neg_z, summary = dgi(el.x, el.edge_index)\n",
    "        x_list_val.append(summary)\n",
    "        y_list_val.append(el.y)\n",
    "    print(\"Done validation conversion.\")\n",
    "dgi_2 = dgi.to('cpu')  # free GPU memory\n",
    "del dgi\n",
    "dgi = dgi_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20c5e4e48f4547e5b7fd7155c836e37f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/550 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6687abd01e2b4ad9806a967b65827e16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/119 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50])\n",
      "torch.Size([50])\n",
      "torch.Size([550, 50])\n",
      "torch.Size([550])\n",
      "------------------------------------\n",
      "torch.Size([119, 50])\n",
      "torch.Size([119])\n"
     ]
    }
   ],
   "source": [
    "max_len = 0\n",
    "for i in range(0, len(x_list)):\n",
    "    if x_list[i].shape[0] > max_len:\n",
    "        max_len = x_list[i].shape[0]\n",
    "for i in range(0, len(x_list_val)):\n",
    "    if x_list_val[i].shape[0] > max_len:\n",
    "        max_len = x_list_val[i].shape[0]\n",
    "\n",
    "# Mean embeddings if required\n",
    "for i in tqdm(range(0, len(x_list))):\n",
    "    if len(x_list[i].shape) > 1:\n",
    "        x_list[i] = torch.mean(x_list[i], dim=0, keepdim=False)\n",
    "    #x_list[i] = torch.nn.functional.normalize(x_list[i], p=2.0, dim=0, eps=1e-12)\n",
    "\n",
    "for i in tqdm(range(0, len(x_list_val))):\n",
    "    if len(x_list_val[i].shape) > 1:\n",
    "        x_list_val[i] = torch.mean(x_list_val[i], dim=0, keepdim=False)\n",
    "    #x_list[i] = torch.nn.functional.normalize(x_list[i], p=2.0, dim=0, eps=1e-12)\n",
    "\n",
    "print(x_list[0].shape)\n",
    "print(x_list_val[0].shape)\n",
    "x_train = torch.stack(x_list)\n",
    "y_train = torch.tensor(y_list)\n",
    "x_val = torch.stack(x_list_val)\n",
    "y_val = torch.tensor(y_list_val)\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(\"------------------------------------\")\n",
    "print(x_val.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Newton-Cg Score: 0.3865546218487395\n",
      "liblinear Score: 0.3865546218487395\n",
      "sag Score: 0.3865546218487395\n",
      "saga Score: 0.3865546218487395\n",
      "lbfgs Score: 0.3865546218487395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/scipy/optimize/linesearch.py:327: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/utils/optimize.py:203: UserWarning: Line Search failed\n",
      "  warnings.warn(\"Line Search failed\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "scores = dgi.test(train_z=x_train, train_y=y_train, test_z=x_val, test_y=y_val, solver='newton-cg', max_iter=1000)\n",
    "print(f\"Newton-Cg Score: {scores}\")\n",
    "scores = dgi.test(train_z=x_train, train_y=y_train, test_z=x_val, test_y=y_val, solver='liblinear', max_iter=1000)\n",
    "print(f\"liblinear Score: {scores}\")\n",
    "#scores = dgi.test(train_z=x_train, train_y=y_train, test_z=x_val, test_y=y_val, solver='newton-cholesky')\n",
    "#print(f\"newton-cholesky Score: {scores}\")\n",
    "scores = dgi.test(train_z=x_train, train_y=y_train, test_z=x_val, test_y=y_val, solver='sag', max_iter=1000)\n",
    "print(f\"sag Score: {scores}\")\n",
    "scores = dgi.test(train_z=x_train, train_y=y_train, test_z=x_val, test_y=y_val, solver='saga', max_iter=1000)\n",
    "print(f\"saga Score: {scores}\")\n",
    "scores = dgi.test(train_z=x_train, train_y=y_train, test_z=x_val, test_y=y_val, solver='lbfgs', max_iter=1000)\n",
    "print(f\"lbfgs Score: {scores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.05836798\n",
      "Validation score: 0.000000\n",
      "Iteration 2, loss = 1.89421475\n",
      "Validation score: 0.000000\n",
      "Iteration 3, loss = 1.78238684\n",
      "Validation score: 0.000000\n",
      "Iteration 4, loss = 1.71031610\n",
      "Validation score: 0.090909\n",
      "Iteration 5, loss = 1.67092058\n",
      "Validation score: 0.090909\n",
      "Iteration 6, loss = 1.64363641\n",
      "Validation score: 0.090909\n",
      "Iteration 7, loss = 1.62920071\n",
      "Validation score: 0.090909\n",
      "Iteration 8, loss = 1.61702020\n",
      "Validation score: 0.181818\n",
      "Iteration 9, loss = 1.60619582\n",
      "Validation score: 0.181818\n",
      "Iteration 10, loss = 1.59821643\n",
      "Validation score: 0.181818\n",
      "Iteration 11, loss = 1.59155058\n",
      "Validation score: 0.181818\n",
      "Iteration 12, loss = 1.58463251\n",
      "Validation score: 0.181818\n",
      "Iteration 13, loss = 1.57816571\n",
      "Validation score: 0.181818\n",
      "Iteration 14, loss = 1.57303924\n",
      "Validation score: 0.181818\n",
      "Iteration 15, loss = 1.56853618\n",
      "Validation score: 0.090909\n",
      "Iteration 16, loss = 1.56479977\n",
      "Validation score: 0.090909\n",
      "Iteration 17, loss = 1.56087352\n",
      "Validation score: 0.090909\n",
      "Iteration 18, loss = 1.55725143\n",
      "Validation score: 0.181818\n",
      "Iteration 19, loss = 1.55298395\n",
      "Validation score: 0.181818\n",
      "Iteration 20, loss = 1.54969704\n",
      "Validation score: 0.181818\n",
      "Iteration 21, loss = 1.54652055\n",
      "Validation score: 0.181818\n",
      "Iteration 22, loss = 1.54338626\n",
      "Validation score: 0.181818\n",
      "Iteration 23, loss = 1.54030574\n",
      "Validation score: 0.181818\n",
      "Iteration 24, loss = 1.53740012\n",
      "Validation score: 0.181818\n",
      "Iteration 25, loss = 1.53529925\n",
      "Validation score: 0.181818\n",
      "Iteration 26, loss = 1.53212220\n",
      "Validation score: 0.181818\n",
      "Iteration 27, loss = 1.52945472\n",
      "Validation score: 0.181818\n",
      "Iteration 28, loss = 1.52671098\n",
      "Validation score: 0.181818\n",
      "Iteration 29, loss = 1.52414915\n",
      "Validation score: 0.181818\n",
      "Iteration 30, loss = 1.52110244\n",
      "Validation score: 0.181818\n",
      "Iteration 31, loss = 1.51894055\n",
      "Validation score: 0.181818\n",
      "Iteration 32, loss = 1.51614357\n",
      "Validation score: 0.181818\n",
      "Iteration 33, loss = 1.51401070\n",
      "Validation score: 0.181818\n",
      "Iteration 34, loss = 1.51138836\n",
      "Validation score: 0.181818\n",
      "Iteration 35, loss = 1.50839234\n",
      "Validation score: 0.181818\n",
      "Iteration 36, loss = 1.50619249\n",
      "Validation score: 0.181818\n",
      "Iteration 37, loss = 1.50405374\n",
      "Validation score: 0.181818\n",
      "Iteration 38, loss = 1.50153666\n",
      "Validation score: 0.181818\n",
      "Iteration 39, loss = 1.49926993\n",
      "Validation score: 0.181818\n",
      "Iteration 40, loss = 1.49551621\n",
      "Validation score: 0.181818\n",
      "Iteration 41, loss = 1.49407325\n",
      "Validation score: 0.181818\n",
      "Iteration 42, loss = 1.49024466\n",
      "Validation score: 0.181818\n",
      "Iteration 43, loss = 1.48859652\n",
      "Validation score: 0.181818\n",
      "Iteration 44, loss = 1.48522813\n",
      "Validation score: 0.181818\n",
      "Iteration 45, loss = 1.48237240\n",
      "Validation score: 0.181818\n",
      "Iteration 46, loss = 1.48009532\n",
      "Validation score: 0.181818\n",
      "Iteration 47, loss = 1.47772107\n",
      "Validation score: 0.181818\n",
      "Iteration 48, loss = 1.47514244\n",
      "Validation score: 0.181818\n",
      "Iteration 49, loss = 1.47209443\n",
      "Validation score: 0.181818\n",
      "Iteration 50, loss = 1.46988900\n",
      "Validation score: 0.181818\n",
      "Iteration 51, loss = 1.46800665\n",
      "Validation score: 0.181818\n",
      "Iteration 52, loss = 1.46496182\n",
      "Validation score: 0.181818\n",
      "Iteration 53, loss = 1.46329006\n",
      "Validation score: 0.181818\n",
      "Iteration 54, loss = 1.46055976\n",
      "Validation score: 0.272727\n",
      "Iteration 55, loss = 1.45738833\n",
      "Validation score: 0.181818\n",
      "Iteration 56, loss = 1.45518294\n",
      "Validation score: 0.181818\n",
      "Iteration 57, loss = 1.45313889\n",
      "Validation score: 0.181818\n",
      "Iteration 58, loss = 1.44962744\n",
      "Validation score: 0.181818\n",
      "Iteration 59, loss = 1.44710915\n",
      "Validation score: 0.181818\n",
      "Iteration 60, loss = 1.44565099\n",
      "Validation score: 0.181818\n",
      "Iteration 61, loss = 1.44223960\n",
      "Validation score: 0.181818\n",
      "Iteration 62, loss = 1.43971234\n",
      "Validation score: 0.181818\n",
      "Iteration 63, loss = 1.43754166\n",
      "Validation score: 0.272727\n",
      "Iteration 64, loss = 1.43512665\n",
      "Validation score: 0.181818\n",
      "Iteration 65, loss = 1.43253985\n",
      "Validation score: 0.181818\n",
      "Iteration 66, loss = 1.42927861\n",
      "Validation score: 0.181818\n",
      "Iteration 67, loss = 1.42750704\n",
      "Validation score: 0.181818\n",
      "Iteration 68, loss = 1.42415904\n",
      "Validation score: 0.181818\n",
      "Iteration 69, loss = 1.42195514\n",
      "Validation score: 0.181818\n",
      "Iteration 70, loss = 1.41913092\n",
      "Validation score: 0.181818\n",
      "Iteration 71, loss = 1.41633082\n",
      "Validation score: 0.181818\n",
      "Iteration 72, loss = 1.41465370\n",
      "Validation score: 0.181818\n",
      "Iteration 73, loss = 1.41113627\n",
      "Validation score: 0.181818\n",
      "Iteration 74, loss = 1.40958419\n",
      "Validation score: 0.181818\n",
      "Iteration 75, loss = 1.40663097\n",
      "Validation score: 0.181818\n",
      "Iteration 76, loss = 1.40592009\n",
      "Validation score: 0.181818\n",
      "Iteration 77, loss = 1.40298578\n",
      "Validation score: 0.181818\n",
      "Iteration 78, loss = 1.40078152\n",
      "Validation score: 0.181818\n",
      "Iteration 79, loss = 1.39675011\n",
      "Validation score: 0.181818\n",
      "Iteration 80, loss = 1.39410538\n",
      "Validation score: 0.181818\n",
      "Iteration 81, loss = 1.39309229\n",
      "Validation score: 0.181818\n",
      "Iteration 82, loss = 1.38935157\n",
      "Validation score: 0.181818\n",
      "Iteration 83, loss = 1.38623257\n",
      "Validation score: 0.181818\n",
      "Iteration 84, loss = 1.38401897\n",
      "Validation score: 0.181818\n",
      "Iteration 85, loss = 1.38187892\n",
      "Validation score: 0.181818\n",
      "Iteration 86, loss = 1.37930956\n",
      "Validation score: 0.181818\n",
      "Iteration 87, loss = 1.37716019\n",
      "Validation score: 0.181818\n",
      "Iteration 88, loss = 1.37427926\n",
      "Validation score: 0.181818\n",
      "Iteration 89, loss = 1.37269195\n",
      "Validation score: 0.181818\n",
      "Iteration 90, loss = 1.36996765\n",
      "Validation score: 0.181818\n",
      "Iteration 91, loss = 1.36817875\n",
      "Validation score: 0.181818\n",
      "Iteration 92, loss = 1.36423641\n",
      "Validation score: 0.181818\n",
      "Iteration 93, loss = 1.36404173\n",
      "Validation score: 0.181818\n",
      "Iteration 94, loss = 1.36082669\n",
      "Validation score: 0.181818\n",
      "Iteration 95, loss = 1.35798106\n",
      "Validation score: 0.181818\n",
      "Iteration 96, loss = 1.35581582\n",
      "Validation score: 0.181818\n",
      "Iteration 97, loss = 1.35434699\n",
      "Validation score: 0.181818\n",
      "Iteration 98, loss = 1.35086205\n",
      "Validation score: 0.181818\n",
      "Iteration 99, loss = 1.34835905\n",
      "Validation score: 0.181818\n",
      "Iteration 100, loss = 1.34839886\n",
      "Validation score: 0.181818\n",
      "Iteration 101, loss = 1.34462219\n",
      "Validation score: 0.181818\n",
      "Iteration 102, loss = 1.34212137\n",
      "Validation score: 0.181818\n",
      "Iteration 103, loss = 1.34022569\n",
      "Validation score: 0.181818\n",
      "Iteration 104, loss = 1.33696479\n",
      "Validation score: 0.181818\n",
      "Iteration 105, loss = 1.33616973\n",
      "Validation score: 0.181818\n",
      "Iteration 106, loss = 1.33208981\n",
      "Validation score: 0.181818\n",
      "Iteration 107, loss = 1.33018717\n",
      "Validation score: 0.181818\n",
      "Iteration 108, loss = 1.32843313\n",
      "Validation score: 0.181818\n",
      "Iteration 109, loss = 1.32577554\n",
      "Validation score: 0.181818\n",
      "Iteration 110, loss = 1.32652903\n",
      "Validation score: 0.181818\n",
      "Iteration 111, loss = 1.32072399\n",
      "Validation score: 0.181818\n",
      "Iteration 112, loss = 1.32078283\n",
      "Validation score: 0.181818\n",
      "Iteration 113, loss = 1.31733457\n",
      "Validation score: 0.181818\n",
      "Iteration 114, loss = 1.31417768\n",
      "Validation score: 0.181818\n",
      "Iteration 115, loss = 1.31223660\n",
      "Validation score: 0.181818\n",
      "Iteration 116, loss = 1.30960545\n",
      "Validation score: 0.181818\n",
      "Iteration 117, loss = 1.30966630\n",
      "Validation score: 0.181818\n",
      "Iteration 118, loss = 1.30708282\n",
      "Validation score: 0.181818\n",
      "Iteration 119, loss = 1.30446758\n",
      "Validation score: 0.181818\n",
      "Iteration 120, loss = 1.30357987\n",
      "Validation score: 0.181818\n",
      "Iteration 121, loss = 1.30021825\n",
      "Validation score: 0.181818\n",
      "Iteration 122, loss = 1.29737784\n",
      "Validation score: 0.181818\n",
      "Iteration 123, loss = 1.29593939\n",
      "Validation score: 0.181818\n",
      "Iteration 124, loss = 1.29265409\n",
      "Validation score: 0.181818\n",
      "Iteration 125, loss = 1.29080275\n",
      "Validation score: 0.181818\n",
      "Iteration 126, loss = 1.28890906\n",
      "Validation score: 0.181818\n",
      "Iteration 127, loss = 1.28680068\n",
      "Validation score: 0.181818\n",
      "Iteration 128, loss = 1.28423702\n",
      "Validation score: 0.181818\n",
      "Iteration 129, loss = 1.28216334\n",
      "Validation score: 0.181818\n",
      "Iteration 130, loss = 1.28010110\n",
      "Validation score: 0.181818\n",
      "Iteration 131, loss = 1.27825299\n",
      "Validation score: 0.181818\n",
      "Iteration 132, loss = 1.27587703\n",
      "Validation score: 0.181818\n",
      "Iteration 133, loss = 1.27544029\n",
      "Validation score: 0.181818\n",
      "Iteration 134, loss = 1.27269431\n",
      "Validation score: 0.181818\n",
      "Iteration 135, loss = 1.26931031\n",
      "Validation score: 0.181818\n",
      "Iteration 136, loss = 1.26810652\n",
      "Validation score: 0.181818\n",
      "Iteration 137, loss = 1.26592567\n",
      "Validation score: 0.181818\n",
      "Iteration 138, loss = 1.26376392\n",
      "Validation score: 0.181818\n",
      "Iteration 139, loss = 1.26151801\n",
      "Validation score: 0.181818\n",
      "Iteration 140, loss = 1.25826332\n",
      "Validation score: 0.181818\n",
      "Iteration 141, loss = 1.25686751\n",
      "Validation score: 0.181818\n",
      "Iteration 142, loss = 1.25498430\n",
      "Validation score: 0.181818\n",
      "Iteration 143, loss = 1.25240518\n",
      "Validation score: 0.181818\n",
      "Iteration 144, loss = 1.25136347\n",
      "Validation score: 0.181818\n",
      "Iteration 145, loss = 1.24861348\n",
      "Validation score: 0.181818\n",
      "Iteration 146, loss = 1.24856169\n",
      "Validation score: 0.181818\n",
      "Iteration 147, loss = 1.24508454\n",
      "Validation score: 0.181818\n",
      "Iteration 148, loss = 1.24368916\n",
      "Validation score: 0.181818\n",
      "Iteration 149, loss = 1.24196213\n",
      "Validation score: 0.181818\n",
      "Iteration 150, loss = 1.23883846\n",
      "Validation score: 0.181818\n",
      "Iteration 151, loss = 1.23664760\n",
      "Validation score: 0.181818\n",
      "Iteration 152, loss = 1.23613275\n",
      "Validation score: 0.181818\n",
      "Iteration 153, loss = 1.23348564\n",
      "Validation score: 0.181818\n",
      "Iteration 154, loss = 1.23188771\n",
      "Validation score: 0.181818\n",
      "Iteration 155, loss = 1.22870473\n",
      "Validation score: 0.181818\n",
      "Iteration 156, loss = 1.23141102\n",
      "Validation score: 0.181818\n",
      "Iteration 157, loss = 1.22763934\n",
      "Validation score: 0.181818\n",
      "Iteration 158, loss = 1.22515444\n",
      "Validation score: 0.181818\n",
      "Iteration 159, loss = 1.22093868\n",
      "Validation score: 0.181818\n",
      "Iteration 160, loss = 1.22129942\n",
      "Validation score: 0.181818\n",
      "Iteration 161, loss = 1.21691190\n",
      "Validation score: 0.181818\n",
      "Iteration 162, loss = 1.21532469\n",
      "Validation score: 0.181818\n",
      "Iteration 163, loss = 1.21403849\n",
      "Validation score: 0.181818\n",
      "Iteration 164, loss = 1.21144684\n",
      "Validation score: 0.181818\n",
      "Iteration 165, loss = 1.21092238\n",
      "Validation score: 0.181818\n",
      "Iteration 166, loss = 1.20851344\n",
      "Validation score: 0.181818\n",
      "Iteration 167, loss = 1.20917612\n",
      "Validation score: 0.181818\n",
      "Iteration 168, loss = 1.20363737\n",
      "Validation score: 0.181818\n",
      "Iteration 169, loss = 1.20151138\n",
      "Validation score: 0.181818\n",
      "Iteration 170, loss = 1.20040065\n",
      "Validation score: 0.181818\n",
      "Iteration 171, loss = 1.19870259\n",
      "Validation score: 0.181818\n",
      "Iteration 172, loss = 1.19887504\n",
      "Validation score: 0.181818\n",
      "Iteration 173, loss = 1.19558916\n",
      "Validation score: 0.181818\n",
      "Iteration 174, loss = 1.19265284\n",
      "Validation score: 0.181818\n",
      "Iteration 175, loss = 1.18971768\n",
      "Validation score: 0.181818\n",
      "Iteration 176, loss = 1.18814952\n",
      "Validation score: 0.181818\n",
      "Iteration 177, loss = 1.18680736\n",
      "Validation score: 0.181818\n",
      "Iteration 178, loss = 1.18489533\n",
      "Validation score: 0.181818\n",
      "Iteration 179, loss = 1.18300063\n",
      "Validation score: 0.181818\n",
      "Iteration 180, loss = 1.18159440\n",
      "Validation score: 0.181818\n",
      "Iteration 181, loss = 1.17887787\n",
      "Validation score: 0.181818\n",
      "Iteration 182, loss = 1.17663709\n",
      "Validation score: 0.181818\n",
      "Iteration 183, loss = 1.17585356\n",
      "Validation score: 0.181818\n",
      "Iteration 184, loss = 1.17374724\n",
      "Validation score: 0.181818\n",
      "Iteration 185, loss = 1.17040137\n",
      "Validation score: 0.181818\n",
      "Iteration 186, loss = 1.17004568\n",
      "Validation score: 0.181818\n",
      "Iteration 187, loss = 1.16662334\n",
      "Validation score: 0.181818\n",
      "Iteration 188, loss = 1.16510594\n",
      "Validation score: 0.181818\n",
      "Iteration 189, loss = 1.16446869\n",
      "Validation score: 0.181818\n",
      "Iteration 190, loss = 1.16273889\n",
      "Validation score: 0.181818\n",
      "Iteration 191, loss = 1.16263147\n",
      "Validation score: 0.181818\n",
      "Iteration 192, loss = 1.15979805\n",
      "Validation score: 0.181818\n",
      "Iteration 193, loss = 1.15777811\n",
      "Validation score: 0.181818\n",
      "Iteration 194, loss = 1.15676757\n",
      "Validation score: 0.181818\n",
      "Iteration 195, loss = 1.15193413\n",
      "Validation score: 0.181818\n",
      "Iteration 196, loss = 1.15171334\n",
      "Validation score: 0.181818\n",
      "Iteration 197, loss = 1.15099696\n",
      "Validation score: 0.181818\n",
      "Iteration 198, loss = 1.14786503\n",
      "Validation score: 0.181818\n",
      "Iteration 199, loss = 1.14640842\n",
      "Validation score: 0.181818\n",
      "Iteration 200, loss = 1.14531595\n",
      "Validation score: 0.181818\n",
      "Iteration 201, loss = 1.14120510\n",
      "Validation score: 0.181818\n",
      "Iteration 202, loss = 1.14252576\n",
      "Validation score: 0.181818\n",
      "Iteration 203, loss = 1.13881931\n",
      "Validation score: 0.181818\n",
      "Iteration 204, loss = 1.13751976\n",
      "Validation score: 0.181818\n",
      "Iteration 205, loss = 1.13331665\n",
      "Validation score: 0.181818\n",
      "Iteration 206, loss = 1.13298593\n",
      "Validation score: 0.181818\n",
      "Iteration 207, loss = 1.13003104\n",
      "Validation score: 0.181818\n",
      "Iteration 208, loss = 1.12901235\n",
      "Validation score: 0.181818\n",
      "Iteration 209, loss = 1.12779019\n",
      "Validation score: 0.181818\n",
      "Iteration 210, loss = 1.12468464\n",
      "Validation score: 0.181818\n",
      "Iteration 211, loss = 1.12321699\n",
      "Validation score: 0.181818\n",
      "Iteration 212, loss = 1.11956592\n",
      "Validation score: 0.181818\n",
      "Iteration 213, loss = 1.11806200\n",
      "Validation score: 0.181818\n",
      "Iteration 214, loss = 1.11809707\n",
      "Validation score: 0.181818\n",
      "Iteration 215, loss = 1.11616197\n",
      "Validation score: 0.181818\n",
      "Iteration 216, loss = 1.11389327\n",
      "Validation score: 0.181818\n",
      "Iteration 217, loss = 1.11422701\n",
      "Validation score: 0.181818\n",
      "Iteration 218, loss = 1.11043830\n",
      "Validation score: 0.181818\n",
      "Iteration 219, loss = 1.10992569\n",
      "Validation score: 0.181818\n",
      "Iteration 220, loss = 1.10667826\n",
      "Validation score: 0.181818\n",
      "Iteration 221, loss = 1.10583911\n",
      "Validation score: 0.181818\n",
      "Iteration 222, loss = 1.10294239\n",
      "Validation score: 0.181818\n",
      "Iteration 223, loss = 1.10349244\n",
      "Validation score: 0.181818\n",
      "Iteration 224, loss = 1.10072014\n",
      "Validation score: 0.181818\n",
      "Iteration 225, loss = 1.09947121\n",
      "Validation score: 0.181818\n",
      "Iteration 226, loss = 1.09750998\n",
      "Validation score: 0.181818\n",
      "Iteration 227, loss = 1.09468031\n",
      "Validation score: 0.181818\n",
      "Iteration 228, loss = 1.09277562\n",
      "Validation score: 0.181818\n",
      "Iteration 229, loss = 1.09269516\n",
      "Validation score: 0.181818\n",
      "Iteration 230, loss = 1.08973164\n",
      "Validation score: 0.181818\n",
      "Iteration 231, loss = 1.08805136\n",
      "Validation score: 0.181818\n",
      "Iteration 232, loss = 1.08605868\n",
      "Validation score: 0.181818\n",
      "Iteration 233, loss = 1.08457512\n",
      "Validation score: 0.181818\n",
      "Iteration 234, loss = 1.08101257\n",
      "Validation score: 0.181818\n",
      "Validation score did not improve more than tol=0.000000 for 170 consecutive epochs. Stopping.\n",
      "0.4369747899159664\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(50, 50), verbose=True, warm_start=False, early_stopping=True, learning_rate='adaptive', max_iter=300, validation_fraction=0.02, n_iter_no_change=170, solver='adam',\n",
    "                    tol=1e-30)\n",
    "mlp.fit(x_train.detach().cpu().numpy(), y_train.detach().cpu().numpy())\n",
    "print(mlp.score(x_val.detach().cpu().numpy(), y_val.detach().cpu().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/4 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7e55042fda054d15a7e7bbc037ba0697"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM: best_params: {'kernel': 'rbf', 'C': 0.0001, 'decision_shape': 'ovr', 'degree': None}, accuracy: 0.4369747899159664\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "kernels = [\"rbf\", \"linear\", \"sigmoid\", \"poly\"]\n",
    "decision_shapes = [\"ovr\", \"ovo\"]\n",
    "penalties = [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0, 1.5]\n",
    "degrees = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 20, 25, 30, 40, 50]\n",
    "max_acc: float = 0.0\n",
    "best_params = {\"kernel\": None, \"C\": None, \"decision_shape\": None, \"degree\": None}\n",
    "\n",
    "\n",
    "for kernel in tqdm(kernels):\n",
    "    for decision_shape in decision_shapes:\n",
    "        for c in penalties:\n",
    "            if kernel == \"poly\":\n",
    "                for degree in penalties:\n",
    "                    svm = SVC(kernel=kernel, decision_function_shape=decision_shape, C=c, degree=degree, random_state=47)\n",
    "                    svm.fit(x_train.detach().cpu().numpy(), y_train.detach().cpu().numpy())\n",
    "                    acc = svm.score(x_val.detach().cpu().numpy(), y_val.detach().cpu().numpy())\n",
    "\n",
    "                    if acc > max_acc:\n",
    "                        best_params[\"kernel\"] = kernel\n",
    "                        best_params[\"C\"] = c\n",
    "                        best_params[\"decision_shape\"] = decision_shape\n",
    "                        best_params[\"degree\"] = degree\n",
    "            else:\n",
    "                svm = SVC(kernel=kernel, decision_function_shape=decision_shape, C=c, random_state=47)\n",
    "                svm.fit(x_train.detach().cpu().numpy(), y_train.detach().cpu().numpy())\n",
    "                acc = svm.score(x_val.detach().cpu().numpy(), y_val.detach().cpu().numpy())\n",
    "\n",
    "                if acc > max_acc:\n",
    "                    max_acc = acc\n",
    "                    best_params[\"kernel\"] = kernel\n",
    "                    best_params[\"C\"] = c\n",
    "                    best_params[\"decision_shape\"] = decision_shape\n",
    "                    best_params[\"degree\"] = None\n",
    "\n",
    "print(f\"SVM: best_params: {best_params}, accuracy: {max_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/148 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6b86fb66d0634f7fb3b40c3af394089d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k: 2, score: 0.14285714285714285\n",
      "k: 3, score: 0.19327731092436976\n",
      "k: 4, score: 0.20168067226890757\n",
      "k: 5, score: 0.19327731092436976\n",
      "k: 6, score: 0.21008403361344538\n",
      "k: 7, score: 0.23529411764705882\n",
      "k: 8, score: 0.2773109243697479\n",
      "k: 9, score: 0.2773109243697479\n",
      "k: 10, score: 0.31932773109243695\n",
      "k: 11, score: 0.2773109243697479\n",
      "k: 12, score: 0.31932773109243695\n",
      "k: 13, score: 0.31932773109243695\n",
      "k: 14, score: 0.3277310924369748\n",
      "k: 15, score: 0.33613445378151263\n",
      "k: 16, score: 0.3445378151260504\n",
      "k: 17, score: 0.35294117647058826\n",
      "k: 18, score: 0.3445378151260504\n",
      "k: 19, score: 0.35294117647058826\n",
      "k: 20, score: 0.35294117647058826\n",
      "k: 21, score: 0.3949579831932773\n",
      "k: 22, score: 0.3697478991596639\n",
      "k: 23, score: 0.3697478991596639\n",
      "k: 24, score: 0.3697478991596639\n",
      "k: 25, score: 0.37815126050420167\n",
      "k: 26, score: 0.36134453781512604\n",
      "k: 27, score: 0.36134453781512604\n",
      "k: 28, score: 0.37815126050420167\n",
      "k: 29, score: 0.35294117647058826\n",
      "k: 30, score: 0.36134453781512604\n",
      "k: 31, score: 0.3697478991596639\n",
      "k: 32, score: 0.3697478991596639\n",
      "k: 33, score: 0.3949579831932773\n",
      "k: 34, score: 0.37815126050420167\n",
      "k: 35, score: 0.3865546218487395\n",
      "k: 36, score: 0.3865546218487395\n",
      "k: 37, score: 0.3865546218487395\n",
      "k: 38, score: 0.3949579831932773\n",
      "k: 39, score: 0.40336134453781514\n",
      "k: 40, score: 0.3949579831932773\n",
      "k: 41, score: 0.3865546218487395\n",
      "k: 42, score: 0.3949579831932773\n",
      "k: 43, score: 0.3949579831932773\n",
      "k: 44, score: 0.3949579831932773\n",
      "k: 45, score: 0.42016806722689076\n",
      "k: 46, score: 0.4117647058823529\n",
      "k: 47, score: 0.40336134453781514\n",
      "k: 48, score: 0.4117647058823529\n",
      "k: 49, score: 0.42016806722689076\n",
      "k: 50, score: 0.42857142857142855\n",
      "k: 51, score: 0.42016806722689076\n",
      "k: 52, score: 0.42016806722689076\n",
      "k: 53, score: 0.42016806722689076\n",
      "k: 54, score: 0.42016806722689076\n",
      "k: 55, score: 0.4117647058823529\n",
      "k: 56, score: 0.4117647058823529\n",
      "k: 57, score: 0.4117647058823529\n",
      "k: 58, score: 0.42016806722689076\n",
      "k: 59, score: 0.42016806722689076\n",
      "k: 60, score: 0.42016806722689076\n",
      "k: 61, score: 0.42016806722689076\n",
      "k: 62, score: 0.42016806722689076\n",
      "k: 63, score: 0.42016806722689076\n",
      "k: 64, score: 0.42016806722689076\n",
      "k: 65, score: 0.42857142857142855\n",
      "k: 66, score: 0.4369747899159664\n",
      "k: 67, score: 0.44537815126050423\n",
      "k: 68, score: 0.44537815126050423\n",
      "k: 69, score: 0.44537815126050423\n",
      "k: 70, score: 0.44537815126050423\n",
      "k: 71, score: 0.44537815126050423\n",
      "k: 72, score: 0.4369747899159664\n",
      "k: 73, score: 0.44537815126050423\n",
      "k: 74, score: 0.44537815126050423\n",
      "k: 75, score: 0.44537815126050423\n",
      "k: 76, score: 0.44537815126050423\n",
      "k: 77, score: 0.44537815126050423\n",
      "k: 78, score: 0.44537815126050423\n",
      "k: 79, score: 0.44537815126050423\n",
      "k: 80, score: 0.44537815126050423\n",
      "k: 81, score: 0.44537815126050423\n",
      "k: 82, score: 0.4369747899159664\n",
      "k: 83, score: 0.4369747899159664\n",
      "k: 84, score: 0.44537815126050423\n",
      "k: 85, score: 0.4369747899159664\n",
      "k: 86, score: 0.4369747899159664\n",
      "k: 87, score: 0.4369747899159664\n",
      "k: 88, score: 0.4369747899159664\n",
      "k: 89, score: 0.4369747899159664\n",
      "k: 90, score: 0.4369747899159664\n",
      "k: 91, score: 0.4369747899159664\n",
      "k: 92, score: 0.4369747899159664\n",
      "k: 93, score: 0.4369747899159664\n",
      "k: 94, score: 0.4369747899159664\n",
      "k: 95, score: 0.4369747899159664\n",
      "k: 96, score: 0.4369747899159664\n",
      "k: 97, score: 0.4369747899159664\n",
      "k: 98, score: 0.4369747899159664\n",
      "k: 99, score: 0.4369747899159664\n",
      "k: 100, score: 0.4369747899159664\n",
      "k: 101, score: 0.4369747899159664\n",
      "k: 102, score: 0.4369747899159664\n",
      "k: 103, score: 0.4369747899159664\n",
      "k: 104, score: 0.4369747899159664\n",
      "k: 105, score: 0.4369747899159664\n",
      "k: 106, score: 0.4369747899159664\n",
      "k: 107, score: 0.4369747899159664\n",
      "k: 108, score: 0.4369747899159664\n",
      "k: 109, score: 0.4369747899159664\n",
      "k: 110, score: 0.4369747899159664\n",
      "k: 111, score: 0.4369747899159664\n",
      "k: 112, score: 0.4369747899159664\n",
      "k: 113, score: 0.4369747899159664\n",
      "k: 114, score: 0.4369747899159664\n",
      "k: 115, score: 0.4369747899159664\n",
      "k: 116, score: 0.4369747899159664\n",
      "k: 117, score: 0.4369747899159664\n",
      "k: 118, score: 0.4369747899159664\n",
      "k: 119, score: 0.4369747899159664\n",
      "k: 120, score: 0.4369747899159664\n",
      "k: 121, score: 0.4369747899159664\n",
      "k: 122, score: 0.4369747899159664\n",
      "k: 123, score: 0.4369747899159664\n",
      "k: 124, score: 0.4369747899159664\n",
      "k: 125, score: 0.4369747899159664\n",
      "k: 126, score: 0.4369747899159664\n",
      "k: 127, score: 0.4369747899159664\n",
      "k: 128, score: 0.4369747899159664\n",
      "k: 129, score: 0.4369747899159664\n",
      "k: 130, score: 0.4369747899159664\n",
      "k: 131, score: 0.4369747899159664\n",
      "k: 132, score: 0.4369747899159664\n",
      "k: 133, score: 0.4369747899159664\n",
      "k: 134, score: 0.4369747899159664\n",
      "k: 135, score: 0.4369747899159664\n",
      "k: 136, score: 0.4369747899159664\n",
      "k: 137, score: 0.4369747899159664\n",
      "k: 138, score: 0.4369747899159664\n",
      "k: 139, score: 0.4369747899159664\n",
      "k: 140, score: 0.4369747899159664\n",
      "k: 141, score: 0.4369747899159664\n",
      "k: 142, score: 0.4369747899159664\n",
      "k: 143, score: 0.4369747899159664\n",
      "k: 144, score: 0.4369747899159664\n",
      "k: 145, score: 0.4369747899159664\n",
      "k: 146, score: 0.4369747899159664\n",
      "k: 147, score: 0.4369747899159664\n",
      "k: 148, score: 0.4369747899159664\n",
      "k: 149, score: 0.4369747899159664\n",
      "Best k: 67, best score: 0.44537815126050423\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "max_acc_k = 2\n",
    "max_acc: float = 0.0\n",
    "max_neighbors = 150\n",
    "for k in tqdm(range(2, max_neighbors)):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k, n_jobs=-1)\n",
    "    knn.fit(x_train.detach().cpu().numpy(), y_train.detach().cpu().numpy())\n",
    "    acc = knn.score(x_val.detach().cpu().numpy(), y_val.detach().cpu().numpy())\n",
    "\n",
    "    if acc > max_acc:\n",
    "        max_acc = acc\n",
    "        max_acc_k = k\n",
    "    print(f\"k: {k}, score: {acc}\")\n",
    "\n",
    "print(f\"Best k: {max_acc_k}, best score: {max_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4369747899159664, warm_starts: True, n_restarts: 0, kernel: RBF(length_scale=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4369747899159664, warm_starts: False, n_restarts: 0, kernel: RBF(length_scale=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4369747899159664, warm_starts: True, n_restarts: 5, kernel: RBF(length_scale=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4369747899159664, warm_starts: False, n_restarts: 5, kernel: RBF(length_scale=1)\n",
      "Accuracy: 0.4369747899159664, warm_starts: True, n_restarts: 10, kernel: RBF(length_scale=1)\n",
      "Accuracy: 0.4369747899159664, warm_starts: False, n_restarts: 10, kernel: RBF(length_scale=1)\n",
      "Accuracy: 0.4369747899159664, warm_starts: True, n_restarts: 15, kernel: RBF(length_scale=1)\n",
      "Accuracy: 0.4369747899159664, warm_starts: False, n_restarts: 15, kernel: RBF(length_scale=1)\n",
      "Accuracy: 0.4369747899159664, warm_starts: True, n_restarts: 20, kernel: RBF(length_scale=1)\n",
      "Accuracy: 0.4369747899159664, warm_starts: False, n_restarts: 20, kernel: RBF(length_scale=1)\n",
      "Accuracy: 0.4369747899159664, warm_starts: True, n_restarts: 25, kernel: RBF(length_scale=1)\n",
      "Accuracy: 0.4369747899159664, warm_starts: False, n_restarts: 25, kernel: RBF(length_scale=1)\n",
      "Accuracy: 0.4369747899159664, warm_starts: True, n_restarts: 0, kernel: WhiteKernel(noise_level=1)\n",
      "Accuracy: 0.4369747899159664, warm_starts: False, n_restarts: 0, kernel: WhiteKernel(noise_level=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter noise_level is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter noise_level is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter noise_level is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter noise_level is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter noise_level is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter noise_level is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4369747899159664, warm_starts: True, n_restarts: 5, kernel: WhiteKernel(noise_level=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter noise_level is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter noise_level is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter noise_level is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter noise_level is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter noise_level is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter noise_level is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4369747899159664, warm_starts: False, n_restarts: 5, kernel: WhiteKernel(noise_level=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter noise_level is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter noise_level is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter noise_level is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter noise_level is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter noise_level is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter noise_level is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4369747899159664, warm_starts: True, n_restarts: 10, kernel: WhiteKernel(noise_level=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter noise_level is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter noise_level is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4369747899159664, warm_starts: False, n_restarts: 10, kernel: WhiteKernel(noise_level=1)\n",
      "Accuracy: 0.4369747899159664, warm_starts: True, n_restarts: 15, kernel: WhiteKernel(noise_level=1)\n",
      "Accuracy: 0.4369747899159664, warm_starts: False, n_restarts: 15, kernel: WhiteKernel(noise_level=1)\n",
      "Accuracy: 0.4369747899159664, warm_starts: True, n_restarts: 20, kernel: WhiteKernel(noise_level=1)\n",
      "Accuracy: 0.4369747899159664, warm_starts: False, n_restarts: 20, kernel: WhiteKernel(noise_level=1)\n",
      "Accuracy: 0.4369747899159664, warm_starts: True, n_restarts: 25, kernel: WhiteKernel(noise_level=1)\n",
      "Accuracy: 0.4369747899159664, warm_starts: False, n_restarts: 25, kernel: WhiteKernel(noise_level=1)\n",
      "Accuracy: 0.4369747899159664, warm_starts: True, n_restarts: 0, kernel: 1**2\n",
      "Accuracy: 0.4369747899159664, warm_starts: False, n_restarts: 0, kernel: 1**2\n",
      "Accuracy: 0.4369747899159664, warm_starts: True, n_restarts: 5, kernel: 1**2\n",
      "Accuracy: 0.4369747899159664, warm_starts: False, n_restarts: 5, kernel: 1**2\n",
      "Accuracy: 0.4369747899159664, warm_starts: True, n_restarts: 10, kernel: 1**2\n",
      "Accuracy: 0.4369747899159664, warm_starts: False, n_restarts: 10, kernel: 1**2\n",
      "Accuracy: 0.4369747899159664, warm_starts: True, n_restarts: 15, kernel: 1**2\n",
      "Accuracy: 0.4369747899159664, warm_starts: False, n_restarts: 15, kernel: 1**2\n",
      "Accuracy: 0.4369747899159664, warm_starts: True, n_restarts: 20, kernel: 1**2\n",
      "Accuracy: 0.4369747899159664, warm_starts: False, n_restarts: 20, kernel: 1**2\n",
      "Accuracy: 0.4369747899159664, warm_starts: True, n_restarts: 25, kernel: 1**2\n",
      "Accuracy: 0.4369747899159664, warm_starts: False, n_restarts: 25, kernel: 1**2\n",
      "Accuracy: 0.3865546218487395, warm_starts: True, n_restarts: 0, kernel: PairwiseKernel(gamma=1.0, metric=linear)\n",
      "Accuracy: 0.3865546218487395, warm_starts: False, n_restarts: 0, kernel: PairwiseKernel(gamma=1.0, metric=linear)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter gamma is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter gamma is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter gamma is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter gamma is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3865546218487395, warm_starts: True, n_restarts: 5, kernel: PairwiseKernel(gamma=1.0, metric=linear)\n",
      "Accuracy: 0.3865546218487395, warm_starts: False, n_restarts: 5, kernel: PairwiseKernel(gamma=1.0, metric=linear)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter gamma is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter gamma is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter gamma is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter gamma is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter gamma is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3865546218487395, warm_starts: True, n_restarts: 10, kernel: PairwiseKernel(gamma=1.0, metric=linear)\n",
      "Accuracy: 0.3865546218487395, warm_starts: False, n_restarts: 10, kernel: PairwiseKernel(gamma=1.0, metric=linear)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter gamma is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter gamma is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter gamma is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter gamma is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3865546218487395, warm_starts: True, n_restarts: 15, kernel: PairwiseKernel(gamma=1.0, metric=linear)\n",
      "Accuracy: 0.3865546218487395, warm_starts: False, n_restarts: 15, kernel: PairwiseKernel(gamma=1.0, metric=linear)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter gamma is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter gamma is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter gamma is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3865546218487395, warm_starts: True, n_restarts: 20, kernel: PairwiseKernel(gamma=1.0, metric=linear)\n",
      "Accuracy: 0.3865546218487395, warm_starts: False, n_restarts: 20, kernel: PairwiseKernel(gamma=1.0, metric=linear)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter gamma is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter gamma is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter gamma is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter gamma is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3865546218487395, warm_starts: True, n_restarts: 25, kernel: PairwiseKernel(gamma=1.0, metric=linear)\n",
      "Accuracy: 0.3865546218487395, warm_starts: False, n_restarts: 25, kernel: PairwiseKernel(gamma=1.0, metric=linear)\n",
      "Best params: {'n_restarts': 0, 'warm_start': True, 'kernel': RBF(length_scale=1)}, best accuracy: 0.4369747899159664\n"
     ]
    }
   ],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF, WhiteKernel, ConstantKernel, PairwiseKernel\n",
    "\n",
    "\n",
    "n_restarts = [0, 5, 10, 15, 20, 25]\n",
    "kernels = [RBF(), WhiteKernel(), ConstantKernel(), PairwiseKernel()]\n",
    "warm_starts = [True, False]\n",
    "max_acc = 0.0\n",
    "best_params = {\"n_restarts\": None, \"warm_start\": None, \"kernel\": None}\n",
    "\n",
    "for kernel in kernels:\n",
    "    for n in n_restarts:\n",
    "        for ws in warm_starts:\n",
    "            gpc = GaussianProcessClassifier(kernel=kernel, n_restarts_optimizer=n, max_iter_predict=100, warm_start=ws, random_state=47, n_jobs=-1)\n",
    "            gpc.fit(x_train.detach().cpu().numpy(), y_train.detach().cpu().numpy())\n",
    "            acc = gpc.score(x_val.detach().cpu().numpy(), y_val.detach().cpu().numpy())\n",
    "            if acc > max_acc:\n",
    "                max_acc = acc\n",
    "                best_params[\"n_restarts\"] = n\n",
    "                best_params[\"warm_start\"] = ws\n",
    "                best_params[\"kernel\"] = kernel\n",
    "            print(f\"Accuracy: {acc}, warm_starts: {ws}, n_restarts: {n}, kernel: {kernel}\")\n",
    "print(f\"Best params: {best_params}, best accuracy: {max_acc}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14285714285714285\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "nbc = GaussianNB()\n",
    "nbc.fit(x_train.detach().cpu().numpy(), y_train.detach().cpu().numpy())\n",
    "acc = nbc.score(x_val.detach().cpu().numpy(), y_val.detach().cpu().numpy())\n",
    "print(acc)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear discriminant analysis: best params: {'solver': 'lsqr'}, best accuracy: 0.33613445378151263\n",
      "Quadratic discriminant analysis: best params: {'reg_param': 0.5}, best accuracy: 0.3445378151260504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/discriminant_analysis.py:878: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/discriminant_analysis.py:878: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/discriminant_analysis.py:878: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/discriminant_analysis.py:878: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/discriminant_analysis.py:878: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/discriminant_analysis.py:878: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/discriminant_analysis.py:878: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/discriminant_analysis.py:878: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/discriminant_analysis.py:878: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/discriminant_analysis.py:878: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/discriminant_analysis.py:878: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "/home/prot_prj/anaconda3/envs/prot_proj/lib/python3.9/site-packages/sklearn/discriminant_analysis.py:878: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "\n",
    "solvers = [\"svd\", \"lsqr\", \"eigen\"]\n",
    "reg_params = [0.0, 0.01, 0.05, 0.1, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 1.0, 1.1]\n",
    "max_acc = 0.0\n",
    "best_params = {\"solver\": None}\n",
    "\n",
    "for solver in solvers:\n",
    "    lda = LinearDiscriminantAnalysis(solver=solver)\n",
    "    lda.fit(x_train.detach().cpu().numpy(), y_train.detach().cpu().numpy())\n",
    "    acc = lda.score(x_val.detach().cpu().numpy(), y_val.detach().cpu().numpy())\n",
    "\n",
    "    if acc > max_acc:\n",
    "        best_params[\"solver\"] = solver\n",
    "        max_acc = acc\n",
    "print(f\"Linear discriminant analysis: best params: {best_params}, best accuracy: {max_acc}\")\n",
    "\n",
    "best_params = {\"reg_param\": None}\n",
    "max_acc = 0.0\n",
    "for reg in reg_params:\n",
    "    qda = QuadraticDiscriminantAnalysis(reg_param=reg)\n",
    "    qda.fit(x_train.detach().cpu().numpy(), y_train.detach().cpu().numpy())\n",
    "    acc = qda.score(x_val.detach().cpu().numpy(), y_val.detach().cpu().numpy())\n",
    "    if acc > max_acc:\n",
    "        best_params[\"reg_param\"] = reg\n",
    "        max_acc = acc\n",
    "print(f\"Quadratic discriminant analysis: best params: {best_params}, best accuracy: {max_acc}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/18 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "15fb1a98a2c445e6ae8578692a3e4bf5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision tree: best params: {'criterion': 'entropy', 'max_depth': 5}, best accuracy: 0.37815126050420167\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "criterions = [\"gini\", \"entropy\"]\n",
    "max_depths = [None, 5, 6, 7, 8, 9, 10, 20, 25, 30, 40, 50, 60, 70, 80, 90, 100, 150]\n",
    "best_params = {\"criterion\": None, \"max_depth\": None}\n",
    "max_acc = 0.0\n",
    "\n",
    "for max_depth in tqdm(max_depths):\n",
    "    for criterion in criterions:\n",
    "        dtc = DecisionTreeClassifier(criterion=criterion, max_depth=max_depth)\n",
    "        dtc.fit(x_train.detach().cpu().numpy(), y_train.detach().cpu().numpy())\n",
    "        acc = dtc.score(x_val.detach().cpu().numpy(), y_val.detach().cpu().numpy())\n",
    "        if acc > max_acc:\n",
    "            best_params[\"criterion\"] = criterion\n",
    "            best_params[\"max_depth\"] = max_depth\n",
    "            max_acc = acc\n",
    "print(f\"Decision tree: best params: {best_params}, best accuracy: {max_acc}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/18 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9feb4f1fd5474fd5a275e36ab7a09d8e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest: best params: {'n_estimators': 4, 'criterion': 'gini', 'max_depth': 5}, best accuracy: 0.453781512605042\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "estimators = [2, 3, 4, 5, 10, 20, 30, 40, 50, 100, 150, 200, 250, 300, 350, 400, 450, 500]\n",
    "criterions = [\"gini\", \"entropy\"]\n",
    "max_depths = [None, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 20, 25, 30, 50]\n",
    "best_params = {\"n_estimators\": None, \"criterion\": None, \"max_depth\": None}\n",
    "max_acc = 0.0\n",
    "\n",
    "for n in tqdm(estimators):\n",
    "    for max_depth in max_depths:\n",
    "        for criterion in criterions:\n",
    "            rfc = RandomForestClassifier(n_estimators=n, criterion=criterion, max_depth=max_depth, random_state=47)\n",
    "            rfc.fit(x_train.detach().cpu().numpy(), y_train.detach().cpu().numpy())\n",
    "            acc = rfc.score(x_val.detach().cpu().numpy(), y_val.detach().cpu().numpy())\n",
    "            if acc > max_acc:\n",
    "                best_params[\"criterion\"] = criterion\n",
    "                best_params[\"max_depth\"] = max_depth\n",
    "                best_params[\"n_estimators\"] = n\n",
    "                max_acc = acc\n",
    "print(f\"Random Forest: best params: {best_params}, best accuracy: {max_acc}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/13 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2ddf0cf941ce44d1a70a0c2aa90c9251"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost: best params: {'n_estimators': 3, 'criterion': 'gini', 'max_depth': 1}, best accuracy: 0.44537815126050423\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "\n",
    "estimators = [2, 3, 4, 5, 10, 20, 30, 40, 50, 100, 150, 200, 250]\n",
    "criterions = [\"gini\", \"entropy\"]\n",
    "max_depths = [None, 1, 2, 3, 5, 6, 7, 8, 9, 10]\n",
    "best_params = {\"n_estimators\": None, \"criterion\": None, \"max_depth\": None}\n",
    "max_acc = 0.0\n",
    "\n",
    "for n in tqdm(estimators):\n",
    "    for max_depth in max_depths:\n",
    "        for criterion in criterions:\n",
    "            dtc = DecisionTreeClassifier(criterion=criterion, max_depth=max_depth)\n",
    "            adaboost = AdaBoostClassifier(base_estimator=dtc, n_estimators=n, random_state=47)\n",
    "            adaboost.fit(x_train.detach().cpu().numpy(), y_train.detach().cpu().numpy())\n",
    "            acc = adaboost.score(x_val.detach().cpu().numpy(), y_val.detach().cpu().numpy())\n",
    "            if acc > max_acc:\n",
    "                best_params[\"criterion\"] = criterion\n",
    "                best_params[\"max_depth\"] = max_depth\n",
    "                best_params[\"n_estimators\"] = n\n",
    "                max_acc = acc\n",
    "print(f\"AdaBoost: best params: {best_params}, best accuracy: {max_acc}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/13 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "643b7e5531224dfe9d627d41c516f198"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoost: best params: {'n_estimators': 2, 'criterion': 'friedman_mse', 'max_depth': 1, 'loss': None}, best accuracy: 0.4369747899159664\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "\n",
    "estimators = [2, 3, 4, 5, 10, 20, 30, 40, 50, 100, 150, 200, 250]\n",
    "criterions = [\"friedman_mse\", \"squared_error\"]\n",
    "losses = [\"deviance\"]\n",
    "max_depths = [None, 1, 2, 3, 5, 6, 7, 8, 9, 10]\n",
    "best_params = {\"n_estimators\": None, \"criterion\": None, \"max_depth\": None}\n",
    "max_acc = 0.0\n",
    "\n",
    "for n in tqdm(estimators):\n",
    "    for max_depth in max_depths:\n",
    "        for criterion in criterions:\n",
    "            adaboost = GradientBoostingClassifier(n_estimators=n, criterion=criterion, max_depth=max_depth, random_state=47)\n",
    "            adaboost.fit(x_train.detach().cpu().numpy(), y_train.detach().cpu().numpy())\n",
    "            acc = adaboost.score(x_val.detach().cpu().numpy(), y_val.detach().cpu().numpy())\n",
    "            if acc > max_acc:\n",
    "                best_params[\"criterion\"] = criterion\n",
    "                best_params[\"max_depth\"] = max_depth\n",
    "                best_params[\"n_estimators\"] = n\n",
    "                max_acc = acc\n",
    "print(f\"GradientBoost: best params: {best_params}, best accuracy: {max_acc}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
